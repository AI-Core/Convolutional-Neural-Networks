{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "__Prerequisites__\n",
    "\n",
    "- [Neural Networks](https://github.com/AI-Core/Neural-Networks/blob/master/Neural%20Networks.ipynb)\n",
    "\n",
    "The fully connected neural network we looked at in the previous lesson takes in a vector as input thus a flattened image could be passed in as input and used for classification problems successfully. But this is not the best way to do it. When trying to interpret an image, the spatial relations between the different pixels is crucial to our understanding. When we flatten the image, we lose this information.\n",
    "\n",
    "Convolutional neural networks solve this very problem. Rather than performing a matrix multiplication, a convolution operation is performed which can take in a 2d input and give a 2d output hence keep the information about the spatial relations of the pixels. This greatly increases their performance on image and video processing tasks.\n",
    "\n",
    "In the convolution proccess, you have a filter which you start on the top left side of the image and slide across the whole image, taking a dot product between the values of the filter and pixel values of the image. Bear in mind that colour images have three colour channels so your filter may be 3d so you take the dot product across a 3d volume. Each dot product corresponds to a single activation value in a 2d matrix of neurons which corresponds to a single layer in the output.\n",
    "\n",
    "![image](images/CNN_RGB.JPG)\n",
    "\n",
    "The animation below shows how a 1x3x3 filter is applied to a 1x5x5 image. Notice how the output has high  values when the filter is passed over locations where there is an X shape in the input image. This is because the values of the filter are such that it is performing pattern matching for the X shape.\n",
    "\n",
    "![image](images/convolution_animation.gif)\n",
    "\n",
    "Convolution operations have 3 parameters, the kernel size, which is the width and height of our filter, the stride, which is the number of pixels we translate our kernel by to compute the next feature, and the padding which is how many layers of 0 padding we add to the input image. We som\n",
    "\n",
    "![image](images/CNN_diagram.JPG)\n",
    "\n",
    "Each computed feature is a linear function of the pixels in a local region as opposed to fully connected nets where each computed feature is a linear function of all the pixels in the image.\n",
    "\n",
    "We have some prior understanding of how image data should be processed. We apply the same set of weights at different locations all over the image because we know that features are repeated at different locations throughout the image. This makes the learned features translation invariant.\n",
    "\n",
    "\n",
    "![image](images/CNN_FNN_comparison.JPG)\n",
    "\n",
    "\n",
    "For a long time, operations like this were used in computer vision to find different patterns in images with the engineers having to manually tune the values of the filters to perform the required function. The only difference now is that we apply an activation such as Relu or Sigmoid at each layer and after setting up the structure of the network, we initialize the filter values randomly before using gradient descent to automatically tune the values of the filters. We can also apply max pooling operations to subsample the output at each layer therefore reducing the number of parameters that need to be learned for the succeeding convolution operation.\n",
    "\n",
    "\n",
    "Just like before, each layer in the whole network learns higher level abstract features from the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# GET THE TRAINING DATASET\n",
    "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
    "                            transform=transforms.ToTensor(),          # transform the data from a PIL image to a tensor\n",
    "                            train=True,                               # is this training data?\n",
    "                            download=True                             # should i download it if it's not already here?\n",
    "                           )\n",
    "\n",
    "# GET THE TEST DATASET\n",
    "test_data = datasets.MNIST(root='MNIST-data',\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           train=False,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = train_data[np.random.randint(0, 300)][0]    # get a random example\n",
    "#print(x)\n",
    "plt.imshow(x[0].numpy(),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# MAKE TRAINING DATALOADER\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE VALIDATION DATALOADER\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE TEST DATALOADER\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            # conv2d(in_channels, out_channels, kernel_size)\n",
    "            # in_channels is the number of layers which it takes in (i.e.num color channels in 1st layer)\n",
    "            # out_channels is the number of different filters that we use\n",
    "            # kernel_size is the depthxwidthxheight of the kernel#\n",
    "            # stride is how many pixels we shift the kernel by each time\n",
    "        self.conv_layers = torch.nn.Sequential( # put your convolutional architecture here using torch.nn.Sequential \n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32*20*20, 10) # put your convolutional architecture here using torch.nn.Sequential \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)# pass through conv layers\n",
    "        x = x.view(x.shape[0], -1)# flatten output ready for fully connected layer\n",
    "        x = self.fc_layers(x)# pass through fully connected layer\n",
    "        x = F.softmax(x, dim=1)# softmax activation function on outputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available() #checks if gpu is available\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0005 # set learning rate\n",
    "epochs = 5 # set number of epochs\n",
    "\n",
    "cnn = ConvNet() #.to(device)#instantiate model\n",
    "criterion = torch.nn.CrossEntropyLoss() #use cross entropy loss function\n",
    "optimiser = torch.optim.Adam(cnn.parameters(), lr=learning_rate) # use Adam optimizer, passing it the parameters of your model and the learning rate\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, verbose=True, tag='Loss/Train'):\n",
    "    for epoch in range(epochs):\n",
    "        for idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # pass x through your model to get a prediction\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the cost\n",
    "            if verbose: print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss.item())\n",
    "            \n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of all of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and store all of the model's param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            \n",
    "            writer.add_scalar(tag, loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "    print('Training Complete. Final loss =',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 0 \tLoss: 2.3023204803466797\n",
      "Epoch: 0 \tBatch: 1 \tLoss: 2.293855905532837\n",
      "Epoch: 0 \tBatch: 2 \tLoss: 2.282557725906372\n",
      "Epoch: 0 \tBatch: 3 \tLoss: 2.276531219482422\n",
      "Epoch: 0 \tBatch: 4 \tLoss: 2.241830587387085\n",
      "Epoch: 0 \tBatch: 5 \tLoss: 2.240964889526367\n",
      "Epoch: 0 \tBatch: 6 \tLoss: 2.2257602214813232\n",
      "Epoch: 0 \tBatch: 7 \tLoss: 2.1886038780212402\n",
      "Epoch: 0 \tBatch: 8 \tLoss: 2.128694772720337\n",
      "Epoch: 0 \tBatch: 9 \tLoss: 2.064389228820801\n",
      "Epoch: 0 \tBatch: 10 \tLoss: 2.0461690425872803\n",
      "Epoch: 0 \tBatch: 11 \tLoss: 1.9763458967208862\n",
      "Epoch: 0 \tBatch: 12 \tLoss: 1.9658535718917847\n",
      "Epoch: 0 \tBatch: 13 \tLoss: 1.9424477815628052\n",
      "Epoch: 0 \tBatch: 14 \tLoss: 1.8840078115463257\n",
      "Epoch: 0 \tBatch: 15 \tLoss: 1.8187620639801025\n",
      "Epoch: 0 \tBatch: 16 \tLoss: 1.8242665529251099\n",
      "Epoch: 0 \tBatch: 17 \tLoss: 1.8340113162994385\n",
      "Epoch: 0 \tBatch: 18 \tLoss: 1.8173857927322388\n",
      "Epoch: 0 \tBatch: 19 \tLoss: 1.7756268978118896\n",
      "Epoch: 0 \tBatch: 20 \tLoss: 1.7275328636169434\n",
      "Epoch: 0 \tBatch: 21 \tLoss: 1.7566258907318115\n",
      "Epoch: 0 \tBatch: 22 \tLoss: 1.7420488595962524\n",
      "Epoch: 0 \tBatch: 23 \tLoss: 1.7256025075912476\n",
      "Epoch: 0 \tBatch: 24 \tLoss: 1.7699880599975586\n",
      "Epoch: 0 \tBatch: 25 \tLoss: 1.7590861320495605\n",
      "Epoch: 0 \tBatch: 26 \tLoss: 1.752119779586792\n",
      "Epoch: 0 \tBatch: 27 \tLoss: 1.6769685745239258\n",
      "Epoch: 0 \tBatch: 28 \tLoss: 1.7079463005065918\n",
      "Epoch: 0 \tBatch: 29 \tLoss: 1.6319365501403809\n",
      "Epoch: 0 \tBatch: 30 \tLoss: 1.7619268894195557\n",
      "Epoch: 0 \tBatch: 31 \tLoss: 1.7115689516067505\n",
      "Epoch: 0 \tBatch: 32 \tLoss: 1.7105884552001953\n",
      "Epoch: 0 \tBatch: 33 \tLoss: 1.71663236618042\n",
      "Epoch: 0 \tBatch: 34 \tLoss: 1.6705539226531982\n",
      "Epoch: 0 \tBatch: 35 \tLoss: 1.6753194332122803\n",
      "Epoch: 0 \tBatch: 36 \tLoss: 1.689581036567688\n",
      "Epoch: 0 \tBatch: 37 \tLoss: 1.6531708240509033\n",
      "Epoch: 0 \tBatch: 38 \tLoss: 1.691304326057434\n",
      "Epoch: 0 \tBatch: 39 \tLoss: 1.655622124671936\n",
      "Epoch: 0 \tBatch: 40 \tLoss: 1.6535675525665283\n",
      "Epoch: 0 \tBatch: 41 \tLoss: 1.6833336353302002\n",
      "Epoch: 0 \tBatch: 42 \tLoss: 1.7636626958847046\n",
      "Epoch: 0 \tBatch: 43 \tLoss: 1.5939887762069702\n",
      "Epoch: 0 \tBatch: 44 \tLoss: 1.6908482313156128\n",
      "Epoch: 0 \tBatch: 45 \tLoss: 1.6294060945510864\n",
      "Epoch: 0 \tBatch: 46 \tLoss: 1.650288462638855\n",
      "Epoch: 0 \tBatch: 47 \tLoss: 1.6364014148712158\n",
      "Epoch: 0 \tBatch: 48 \tLoss: 1.6653138399124146\n",
      "Epoch: 0 \tBatch: 49 \tLoss: 1.6776008605957031\n",
      "Epoch: 0 \tBatch: 50 \tLoss: 1.7163472175598145\n",
      "Epoch: 0 \tBatch: 51 \tLoss: 1.7158937454223633\n",
      "Epoch: 0 \tBatch: 52 \tLoss: 1.6509168148040771\n",
      "Epoch: 0 \tBatch: 53 \tLoss: 1.6464698314666748\n",
      "Epoch: 0 \tBatch: 54 \tLoss: 1.6627579927444458\n",
      "Epoch: 0 \tBatch: 55 \tLoss: 1.6601769924163818\n",
      "Epoch: 0 \tBatch: 56 \tLoss: 1.6883984804153442\n",
      "Epoch: 0 \tBatch: 57 \tLoss: 1.6084192991256714\n",
      "Epoch: 0 \tBatch: 58 \tLoss: 1.6777515411376953\n",
      "Epoch: 0 \tBatch: 59 \tLoss: 1.6287921667099\n",
      "Epoch: 0 \tBatch: 60 \tLoss: 1.6972429752349854\n",
      "Epoch: 0 \tBatch: 61 \tLoss: 1.598389744758606\n",
      "Epoch: 0 \tBatch: 62 \tLoss: 1.6785385608673096\n",
      "Epoch: 0 \tBatch: 63 \tLoss: 1.653885006904602\n",
      "Epoch: 0 \tBatch: 64 \tLoss: 1.672229290008545\n",
      "Epoch: 0 \tBatch: 65 \tLoss: 1.7290581464767456\n",
      "Epoch: 0 \tBatch: 66 \tLoss: 1.6247611045837402\n",
      "Epoch: 0 \tBatch: 67 \tLoss: 1.6261385679244995\n",
      "Epoch: 0 \tBatch: 68 \tLoss: 1.7129693031311035\n",
      "Epoch: 0 \tBatch: 69 \tLoss: 1.672852635383606\n",
      "Epoch: 0 \tBatch: 70 \tLoss: 1.660400629043579\n",
      "Epoch: 0 \tBatch: 71 \tLoss: 1.650012731552124\n",
      "Epoch: 0 \tBatch: 72 \tLoss: 1.6144013404846191\n",
      "Epoch: 0 \tBatch: 73 \tLoss: 1.6397583484649658\n",
      "Epoch: 0 \tBatch: 74 \tLoss: 1.586707353591919\n",
      "Epoch: 0 \tBatch: 75 \tLoss: 1.6598196029663086\n",
      "Epoch: 0 \tBatch: 76 \tLoss: 1.616565227508545\n",
      "Epoch: 0 \tBatch: 77 \tLoss: 1.6380393505096436\n",
      "Epoch: 0 \tBatch: 78 \tLoss: 1.6026337146759033\n",
      "Epoch: 0 \tBatch: 79 \tLoss: 1.6443872451782227\n",
      "Epoch: 0 \tBatch: 80 \tLoss: 1.6847286224365234\n",
      "Epoch: 0 \tBatch: 81 \tLoss: 1.65625\n",
      "Epoch: 0 \tBatch: 82 \tLoss: 1.5761793851852417\n",
      "Epoch: 0 \tBatch: 83 \tLoss: 1.7179412841796875\n",
      "Epoch: 0 \tBatch: 84 \tLoss: 1.6824824810028076\n",
      "Epoch: 0 \tBatch: 85 \tLoss: 1.6516344547271729\n",
      "Epoch: 0 \tBatch: 86 \tLoss: 1.6529712677001953\n",
      "Epoch: 0 \tBatch: 87 \tLoss: 1.6078628301620483\n",
      "Epoch: 0 \tBatch: 88 \tLoss: 1.6273233890533447\n",
      "Epoch: 0 \tBatch: 89 \tLoss: 1.6610137224197388\n",
      "Epoch: 0 \tBatch: 90 \tLoss: 1.5964115858078003\n",
      "Epoch: 0 \tBatch: 91 \tLoss: 1.6555792093276978\n",
      "Epoch: 0 \tBatch: 92 \tLoss: 1.6348391771316528\n",
      "Epoch: 0 \tBatch: 93 \tLoss: 1.598199486732483\n",
      "Epoch: 0 \tBatch: 94 \tLoss: 1.667131781578064\n",
      "Epoch: 0 \tBatch: 95 \tLoss: 1.6624794006347656\n",
      "Epoch: 0 \tBatch: 96 \tLoss: 1.668784260749817\n",
      "Epoch: 0 \tBatch: 97 \tLoss: 1.6876720190048218\n",
      "Epoch: 0 \tBatch: 98 \tLoss: 1.6781315803527832\n",
      "Epoch: 0 \tBatch: 99 \tLoss: 1.6761761903762817\n",
      "Epoch: 0 \tBatch: 100 \tLoss: 1.6164617538452148\n",
      "Epoch: 0 \tBatch: 101 \tLoss: 1.6297684907913208\n",
      "Epoch: 0 \tBatch: 102 \tLoss: 1.6687899827957153\n",
      "Epoch: 0 \tBatch: 103 \tLoss: 1.6261858940124512\n",
      "Epoch: 0 \tBatch: 104 \tLoss: 1.653335452079773\n",
      "Epoch: 0 \tBatch: 105 \tLoss: 1.6450271606445312\n",
      "Epoch: 0 \tBatch: 106 \tLoss: 1.661665439605713\n",
      "Epoch: 0 \tBatch: 107 \tLoss: 1.6402549743652344\n",
      "Epoch: 0 \tBatch: 108 \tLoss: 1.675355076789856\n",
      "Epoch: 0 \tBatch: 109 \tLoss: 1.6378897428512573\n",
      "Epoch: 0 \tBatch: 110 \tLoss: 1.6762455701828003\n",
      "Epoch: 0 \tBatch: 111 \tLoss: 1.656571388244629\n",
      "Epoch: 0 \tBatch: 112 \tLoss: 1.613269329071045\n",
      "Epoch: 0 \tBatch: 113 \tLoss: 1.6220512390136719\n",
      "Epoch: 0 \tBatch: 114 \tLoss: 1.7108598947525024\n",
      "Epoch: 0 \tBatch: 115 \tLoss: 1.671506643295288\n",
      "Epoch: 0 \tBatch: 116 \tLoss: 1.6631642580032349\n",
      "Epoch: 0 \tBatch: 117 \tLoss: 1.6102226972579956\n",
      "Epoch: 0 \tBatch: 118 \tLoss: 1.6375809907913208\n",
      "Epoch: 0 \tBatch: 119 \tLoss: 1.6268653869628906\n",
      "Epoch: 0 \tBatch: 120 \tLoss: 1.6994203329086304\n",
      "Epoch: 0 \tBatch: 121 \tLoss: 1.6415647268295288\n",
      "Epoch: 0 \tBatch: 122 \tLoss: 1.624801516532898\n",
      "Epoch: 0 \tBatch: 123 \tLoss: 1.62986421585083\n",
      "Epoch: 0 \tBatch: 124 \tLoss: 1.6353731155395508\n",
      "Epoch: 0 \tBatch: 125 \tLoss: 1.5738743543624878\n",
      "Epoch: 0 \tBatch: 126 \tLoss: 1.645626425743103\n",
      "Epoch: 0 \tBatch: 127 \tLoss: 1.5640616416931152\n",
      "Epoch: 0 \tBatch: 128 \tLoss: 1.626965880393982\n",
      "Epoch: 0 \tBatch: 129 \tLoss: 1.6125843524932861\n",
      "Epoch: 0 \tBatch: 130 \tLoss: 1.669001579284668\n",
      "Epoch: 0 \tBatch: 131 \tLoss: 1.6066945791244507\n",
      "Epoch: 0 \tBatch: 132 \tLoss: 1.6615793704986572\n",
      "Epoch: 0 \tBatch: 133 \tLoss: 1.6256892681121826\n",
      "Epoch: 0 \tBatch: 134 \tLoss: 1.6156888008117676\n",
      "Epoch: 0 \tBatch: 135 \tLoss: 1.5996254682540894\n",
      "Epoch: 0 \tBatch: 136 \tLoss: 1.644202470779419\n",
      "Epoch: 0 \tBatch: 137 \tLoss: 1.6374285221099854\n",
      "Epoch: 0 \tBatch: 138 \tLoss: 1.6011698246002197\n",
      "Epoch: 0 \tBatch: 139 \tLoss: 1.6326253414154053\n",
      "Epoch: 0 \tBatch: 140 \tLoss: 1.6352463960647583\n",
      "Epoch: 0 \tBatch: 141 \tLoss: 1.681532859802246\n",
      "Epoch: 0 \tBatch: 142 \tLoss: 1.67433500289917\n",
      "Epoch: 0 \tBatch: 143 \tLoss: 1.6532882452011108\n",
      "Epoch: 0 \tBatch: 144 \tLoss: 1.6179547309875488\n",
      "Epoch: 0 \tBatch: 145 \tLoss: 1.595061182975769\n",
      "Epoch: 0 \tBatch: 146 \tLoss: 1.650969386100769\n",
      "Epoch: 0 \tBatch: 147 \tLoss: 1.6029305458068848\n",
      "Epoch: 0 \tBatch: 148 \tLoss: 1.6639052629470825\n",
      "Epoch: 0 \tBatch: 149 \tLoss: 1.6351635456085205\n",
      "Epoch: 0 \tBatch: 150 \tLoss: 1.6772840023040771\n",
      "Epoch: 0 \tBatch: 151 \tLoss: 1.6527490615844727\n",
      "Epoch: 0 \tBatch: 152 \tLoss: 1.6327792406082153\n",
      "Epoch: 0 \tBatch: 153 \tLoss: 1.5777698755264282\n",
      "Epoch: 0 \tBatch: 154 \tLoss: 1.5851589441299438\n",
      "Epoch: 0 \tBatch: 155 \tLoss: 1.5774307250976562\n",
      "Epoch: 0 \tBatch: 156 \tLoss: 1.5994693040847778\n",
      "Epoch: 0 \tBatch: 157 \tLoss: 1.635905385017395\n",
      "Epoch: 0 \tBatch: 158 \tLoss: 1.6329054832458496\n",
      "Epoch: 0 \tBatch: 159 \tLoss: 1.542485237121582\n",
      "Epoch: 0 \tBatch: 160 \tLoss: 1.5931575298309326\n",
      "Epoch: 0 \tBatch: 161 \tLoss: 1.6411795616149902\n",
      "Epoch: 0 \tBatch: 162 \tLoss: 1.6108418703079224\n",
      "Epoch: 0 \tBatch: 163 \tLoss: 1.6578139066696167\n",
      "Epoch: 0 \tBatch: 164 \tLoss: 1.5679932832717896\n",
      "Epoch: 0 \tBatch: 165 \tLoss: 1.6071475744247437\n",
      "Epoch: 0 \tBatch: 166 \tLoss: 1.632888913154602\n",
      "Epoch: 0 \tBatch: 167 \tLoss: 1.6601057052612305\n",
      "Epoch: 0 \tBatch: 168 \tLoss: 1.6579716205596924\n",
      "Epoch: 0 \tBatch: 169 \tLoss: 1.6310248374938965\n",
      "Epoch: 0 \tBatch: 170 \tLoss: 1.5840730667114258\n",
      "Epoch: 0 \tBatch: 171 \tLoss: 1.69193696975708\n",
      "Epoch: 0 \tBatch: 172 \tLoss: 1.6506609916687012\n",
      "Epoch: 0 \tBatch: 173 \tLoss: 1.6914441585540771\n",
      "Epoch: 0 \tBatch: 174 \tLoss: 1.6509718894958496\n",
      "Epoch: 0 \tBatch: 175 \tLoss: 1.611920952796936\n",
      "Epoch: 0 \tBatch: 176 \tLoss: 1.673593521118164\n",
      "Epoch: 0 \tBatch: 177 \tLoss: 1.597381830215454\n",
      "Epoch: 0 \tBatch: 178 \tLoss: 1.6387218236923218\n",
      "Epoch: 0 \tBatch: 179 \tLoss: 1.604154348373413\n",
      "Epoch: 0 \tBatch: 180 \tLoss: 1.6066263914108276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 181 \tLoss: 1.7140676975250244\n",
      "Epoch: 0 \tBatch: 182 \tLoss: 1.6012084484100342\n",
      "Epoch: 0 \tBatch: 183 \tLoss: 1.5957220792770386\n",
      "Epoch: 0 \tBatch: 184 \tLoss: 1.641563892364502\n",
      "Epoch: 0 \tBatch: 185 \tLoss: 1.6133790016174316\n",
      "Epoch: 0 \tBatch: 186 \tLoss: 1.67202627658844\n",
      "Epoch: 0 \tBatch: 187 \tLoss: 1.636212706565857\n",
      "Epoch: 0 \tBatch: 188 \tLoss: 1.641787052154541\n",
      "Epoch: 0 \tBatch: 189 \tLoss: 1.6366937160491943\n",
      "Epoch: 0 \tBatch: 190 \tLoss: 1.5995666980743408\n",
      "Epoch: 0 \tBatch: 191 \tLoss: 1.581382155418396\n",
      "Epoch: 0 \tBatch: 192 \tLoss: 1.5706340074539185\n",
      "Epoch: 0 \tBatch: 193 \tLoss: 1.5853650569915771\n",
      "Epoch: 0 \tBatch: 194 \tLoss: 1.608154296875\n",
      "Epoch: 0 \tBatch: 195 \tLoss: 1.561804175376892\n",
      "Epoch: 0 \tBatch: 196 \tLoss: 1.6292182207107544\n",
      "Epoch: 0 \tBatch: 197 \tLoss: 1.6246535778045654\n",
      "Epoch: 0 \tBatch: 198 \tLoss: 1.6374037265777588\n",
      "Epoch: 0 \tBatch: 199 \tLoss: 1.6585594415664673\n",
      "Epoch: 0 \tBatch: 200 \tLoss: 1.641361951828003\n",
      "Epoch: 0 \tBatch: 201 \tLoss: 1.5883266925811768\n",
      "Epoch: 0 \tBatch: 202 \tLoss: 1.6207948923110962\n",
      "Epoch: 0 \tBatch: 203 \tLoss: 1.6241366863250732\n",
      "Epoch: 0 \tBatch: 204 \tLoss: 1.6890265941619873\n",
      "Epoch: 0 \tBatch: 205 \tLoss: 1.5923795700073242\n",
      "Epoch: 0 \tBatch: 206 \tLoss: 1.6865055561065674\n",
      "Epoch: 0 \tBatch: 207 \tLoss: 1.6209659576416016\n",
      "Epoch: 0 \tBatch: 208 \tLoss: 1.6236227750778198\n",
      "Epoch: 0 \tBatch: 209 \tLoss: 1.6095173358917236\n",
      "Epoch: 0 \tBatch: 210 \tLoss: 1.667079210281372\n",
      "Epoch: 0 \tBatch: 211 \tLoss: 1.6244053840637207\n",
      "Epoch: 0 \tBatch: 212 \tLoss: 1.6114256381988525\n",
      "Epoch: 0 \tBatch: 213 \tLoss: 1.5958374738693237\n",
      "Epoch: 0 \tBatch: 214 \tLoss: 1.5711145401000977\n",
      "Epoch: 0 \tBatch: 215 \tLoss: 1.6652230024337769\n",
      "Epoch: 0 \tBatch: 216 \tLoss: 1.6222811937332153\n",
      "Epoch: 0 \tBatch: 217 \tLoss: 1.6334177255630493\n",
      "Epoch: 0 \tBatch: 218 \tLoss: 1.6316887140274048\n",
      "Epoch: 0 \tBatch: 219 \tLoss: 1.6197588443756104\n",
      "Epoch: 0 \tBatch: 220 \tLoss: 1.584883451461792\n",
      "Epoch: 0 \tBatch: 221 \tLoss: 1.578035831451416\n",
      "Epoch: 0 \tBatch: 222 \tLoss: 1.638856053352356\n",
      "Epoch: 0 \tBatch: 223 \tLoss: 1.6260912418365479\n",
      "Epoch: 0 \tBatch: 224 \tLoss: 1.5770881175994873\n",
      "Epoch: 0 \tBatch: 225 \tLoss: 1.6555434465408325\n",
      "Epoch: 0 \tBatch: 226 \tLoss: 1.6435123682022095\n",
      "Epoch: 0 \tBatch: 227 \tLoss: 1.567037582397461\n",
      "Epoch: 0 \tBatch: 228 \tLoss: 1.6529878377914429\n",
      "Epoch: 0 \tBatch: 229 \tLoss: 1.5657294988632202\n",
      "Epoch: 0 \tBatch: 230 \tLoss: 1.6360437870025635\n",
      "Epoch: 0 \tBatch: 231 \tLoss: 1.5679610967636108\n",
      "Epoch: 0 \tBatch: 232 \tLoss: 1.593287467956543\n",
      "Epoch: 0 \tBatch: 233 \tLoss: 1.6361204385757446\n",
      "Epoch: 0 \tBatch: 234 \tLoss: 1.5863922834396362\n",
      "Epoch: 0 \tBatch: 235 \tLoss: 1.573638677597046\n",
      "Epoch: 0 \tBatch: 236 \tLoss: 1.6322206258773804\n",
      "Epoch: 0 \tBatch: 237 \tLoss: 1.6242804527282715\n",
      "Epoch: 0 \tBatch: 238 \tLoss: 1.6051818132400513\n",
      "Epoch: 0 \tBatch: 239 \tLoss: 1.6212576627731323\n",
      "Epoch: 0 \tBatch: 240 \tLoss: 1.6147873401641846\n",
      "Epoch: 0 \tBatch: 241 \tLoss: 1.593055009841919\n",
      "Epoch: 0 \tBatch: 242 \tLoss: 1.6653043031692505\n",
      "Epoch: 0 \tBatch: 243 \tLoss: 1.6507254838943481\n",
      "Epoch: 0 \tBatch: 244 \tLoss: 1.6604527235031128\n",
      "Epoch: 0 \tBatch: 245 \tLoss: 1.648050308227539\n",
      "Epoch: 0 \tBatch: 246 \tLoss: 1.6291967630386353\n",
      "Epoch: 0 \tBatch: 247 \tLoss: 1.5608229637145996\n",
      "Epoch: 0 \tBatch: 248 \tLoss: 1.5773422718048096\n",
      "Epoch: 0 \tBatch: 249 \tLoss: 1.627611756324768\n",
      "Epoch: 0 \tBatch: 250 \tLoss: 1.6207064390182495\n",
      "Epoch: 0 \tBatch: 251 \tLoss: 1.6177146434783936\n",
      "Epoch: 0 \tBatch: 252 \tLoss: 1.641329050064087\n",
      "Epoch: 0 \tBatch: 253 \tLoss: 1.5640252828598022\n",
      "Epoch: 0 \tBatch: 254 \tLoss: 1.5353145599365234\n",
      "Epoch: 0 \tBatch: 255 \tLoss: 1.6276638507843018\n",
      "Epoch: 0 \tBatch: 256 \tLoss: 1.565075159072876\n",
      "Epoch: 0 \tBatch: 257 \tLoss: 1.5886199474334717\n",
      "Epoch: 0 \tBatch: 258 \tLoss: 1.602098822593689\n",
      "Epoch: 0 \tBatch: 259 \tLoss: 1.5618882179260254\n",
      "Epoch: 0 \tBatch: 260 \tLoss: 1.584436297416687\n",
      "Epoch: 0 \tBatch: 261 \tLoss: 1.584867000579834\n",
      "Epoch: 0 \tBatch: 262 \tLoss: 1.5662294626235962\n",
      "Epoch: 0 \tBatch: 263 \tLoss: 1.5762635469436646\n",
      "Epoch: 0 \tBatch: 264 \tLoss: 1.59491765499115\n",
      "Epoch: 0 \tBatch: 265 \tLoss: 1.5722955465316772\n",
      "Epoch: 0 \tBatch: 266 \tLoss: 1.598966121673584\n",
      "Epoch: 0 \tBatch: 267 \tLoss: 1.6017001867294312\n",
      "Epoch: 0 \tBatch: 268 \tLoss: 1.6437785625457764\n",
      "Epoch: 0 \tBatch: 269 \tLoss: 1.5966044664382935\n",
      "Epoch: 0 \tBatch: 270 \tLoss: 1.6311966180801392\n",
      "Epoch: 0 \tBatch: 271 \tLoss: 1.6149247884750366\n",
      "Epoch: 0 \tBatch: 272 \tLoss: 1.5787994861602783\n",
      "Epoch: 0 \tBatch: 273 \tLoss: 1.634237289428711\n",
      "Epoch: 0 \tBatch: 274 \tLoss: 1.5449103116989136\n",
      "Epoch: 0 \tBatch: 275 \tLoss: 1.611494779586792\n",
      "Epoch: 0 \tBatch: 276 \tLoss: 1.6127827167510986\n",
      "Epoch: 0 \tBatch: 277 \tLoss: 1.5696969032287598\n",
      "Epoch: 0 \tBatch: 278 \tLoss: 1.6044856309890747\n",
      "Epoch: 0 \tBatch: 279 \tLoss: 1.629573106765747\n",
      "Epoch: 0 \tBatch: 280 \tLoss: 1.5798017978668213\n",
      "Epoch: 0 \tBatch: 281 \tLoss: 1.6004211902618408\n",
      "Epoch: 0 \tBatch: 282 \tLoss: 1.5467112064361572\n",
      "Epoch: 0 \tBatch: 283 \tLoss: 1.6409591436386108\n",
      "Epoch: 0 \tBatch: 284 \tLoss: 1.6176975965499878\n",
      "Epoch: 0 \tBatch: 285 \tLoss: 1.627663493156433\n",
      "Epoch: 0 \tBatch: 286 \tLoss: 1.6456717252731323\n",
      "Epoch: 0 \tBatch: 287 \tLoss: 1.6295573711395264\n",
      "Epoch: 0 \tBatch: 288 \tLoss: 1.6110789775848389\n",
      "Epoch: 0 \tBatch: 289 \tLoss: 1.6199921369552612\n",
      "Epoch: 0 \tBatch: 290 \tLoss: 1.5844987630844116\n",
      "Epoch: 0 \tBatch: 291 \tLoss: 1.607595682144165\n",
      "Epoch: 0 \tBatch: 292 \tLoss: 1.619160532951355\n",
      "Epoch: 0 \tBatch: 293 \tLoss: 1.5580352544784546\n",
      "Epoch: 0 \tBatch: 294 \tLoss: 1.641486406326294\n",
      "Epoch: 0 \tBatch: 295 \tLoss: 1.6058493852615356\n",
      "Epoch: 0 \tBatch: 296 \tLoss: 1.618563175201416\n",
      "Epoch: 0 \tBatch: 297 \tLoss: 1.6138689517974854\n",
      "Epoch: 0 \tBatch: 298 \tLoss: 1.5902754068374634\n",
      "Epoch: 0 \tBatch: 299 \tLoss: 1.6115877628326416\n",
      "Epoch: 0 \tBatch: 300 \tLoss: 1.5986860990524292\n",
      "Epoch: 0 \tBatch: 301 \tLoss: 1.6092435121536255\n",
      "Epoch: 0 \tBatch: 302 \tLoss: 1.5811972618103027\n",
      "Epoch: 0 \tBatch: 303 \tLoss: 1.6734764575958252\n",
      "Epoch: 0 \tBatch: 304 \tLoss: 1.5376828908920288\n",
      "Epoch: 0 \tBatch: 305 \tLoss: 1.5687291622161865\n",
      "Epoch: 0 \tBatch: 306 \tLoss: 1.5745282173156738\n",
      "Epoch: 0 \tBatch: 307 \tLoss: 1.5365768671035767\n",
      "Epoch: 0 \tBatch: 308 \tLoss: 1.5959556102752686\n",
      "Epoch: 0 \tBatch: 309 \tLoss: 1.577991247177124\n",
      "Epoch: 0 \tBatch: 310 \tLoss: 1.5791507959365845\n",
      "Epoch: 0 \tBatch: 311 \tLoss: 1.5900919437408447\n",
      "Epoch: 0 \tBatch: 312 \tLoss: 1.593434453010559\n",
      "Epoch: 0 \tBatch: 313 \tLoss: 1.640785813331604\n",
      "Epoch: 0 \tBatch: 314 \tLoss: 1.5860412120819092\n",
      "Epoch: 0 \tBatch: 315 \tLoss: 1.570456624031067\n",
      "Epoch: 0 \tBatch: 316 \tLoss: 1.6078218221664429\n",
      "Epoch: 0 \tBatch: 317 \tLoss: 1.5804816484451294\n",
      "Epoch: 0 \tBatch: 318 \tLoss: 1.685961365699768\n",
      "Epoch: 0 \tBatch: 319 \tLoss: 1.5532358884811401\n",
      "Epoch: 0 \tBatch: 320 \tLoss: 1.599789023399353\n",
      "Epoch: 0 \tBatch: 321 \tLoss: 1.6109704971313477\n",
      "Epoch: 0 \tBatch: 322 \tLoss: 1.5974798202514648\n",
      "Epoch: 0 \tBatch: 323 \tLoss: 1.5645183324813843\n",
      "Epoch: 0 \tBatch: 324 \tLoss: 1.661062240600586\n",
      "Epoch: 0 \tBatch: 325 \tLoss: 1.6397165060043335\n",
      "Epoch: 0 \tBatch: 326 \tLoss: 1.6179348230361938\n",
      "Epoch: 0 \tBatch: 327 \tLoss: 1.6059104204177856\n",
      "Epoch: 0 \tBatch: 328 \tLoss: 1.5927584171295166\n",
      "Epoch: 0 \tBatch: 329 \tLoss: 1.6136891841888428\n",
      "Epoch: 0 \tBatch: 330 \tLoss: 1.6020451784133911\n",
      "Epoch: 0 \tBatch: 331 \tLoss: 1.6007250547409058\n",
      "Epoch: 0 \tBatch: 332 \tLoss: 1.6036075353622437\n",
      "Epoch: 0 \tBatch: 333 \tLoss: 1.5877125263214111\n",
      "Epoch: 0 \tBatch: 334 \tLoss: 1.6333009004592896\n",
      "Epoch: 0 \tBatch: 335 \tLoss: 1.5331555604934692\n",
      "Epoch: 0 \tBatch: 336 \tLoss: 1.6120842695236206\n",
      "Epoch: 0 \tBatch: 337 \tLoss: 1.6202342510223389\n",
      "Epoch: 0 \tBatch: 338 \tLoss: 1.591281771659851\n",
      "Epoch: 0 \tBatch: 339 \tLoss: 1.5820703506469727\n",
      "Epoch: 0 \tBatch: 340 \tLoss: 1.584059238433838\n",
      "Epoch: 0 \tBatch: 341 \tLoss: 1.59929621219635\n",
      "Epoch: 0 \tBatch: 342 \tLoss: 1.6341807842254639\n",
      "Epoch: 0 \tBatch: 343 \tLoss: 1.615985631942749\n",
      "Epoch: 0 \tBatch: 344 \tLoss: 1.5990879535675049\n",
      "Epoch: 0 \tBatch: 345 \tLoss: 1.6248255968093872\n",
      "Epoch: 0 \tBatch: 346 \tLoss: 1.6342847347259521\n",
      "Epoch: 0 \tBatch: 347 \tLoss: 1.5859689712524414\n",
      "Epoch: 0 \tBatch: 348 \tLoss: 1.6252412796020508\n",
      "Epoch: 0 \tBatch: 349 \tLoss: 1.6309385299682617\n",
      "Epoch: 0 \tBatch: 350 \tLoss: 1.598890781402588\n",
      "Epoch: 0 \tBatch: 351 \tLoss: 1.5774834156036377\n",
      "Epoch: 0 \tBatch: 352 \tLoss: 1.5907871723175049\n",
      "Epoch: 0 \tBatch: 353 \tLoss: 1.5899605751037598\n",
      "Epoch: 0 \tBatch: 354 \tLoss: 1.5851709842681885\n",
      "Epoch: 0 \tBatch: 355 \tLoss: 1.599318265914917\n",
      "Epoch: 0 \tBatch: 356 \tLoss: 1.586670994758606\n",
      "Epoch: 0 \tBatch: 357 \tLoss: 1.5677520036697388\n",
      "Epoch: 0 \tBatch: 358 \tLoss: 1.5666866302490234\n",
      "Epoch: 0 \tBatch: 359 \tLoss: 1.553088665008545\n",
      "Epoch: 0 \tBatch: 360 \tLoss: 1.5932049751281738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 361 \tLoss: 1.603010892868042\n",
      "Epoch: 0 \tBatch: 362 \tLoss: 1.5764633417129517\n",
      "Epoch: 0 \tBatch: 363 \tLoss: 1.5998725891113281\n",
      "Epoch: 0 \tBatch: 364 \tLoss: 1.581624150276184\n",
      "Epoch: 0 \tBatch: 365 \tLoss: 1.5623894929885864\n",
      "Epoch: 0 \tBatch: 366 \tLoss: 1.590279221534729\n",
      "Epoch: 0 \tBatch: 367 \tLoss: 1.593650460243225\n",
      "Epoch: 0 \tBatch: 368 \tLoss: 1.6028517484664917\n",
      "Epoch: 0 \tBatch: 369 \tLoss: 1.6095980405807495\n",
      "Epoch: 0 \tBatch: 370 \tLoss: 1.67770516872406\n",
      "Epoch: 0 \tBatch: 371 \tLoss: 1.6095725297927856\n",
      "Epoch: 0 \tBatch: 372 \tLoss: 1.5738474130630493\n",
      "Epoch: 0 \tBatch: 373 \tLoss: 1.5672917366027832\n",
      "Epoch: 0 \tBatch: 374 \tLoss: 1.5832558870315552\n",
      "Epoch: 0 \tBatch: 375 \tLoss: 1.5995733737945557\n",
      "Epoch: 0 \tBatch: 376 \tLoss: 1.626615285873413\n",
      "Epoch: 0 \tBatch: 377 \tLoss: 1.6384879350662231\n",
      "Epoch: 0 \tBatch: 378 \tLoss: 1.5839964151382446\n",
      "Epoch: 0 \tBatch: 379 \tLoss: 1.5846731662750244\n",
      "Epoch: 0 \tBatch: 380 \tLoss: 1.5853960514068604\n",
      "Epoch: 0 \tBatch: 381 \tLoss: 1.5844664573669434\n",
      "Epoch: 0 \tBatch: 382 \tLoss: 1.574418544769287\n",
      "Epoch: 0 \tBatch: 383 \tLoss: 1.59639573097229\n",
      "Epoch: 0 \tBatch: 384 \tLoss: 1.555415391921997\n",
      "Epoch: 0 \tBatch: 385 \tLoss: 1.6267472505569458\n",
      "Epoch: 0 \tBatch: 386 \tLoss: 1.6135714054107666\n",
      "Epoch: 0 \tBatch: 387 \tLoss: 1.5722957849502563\n",
      "Epoch: 0 \tBatch: 388 \tLoss: 1.540081262588501\n",
      "Epoch: 0 \tBatch: 389 \tLoss: 1.5432896614074707\n",
      "Epoch: 0 \tBatch: 390 \tLoss: 1.6196672916412354\n",
      "Epoch: 1 \tBatch: 0 \tLoss: 1.598623514175415\n",
      "Epoch: 1 \tBatch: 1 \tLoss: 1.623399019241333\n",
      "Epoch: 1 \tBatch: 2 \tLoss: 1.6391432285308838\n",
      "Epoch: 1 \tBatch: 3 \tLoss: 1.5659358501434326\n",
      "Epoch: 1 \tBatch: 4 \tLoss: 1.5851857662200928\n",
      "Epoch: 1 \tBatch: 5 \tLoss: 1.6142033338546753\n",
      "Epoch: 1 \tBatch: 6 \tLoss: 1.6105917692184448\n",
      "Epoch: 1 \tBatch: 7 \tLoss: 1.561155080795288\n",
      "Epoch: 1 \tBatch: 8 \tLoss: 1.6420761346817017\n",
      "Epoch: 1 \tBatch: 9 \tLoss: 1.6331309080123901\n",
      "Epoch: 1 \tBatch: 10 \tLoss: 1.5673415660858154\n",
      "Epoch: 1 \tBatch: 11 \tLoss: 1.52913498878479\n",
      "Epoch: 1 \tBatch: 12 \tLoss: 1.6199673414230347\n",
      "Epoch: 1 \tBatch: 13 \tLoss: 1.5865365266799927\n",
      "Epoch: 1 \tBatch: 14 \tLoss: 1.5648549795150757\n",
      "Epoch: 1 \tBatch: 15 \tLoss: 1.5608839988708496\n",
      "Epoch: 1 \tBatch: 16 \tLoss: 1.5569809675216675\n",
      "Epoch: 1 \tBatch: 17 \tLoss: 1.5781524181365967\n",
      "Epoch: 1 \tBatch: 18 \tLoss: 1.6053073406219482\n",
      "Epoch: 1 \tBatch: 19 \tLoss: 1.6545926332473755\n",
      "Epoch: 1 \tBatch: 20 \tLoss: 1.5897363424301147\n",
      "Epoch: 1 \tBatch: 21 \tLoss: 1.6254514455795288\n",
      "Epoch: 1 \tBatch: 22 \tLoss: 1.5970685482025146\n",
      "Epoch: 1 \tBatch: 23 \tLoss: 1.6426347494125366\n",
      "Epoch: 1 \tBatch: 24 \tLoss: 1.5742756128311157\n",
      "Epoch: 1 \tBatch: 25 \tLoss: 1.6057207584381104\n",
      "Epoch: 1 \tBatch: 26 \tLoss: 1.590627908706665\n",
      "Epoch: 1 \tBatch: 27 \tLoss: 1.6075950860977173\n",
      "Epoch: 1 \tBatch: 28 \tLoss: 1.582625389099121\n",
      "Epoch: 1 \tBatch: 29 \tLoss: 1.625995397567749\n",
      "Epoch: 1 \tBatch: 30 \tLoss: 1.5632683038711548\n",
      "Epoch: 1 \tBatch: 31 \tLoss: 1.6324437856674194\n",
      "Epoch: 1 \tBatch: 32 \tLoss: 1.6047099828720093\n",
      "Epoch: 1 \tBatch: 33 \tLoss: 1.6330047845840454\n",
      "Epoch: 1 \tBatch: 34 \tLoss: 1.5391607284545898\n",
      "Epoch: 1 \tBatch: 35 \tLoss: 1.6132572889328003\n",
      "Epoch: 1 \tBatch: 36 \tLoss: 1.5672433376312256\n",
      "Epoch: 1 \tBatch: 37 \tLoss: 1.5299160480499268\n",
      "Epoch: 1 \tBatch: 38 \tLoss: 1.5233025550842285\n",
      "Epoch: 1 \tBatch: 39 \tLoss: 1.6115325689315796\n",
      "Epoch: 1 \tBatch: 40 \tLoss: 1.5317730903625488\n",
      "Epoch: 1 \tBatch: 41 \tLoss: 1.5872819423675537\n",
      "Epoch: 1 \tBatch: 42 \tLoss: 1.6109524965286255\n",
      "Epoch: 1 \tBatch: 43 \tLoss: 1.6061162948608398\n",
      "Epoch: 1 \tBatch: 44 \tLoss: 1.6333290338516235\n",
      "Epoch: 1 \tBatch: 45 \tLoss: 1.574459433555603\n",
      "Epoch: 1 \tBatch: 46 \tLoss: 1.614729881286621\n",
      "Epoch: 1 \tBatch: 47 \tLoss: 1.6246103048324585\n",
      "Epoch: 1 \tBatch: 48 \tLoss: 1.5740175247192383\n",
      "Epoch: 1 \tBatch: 49 \tLoss: 1.5409801006317139\n",
      "Epoch: 1 \tBatch: 50 \tLoss: 1.5921680927276611\n",
      "Epoch: 1 \tBatch: 51 \tLoss: 1.5857230424880981\n",
      "Epoch: 1 \tBatch: 52 \tLoss: 1.570490837097168\n",
      "Epoch: 1 \tBatch: 53 \tLoss: 1.6050525903701782\n",
      "Epoch: 1 \tBatch: 54 \tLoss: 1.6211568117141724\n",
      "Epoch: 1 \tBatch: 55 \tLoss: 1.5340888500213623\n",
      "Epoch: 1 \tBatch: 56 \tLoss: 1.594218134880066\n",
      "Epoch: 1 \tBatch: 57 \tLoss: 1.5840198993682861\n",
      "Epoch: 1 \tBatch: 58 \tLoss: 1.5529531240463257\n",
      "Epoch: 1 \tBatch: 59 \tLoss: 1.5636321306228638\n",
      "Epoch: 1 \tBatch: 60 \tLoss: 1.565771460533142\n",
      "Epoch: 1 \tBatch: 61 \tLoss: 1.6016193628311157\n",
      "Epoch: 1 \tBatch: 62 \tLoss: 1.5692306756973267\n",
      "Epoch: 1 \tBatch: 63 \tLoss: 1.5907642841339111\n",
      "Epoch: 1 \tBatch: 64 \tLoss: 1.5638428926467896\n",
      "Epoch: 1 \tBatch: 65 \tLoss: 1.5702948570251465\n",
      "Epoch: 1 \tBatch: 66 \tLoss: 1.5591696500778198\n",
      "Epoch: 1 \tBatch: 67 \tLoss: 1.593047022819519\n",
      "Epoch: 1 \tBatch: 68 \tLoss: 1.553032636642456\n",
      "Epoch: 1 \tBatch: 69 \tLoss: 1.5531234741210938\n",
      "Epoch: 1 \tBatch: 70 \tLoss: 1.5600533485412598\n",
      "Epoch: 1 \tBatch: 71 \tLoss: 1.5930931568145752\n",
      "Epoch: 1 \tBatch: 72 \tLoss: 1.5937930345535278\n",
      "Epoch: 1 \tBatch: 73 \tLoss: 1.5348365306854248\n",
      "Epoch: 1 \tBatch: 74 \tLoss: 1.5644465684890747\n",
      "Epoch: 1 \tBatch: 75 \tLoss: 1.606900691986084\n",
      "Epoch: 1 \tBatch: 76 \tLoss: 1.5638659000396729\n",
      "Epoch: 1 \tBatch: 77 \tLoss: 1.5244548320770264\n",
      "Epoch: 1 \tBatch: 78 \tLoss: 1.5975077152252197\n",
      "Epoch: 1 \tBatch: 79 \tLoss: 1.5965529680252075\n",
      "Epoch: 1 \tBatch: 80 \tLoss: 1.5966694355010986\n",
      "Epoch: 1 \tBatch: 81 \tLoss: 1.6315091848373413\n",
      "Epoch: 1 \tBatch: 82 \tLoss: 1.5533288717269897\n",
      "Epoch: 1 \tBatch: 83 \tLoss: 1.6140012741088867\n",
      "Epoch: 1 \tBatch: 84 \tLoss: 1.6201865673065186\n",
      "Epoch: 1 \tBatch: 85 \tLoss: 1.6157201528549194\n",
      "Epoch: 1 \tBatch: 86 \tLoss: 1.558284044265747\n",
      "Epoch: 1 \tBatch: 87 \tLoss: 1.6018115282058716\n",
      "Epoch: 1 \tBatch: 88 \tLoss: 1.611928105354309\n",
      "Epoch: 1 \tBatch: 89 \tLoss: 1.5963884592056274\n",
      "Epoch: 1 \tBatch: 90 \tLoss: 1.561232566833496\n",
      "Epoch: 1 \tBatch: 91 \tLoss: 1.5708168745040894\n",
      "Epoch: 1 \tBatch: 92 \tLoss: 1.579940915107727\n",
      "Epoch: 1 \tBatch: 93 \tLoss: 1.567591667175293\n",
      "Epoch: 1 \tBatch: 94 \tLoss: 1.5374585390090942\n",
      "Epoch: 1 \tBatch: 95 \tLoss: 1.6023517847061157\n",
      "Epoch: 1 \tBatch: 96 \tLoss: 1.5714150667190552\n",
      "Epoch: 1 \tBatch: 97 \tLoss: 1.5657508373260498\n",
      "Epoch: 1 \tBatch: 98 \tLoss: 1.59897780418396\n",
      "Epoch: 1 \tBatch: 99 \tLoss: 1.5599892139434814\n",
      "Epoch: 1 \tBatch: 100 \tLoss: 1.60965895652771\n",
      "Epoch: 1 \tBatch: 101 \tLoss: 1.5872857570648193\n",
      "Epoch: 1 \tBatch: 102 \tLoss: 1.5804357528686523\n",
      "Epoch: 1 \tBatch: 103 \tLoss: 1.62575364112854\n",
      "Epoch: 1 \tBatch: 104 \tLoss: 1.5649909973144531\n",
      "Epoch: 1 \tBatch: 105 \tLoss: 1.5885957479476929\n",
      "Epoch: 1 \tBatch: 106 \tLoss: 1.5744696855545044\n",
      "Epoch: 1 \tBatch: 107 \tLoss: 1.5776333808898926\n",
      "Epoch: 1 \tBatch: 108 \tLoss: 1.5946969985961914\n",
      "Epoch: 1 \tBatch: 109 \tLoss: 1.6230428218841553\n",
      "Epoch: 1 \tBatch: 110 \tLoss: 1.6057368516921997\n",
      "Epoch: 1 \tBatch: 111 \tLoss: 1.5641347169876099\n",
      "Epoch: 1 \tBatch: 112 \tLoss: 1.6233313083648682\n",
      "Epoch: 1 \tBatch: 113 \tLoss: 1.550804853439331\n",
      "Epoch: 1 \tBatch: 114 \tLoss: 1.561180830001831\n",
      "Epoch: 1 \tBatch: 115 \tLoss: 1.5828372240066528\n",
      "Epoch: 1 \tBatch: 116 \tLoss: 1.635362982749939\n",
      "Epoch: 1 \tBatch: 117 \tLoss: 1.5245671272277832\n",
      "Epoch: 1 \tBatch: 118 \tLoss: 1.6044813394546509\n",
      "Epoch: 1 \tBatch: 119 \tLoss: 1.573699951171875\n",
      "Epoch: 1 \tBatch: 120 \tLoss: 1.5491125583648682\n",
      "Epoch: 1 \tBatch: 121 \tLoss: 1.5910229682922363\n",
      "Epoch: 1 \tBatch: 122 \tLoss: 1.5913450717926025\n",
      "Epoch: 1 \tBatch: 123 \tLoss: 1.607991337776184\n",
      "Epoch: 1 \tBatch: 124 \tLoss: 1.621855616569519\n",
      "Epoch: 1 \tBatch: 125 \tLoss: 1.6039832830429077\n",
      "Epoch: 1 \tBatch: 126 \tLoss: 1.5740971565246582\n",
      "Epoch: 1 \tBatch: 127 \tLoss: 1.6401640176773071\n",
      "Epoch: 1 \tBatch: 128 \tLoss: 1.5896328687667847\n",
      "Epoch: 1 \tBatch: 129 \tLoss: 1.6008033752441406\n",
      "Epoch: 1 \tBatch: 130 \tLoss: 1.5744065046310425\n",
      "Epoch: 1 \tBatch: 131 \tLoss: 1.556742787361145\n",
      "Epoch: 1 \tBatch: 132 \tLoss: 1.5272852182388306\n",
      "Epoch: 1 \tBatch: 133 \tLoss: 1.541462779045105\n",
      "Epoch: 1 \tBatch: 134 \tLoss: 1.536110758781433\n",
      "Epoch: 1 \tBatch: 135 \tLoss: 1.5853815078735352\n",
      "Epoch: 1 \tBatch: 136 \tLoss: 1.5029937028884888\n",
      "Epoch: 1 \tBatch: 137 \tLoss: 1.5999969244003296\n",
      "Epoch: 1 \tBatch: 138 \tLoss: 1.5302479267120361\n",
      "Epoch: 1 \tBatch: 139 \tLoss: 1.525626301765442\n",
      "Epoch: 1 \tBatch: 140 \tLoss: 1.5428485870361328\n",
      "Epoch: 1 \tBatch: 141 \tLoss: 1.5048786401748657\n",
      "Epoch: 1 \tBatch: 142 \tLoss: 1.5360639095306396\n",
      "Epoch: 1 \tBatch: 143 \tLoss: 1.5290069580078125\n",
      "Epoch: 1 \tBatch: 144 \tLoss: 1.5020099878311157\n",
      "Epoch: 1 \tBatch: 145 \tLoss: 1.5228513479232788\n",
      "Epoch: 1 \tBatch: 146 \tLoss: 1.4731051921844482\n",
      "Epoch: 1 \tBatch: 147 \tLoss: 1.5235936641693115\n",
      "Epoch: 1 \tBatch: 148 \tLoss: 1.518109679222107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 149 \tLoss: 1.5043617486953735\n",
      "Epoch: 1 \tBatch: 150 \tLoss: 1.4904876947402954\n",
      "Epoch: 1 \tBatch: 151 \tLoss: 1.487488865852356\n",
      "Epoch: 1 \tBatch: 152 \tLoss: 1.5405703783035278\n",
      "Epoch: 1 \tBatch: 153 \tLoss: 1.5266612768173218\n",
      "Epoch: 1 \tBatch: 154 \tLoss: 1.520258903503418\n",
      "Epoch: 1 \tBatch: 155 \tLoss: 1.494449257850647\n",
      "Epoch: 1 \tBatch: 156 \tLoss: 1.5155320167541504\n",
      "Epoch: 1 \tBatch: 157 \tLoss: 1.5089572668075562\n",
      "Epoch: 1 \tBatch: 158 \tLoss: 1.4992049932479858\n",
      "Epoch: 1 \tBatch: 159 \tLoss: 1.4887661933898926\n",
      "Epoch: 1 \tBatch: 160 \tLoss: 1.5205374956130981\n",
      "Epoch: 1 \tBatch: 161 \tLoss: 1.5023967027664185\n",
      "Epoch: 1 \tBatch: 162 \tLoss: 1.5009278059005737\n",
      "Epoch: 1 \tBatch: 163 \tLoss: 1.4985874891281128\n",
      "Epoch: 1 \tBatch: 164 \tLoss: 1.5083484649658203\n",
      "Epoch: 1 \tBatch: 165 \tLoss: 1.5030169486999512\n",
      "Epoch: 1 \tBatch: 166 \tLoss: 1.535423994064331\n",
      "Epoch: 1 \tBatch: 167 \tLoss: 1.5256454944610596\n",
      "Epoch: 1 \tBatch: 168 \tLoss: 1.515026330947876\n",
      "Epoch: 1 \tBatch: 169 \tLoss: 1.508163332939148\n",
      "Epoch: 1 \tBatch: 170 \tLoss: 1.5074641704559326\n",
      "Epoch: 1 \tBatch: 171 \tLoss: 1.5352442264556885\n",
      "Epoch: 1 \tBatch: 172 \tLoss: 1.4818429946899414\n",
      "Epoch: 1 \tBatch: 173 \tLoss: 1.4921334981918335\n",
      "Epoch: 1 \tBatch: 174 \tLoss: 1.4906978607177734\n",
      "Epoch: 1 \tBatch: 175 \tLoss: 1.5055718421936035\n",
      "Epoch: 1 \tBatch: 176 \tLoss: 1.519639015197754\n",
      "Epoch: 1 \tBatch: 177 \tLoss: 1.517393946647644\n",
      "Epoch: 1 \tBatch: 178 \tLoss: 1.4969927072525024\n",
      "Epoch: 1 \tBatch: 179 \tLoss: 1.5065451860427856\n",
      "Epoch: 1 \tBatch: 180 \tLoss: 1.5327324867248535\n",
      "Epoch: 1 \tBatch: 181 \tLoss: 1.4874670505523682\n",
      "Epoch: 1 \tBatch: 182 \tLoss: 1.5194722414016724\n",
      "Epoch: 1 \tBatch: 183 \tLoss: 1.505650520324707\n",
      "Epoch: 1 \tBatch: 184 \tLoss: 1.4853205680847168\n",
      "Epoch: 1 \tBatch: 185 \tLoss: 1.5022292137145996\n",
      "Epoch: 1 \tBatch: 186 \tLoss: 1.5109879970550537\n",
      "Epoch: 1 \tBatch: 187 \tLoss: 1.5072327852249146\n",
      "Epoch: 1 \tBatch: 188 \tLoss: 1.5082626342773438\n",
      "Epoch: 1 \tBatch: 189 \tLoss: 1.4850066900253296\n",
      "Epoch: 1 \tBatch: 190 \tLoss: 1.4972388744354248\n",
      "Epoch: 1 \tBatch: 191 \tLoss: 1.4807437658309937\n",
      "Epoch: 1 \tBatch: 192 \tLoss: 1.4996830224990845\n",
      "Epoch: 1 \tBatch: 193 \tLoss: 1.5013558864593506\n",
      "Epoch: 1 \tBatch: 194 \tLoss: 1.5027408599853516\n",
      "Epoch: 1 \tBatch: 195 \tLoss: 1.5036252737045288\n",
      "Epoch: 1 \tBatch: 196 \tLoss: 1.4923783540725708\n",
      "Epoch: 1 \tBatch: 197 \tLoss: 1.4940996170043945\n",
      "Epoch: 1 \tBatch: 198 \tLoss: 1.5040515661239624\n",
      "Epoch: 1 \tBatch: 199 \tLoss: 1.4826468229293823\n",
      "Epoch: 1 \tBatch: 200 \tLoss: 1.497840404510498\n",
      "Epoch: 1 \tBatch: 201 \tLoss: 1.5105143785476685\n",
      "Epoch: 1 \tBatch: 202 \tLoss: 1.499775767326355\n",
      "Epoch: 1 \tBatch: 203 \tLoss: 1.5073386430740356\n",
      "Epoch: 1 \tBatch: 204 \tLoss: 1.480656623840332\n",
      "Epoch: 1 \tBatch: 205 \tLoss: 1.4953086376190186\n",
      "Epoch: 1 \tBatch: 206 \tLoss: 1.4918394088745117\n",
      "Epoch: 1 \tBatch: 207 \tLoss: 1.4865682125091553\n",
      "Epoch: 1 \tBatch: 208 \tLoss: 1.5231589078903198\n",
      "Epoch: 1 \tBatch: 209 \tLoss: 1.4897805452346802\n",
      "Epoch: 1 \tBatch: 210 \tLoss: 1.515933871269226\n",
      "Epoch: 1 \tBatch: 211 \tLoss: 1.4951902627944946\n",
      "Epoch: 1 \tBatch: 212 \tLoss: 1.4903819561004639\n",
      "Epoch: 1 \tBatch: 213 \tLoss: 1.5159777402877808\n",
      "Epoch: 1 \tBatch: 214 \tLoss: 1.499758243560791\n",
      "Epoch: 1 \tBatch: 215 \tLoss: 1.5096180438995361\n",
      "Epoch: 1 \tBatch: 216 \tLoss: 1.4950453042984009\n",
      "Epoch: 1 \tBatch: 217 \tLoss: 1.5043367147445679\n",
      "Epoch: 1 \tBatch: 218 \tLoss: 1.5153839588165283\n",
      "Epoch: 1 \tBatch: 219 \tLoss: 1.5145069360733032\n",
      "Epoch: 1 \tBatch: 220 \tLoss: 1.4986283779144287\n",
      "Epoch: 1 \tBatch: 221 \tLoss: 1.4917477369308472\n",
      "Epoch: 1 \tBatch: 222 \tLoss: 1.4946742057800293\n",
      "Epoch: 1 \tBatch: 223 \tLoss: 1.4987825155258179\n",
      "Epoch: 1 \tBatch: 224 \tLoss: 1.4830451011657715\n",
      "Epoch: 1 \tBatch: 225 \tLoss: 1.4982213973999023\n",
      "Epoch: 1 \tBatch: 226 \tLoss: 1.495453119277954\n",
      "Epoch: 1 \tBatch: 227 \tLoss: 1.4878973960876465\n",
      "Epoch: 1 \tBatch: 228 \tLoss: 1.4850080013275146\n",
      "Epoch: 1 \tBatch: 229 \tLoss: 1.4869362115859985\n",
      "Epoch: 1 \tBatch: 230 \tLoss: 1.4982017278671265\n",
      "Epoch: 1 \tBatch: 231 \tLoss: 1.4951220750808716\n",
      "Epoch: 1 \tBatch: 232 \tLoss: 1.4875932931900024\n",
      "Epoch: 1 \tBatch: 233 \tLoss: 1.4854238033294678\n",
      "Epoch: 1 \tBatch: 234 \tLoss: 1.4960198402404785\n",
      "Epoch: 1 \tBatch: 235 \tLoss: 1.4824758768081665\n",
      "Epoch: 1 \tBatch: 236 \tLoss: 1.4881811141967773\n",
      "Epoch: 1 \tBatch: 237 \tLoss: 1.5148227214813232\n",
      "Epoch: 1 \tBatch: 238 \tLoss: 1.4916577339172363\n",
      "Epoch: 1 \tBatch: 239 \tLoss: 1.4843719005584717\n",
      "Epoch: 1 \tBatch: 240 \tLoss: 1.4753512144088745\n",
      "Epoch: 1 \tBatch: 241 \tLoss: 1.4773919582366943\n",
      "Epoch: 1 \tBatch: 242 \tLoss: 1.4991930723190308\n",
      "Epoch: 1 \tBatch: 243 \tLoss: 1.5027083158493042\n",
      "Epoch: 1 \tBatch: 244 \tLoss: 1.4763790369033813\n",
      "Epoch: 1 \tBatch: 245 \tLoss: 1.485461950302124\n",
      "Epoch: 1 \tBatch: 246 \tLoss: 1.5025311708450317\n",
      "Epoch: 1 \tBatch: 247 \tLoss: 1.4822783470153809\n",
      "Epoch: 1 \tBatch: 248 \tLoss: 1.5124832391738892\n",
      "Epoch: 1 \tBatch: 249 \tLoss: 1.5008600950241089\n",
      "Epoch: 1 \tBatch: 250 \tLoss: 1.4913973808288574\n",
      "Epoch: 1 \tBatch: 251 \tLoss: 1.4781221151351929\n",
      "Epoch: 1 \tBatch: 252 \tLoss: 1.4919824600219727\n",
      "Epoch: 1 \tBatch: 253 \tLoss: 1.5137780904769897\n",
      "Epoch: 1 \tBatch: 254 \tLoss: 1.501670002937317\n",
      "Epoch: 1 \tBatch: 255 \tLoss: 1.5185846090316772\n",
      "Epoch: 1 \tBatch: 256 \tLoss: 1.4870585203170776\n",
      "Epoch: 1 \tBatch: 257 \tLoss: 1.487356424331665\n",
      "Epoch: 1 \tBatch: 258 \tLoss: 1.4857769012451172\n",
      "Epoch: 1 \tBatch: 259 \tLoss: 1.5028780698776245\n",
      "Epoch: 1 \tBatch: 260 \tLoss: 1.5231722593307495\n",
      "Epoch: 1 \tBatch: 261 \tLoss: 1.5097172260284424\n",
      "Epoch: 1 \tBatch: 262 \tLoss: 1.483933448791504\n",
      "Epoch: 1 \tBatch: 263 \tLoss: 1.513800024986267\n",
      "Epoch: 1 \tBatch: 264 \tLoss: 1.4814668893814087\n",
      "Epoch: 1 \tBatch: 265 \tLoss: 1.487929344177246\n",
      "Epoch: 1 \tBatch: 266 \tLoss: 1.5273118019104004\n",
      "Epoch: 1 \tBatch: 267 \tLoss: 1.4963213205337524\n",
      "Epoch: 1 \tBatch: 268 \tLoss: 1.4929221868515015\n",
      "Epoch: 1 \tBatch: 269 \tLoss: 1.5136827230453491\n",
      "Epoch: 1 \tBatch: 270 \tLoss: 1.4822673797607422\n",
      "Epoch: 1 \tBatch: 271 \tLoss: 1.4723021984100342\n",
      "Epoch: 1 \tBatch: 272 \tLoss: 1.5106768608093262\n",
      "Epoch: 1 \tBatch: 273 \tLoss: 1.520951509475708\n",
      "Epoch: 1 \tBatch: 274 \tLoss: 1.486154317855835\n",
      "Epoch: 1 \tBatch: 275 \tLoss: 1.463212013244629\n",
      "Epoch: 1 \tBatch: 276 \tLoss: 1.5320000648498535\n",
      "Epoch: 1 \tBatch: 277 \tLoss: 1.5031721591949463\n",
      "Epoch: 1 \tBatch: 278 \tLoss: 1.4923386573791504\n",
      "Epoch: 1 \tBatch: 279 \tLoss: 1.4877979755401611\n",
      "Epoch: 1 \tBatch: 280 \tLoss: 1.4955240488052368\n",
      "Epoch: 1 \tBatch: 281 \tLoss: 1.4913601875305176\n",
      "Epoch: 1 \tBatch: 282 \tLoss: 1.5221060514450073\n",
      "Epoch: 1 \tBatch: 283 \tLoss: 1.5114378929138184\n",
      "Epoch: 1 \tBatch: 284 \tLoss: 1.5157322883605957\n",
      "Epoch: 1 \tBatch: 285 \tLoss: 1.4997094869613647\n",
      "Epoch: 1 \tBatch: 286 \tLoss: 1.4941946268081665\n",
      "Epoch: 1 \tBatch: 287 \tLoss: 1.4935439825057983\n",
      "Epoch: 1 \tBatch: 288 \tLoss: 1.5031205415725708\n",
      "Epoch: 1 \tBatch: 289 \tLoss: 1.5189019441604614\n",
      "Epoch: 1 \tBatch: 290 \tLoss: 1.4804503917694092\n",
      "Epoch: 1 \tBatch: 291 \tLoss: 1.5069912672042847\n",
      "Epoch: 1 \tBatch: 292 \tLoss: 1.4678938388824463\n",
      "Epoch: 1 \tBatch: 293 \tLoss: 1.5000033378601074\n",
      "Epoch: 1 \tBatch: 294 \tLoss: 1.4995759725570679\n",
      "Epoch: 1 \tBatch: 295 \tLoss: 1.4977614879608154\n",
      "Epoch: 1 \tBatch: 296 \tLoss: 1.4746214151382446\n",
      "Epoch: 1 \tBatch: 297 \tLoss: 1.5045466423034668\n",
      "Epoch: 1 \tBatch: 298 \tLoss: 1.5043662786483765\n",
      "Epoch: 1 \tBatch: 299 \tLoss: 1.5027023553848267\n",
      "Epoch: 1 \tBatch: 300 \tLoss: 1.4888771772384644\n",
      "Epoch: 1 \tBatch: 301 \tLoss: 1.5248218774795532\n",
      "Epoch: 1 \tBatch: 302 \tLoss: 1.4860877990722656\n",
      "Epoch: 1 \tBatch: 303 \tLoss: 1.5087217092514038\n",
      "Epoch: 1 \tBatch: 304 \tLoss: 1.4922319650650024\n",
      "Epoch: 1 \tBatch: 305 \tLoss: 1.5125677585601807\n",
      "Epoch: 1 \tBatch: 306 \tLoss: 1.5095410346984863\n",
      "Epoch: 1 \tBatch: 307 \tLoss: 1.4768198728561401\n",
      "Epoch: 1 \tBatch: 308 \tLoss: 1.4789786338806152\n",
      "Epoch: 1 \tBatch: 309 \tLoss: 1.473119854927063\n",
      "Epoch: 1 \tBatch: 310 \tLoss: 1.4826143980026245\n",
      "Epoch: 1 \tBatch: 311 \tLoss: 1.4974396228790283\n",
      "Epoch: 1 \tBatch: 312 \tLoss: 1.5070910453796387\n",
      "Epoch: 1 \tBatch: 313 \tLoss: 1.53534996509552\n",
      "Epoch: 1 \tBatch: 314 \tLoss: 1.4754462242126465\n",
      "Epoch: 1 \tBatch: 315 \tLoss: 1.4865505695343018\n",
      "Epoch: 1 \tBatch: 316 \tLoss: 1.4823697805404663\n",
      "Epoch: 1 \tBatch: 317 \tLoss: 1.486496090888977\n",
      "Epoch: 1 \tBatch: 318 \tLoss: 1.506150484085083\n",
      "Epoch: 1 \tBatch: 319 \tLoss: 1.50733482837677\n",
      "Epoch: 1 \tBatch: 320 \tLoss: 1.4987735748291016\n",
      "Epoch: 1 \tBatch: 321 \tLoss: 1.4893280267715454\n",
      "Epoch: 1 \tBatch: 322 \tLoss: 1.5172277688980103\n",
      "Epoch: 1 \tBatch: 323 \tLoss: 1.4815906286239624\n",
      "Epoch: 1 \tBatch: 324 \tLoss: 1.4972330331802368\n",
      "Epoch: 1 \tBatch: 325 \tLoss: 1.4976569414138794\n",
      "Epoch: 1 \tBatch: 326 \tLoss: 1.5032719373703003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 327 \tLoss: 1.4880386590957642\n",
      "Epoch: 1 \tBatch: 328 \tLoss: 1.5132272243499756\n",
      "Epoch: 1 \tBatch: 329 \tLoss: 1.470795750617981\n",
      "Epoch: 1 \tBatch: 330 \tLoss: 1.5130550861358643\n",
      "Epoch: 1 \tBatch: 331 \tLoss: 1.4897894859313965\n",
      "Epoch: 1 \tBatch: 332 \tLoss: 1.4806727170944214\n",
      "Epoch: 1 \tBatch: 333 \tLoss: 1.4911589622497559\n",
      "Epoch: 1 \tBatch: 334 \tLoss: 1.4865132570266724\n",
      "Epoch: 1 \tBatch: 335 \tLoss: 1.4993791580200195\n",
      "Epoch: 1 \tBatch: 336 \tLoss: 1.5105358362197876\n",
      "Epoch: 1 \tBatch: 337 \tLoss: 1.488644003868103\n",
      "Epoch: 1 \tBatch: 338 \tLoss: 1.4917911291122437\n",
      "Epoch: 1 \tBatch: 339 \tLoss: 1.5028972625732422\n",
      "Epoch: 1 \tBatch: 340 \tLoss: 1.4773188829421997\n",
      "Epoch: 1 \tBatch: 341 \tLoss: 1.4949651956558228\n",
      "Epoch: 1 \tBatch: 342 \tLoss: 1.4689890146255493\n",
      "Epoch: 1 \tBatch: 343 \tLoss: 1.5077959299087524\n",
      "Epoch: 1 \tBatch: 344 \tLoss: 1.4961084127426147\n",
      "Epoch: 1 \tBatch: 345 \tLoss: 1.4909074306488037\n",
      "Epoch: 1 \tBatch: 346 \tLoss: 1.4815117120742798\n",
      "Epoch: 1 \tBatch: 347 \tLoss: 1.4926986694335938\n",
      "Epoch: 1 \tBatch: 348 \tLoss: 1.4947195053100586\n",
      "Epoch: 1 \tBatch: 349 \tLoss: 1.492425799369812\n",
      "Epoch: 1 \tBatch: 350 \tLoss: 1.4898525476455688\n",
      "Epoch: 1 \tBatch: 351 \tLoss: 1.5075137615203857\n",
      "Epoch: 1 \tBatch: 352 \tLoss: 1.533111572265625\n",
      "Epoch: 1 \tBatch: 353 \tLoss: 1.4808098077774048\n",
      "Epoch: 1 \tBatch: 354 \tLoss: 1.4846765995025635\n",
      "Epoch: 1 \tBatch: 355 \tLoss: 1.4874227046966553\n",
      "Epoch: 1 \tBatch: 356 \tLoss: 1.4850982427597046\n",
      "Epoch: 1 \tBatch: 357 \tLoss: 1.484954595565796\n",
      "Epoch: 1 \tBatch: 358 \tLoss: 1.4835366010665894\n",
      "Epoch: 1 \tBatch: 359 \tLoss: 1.4942682981491089\n",
      "Epoch: 1 \tBatch: 360 \tLoss: 1.4929561614990234\n",
      "Epoch: 1 \tBatch: 361 \tLoss: 1.4944195747375488\n",
      "Epoch: 1 \tBatch: 362 \tLoss: 1.4840896129608154\n",
      "Epoch: 1 \tBatch: 363 \tLoss: 1.4688266515731812\n",
      "Epoch: 1 \tBatch: 364 \tLoss: 1.5185469388961792\n",
      "Epoch: 1 \tBatch: 365 \tLoss: 1.5051246881484985\n",
      "Epoch: 1 \tBatch: 366 \tLoss: 1.5100462436676025\n",
      "Epoch: 1 \tBatch: 367 \tLoss: 1.475711464881897\n",
      "Epoch: 1 \tBatch: 368 \tLoss: 1.4795539379119873\n",
      "Epoch: 1 \tBatch: 369 \tLoss: 1.4662760496139526\n",
      "Epoch: 1 \tBatch: 370 \tLoss: 1.4868899583816528\n",
      "Epoch: 1 \tBatch: 371 \tLoss: 1.4848793745040894\n",
      "Epoch: 1 \tBatch: 372 \tLoss: 1.476065993309021\n",
      "Epoch: 1 \tBatch: 373 \tLoss: 1.4841463565826416\n",
      "Epoch: 1 \tBatch: 374 \tLoss: 1.4779167175292969\n",
      "Epoch: 1 \tBatch: 375 \tLoss: 1.4930343627929688\n",
      "Epoch: 1 \tBatch: 376 \tLoss: 1.4859256744384766\n",
      "Epoch: 1 \tBatch: 377 \tLoss: 1.4968349933624268\n",
      "Epoch: 1 \tBatch: 378 \tLoss: 1.5036121606826782\n",
      "Epoch: 1 \tBatch: 379 \tLoss: 1.4938170909881592\n",
      "Epoch: 1 \tBatch: 380 \tLoss: 1.5121896266937256\n",
      "Epoch: 1 \tBatch: 381 \tLoss: 1.489964246749878\n",
      "Epoch: 1 \tBatch: 382 \tLoss: 1.4700355529785156\n",
      "Epoch: 1 \tBatch: 383 \tLoss: 1.5199377536773682\n",
      "Epoch: 1 \tBatch: 384 \tLoss: 1.482692837715149\n",
      "Epoch: 1 \tBatch: 385 \tLoss: 1.4857677221298218\n",
      "Epoch: 1 \tBatch: 386 \tLoss: 1.4694709777832031\n",
      "Epoch: 1 \tBatch: 387 \tLoss: 1.4873555898666382\n",
      "Epoch: 1 \tBatch: 388 \tLoss: 1.4922161102294922\n",
      "Epoch: 1 \tBatch: 389 \tLoss: 1.4897993803024292\n",
      "Epoch: 1 \tBatch: 390 \tLoss: 1.4996665716171265\n",
      "Epoch: 2 \tBatch: 0 \tLoss: 1.489726185798645\n",
      "Epoch: 2 \tBatch: 1 \tLoss: 1.4811071157455444\n",
      "Epoch: 2 \tBatch: 2 \tLoss: 1.491243839263916\n",
      "Epoch: 2 \tBatch: 3 \tLoss: 1.4937671422958374\n",
      "Epoch: 2 \tBatch: 4 \tLoss: 1.5171499252319336\n",
      "Epoch: 2 \tBatch: 5 \tLoss: 1.4934148788452148\n",
      "Epoch: 2 \tBatch: 6 \tLoss: 1.493180513381958\n",
      "Epoch: 2 \tBatch: 7 \tLoss: 1.4761340618133545\n",
      "Epoch: 2 \tBatch: 8 \tLoss: 1.4778485298156738\n",
      "Epoch: 2 \tBatch: 9 \tLoss: 1.478823184967041\n",
      "Epoch: 2 \tBatch: 10 \tLoss: 1.4972528219223022\n",
      "Epoch: 2 \tBatch: 11 \tLoss: 1.4755139350891113\n",
      "Epoch: 2 \tBatch: 12 \tLoss: 1.4835554361343384\n",
      "Epoch: 2 \tBatch: 13 \tLoss: 1.4779027700424194\n",
      "Epoch: 2 \tBatch: 14 \tLoss: 1.481857419013977\n",
      "Epoch: 2 \tBatch: 15 \tLoss: 1.4954631328582764\n",
      "Epoch: 2 \tBatch: 16 \tLoss: 1.4740464687347412\n",
      "Epoch: 2 \tBatch: 17 \tLoss: 1.4992341995239258\n",
      "Epoch: 2 \tBatch: 18 \tLoss: 1.4834972620010376\n",
      "Epoch: 2 \tBatch: 19 \tLoss: 1.4673553705215454\n",
      "Epoch: 2 \tBatch: 20 \tLoss: 1.4796019792556763\n",
      "Epoch: 2 \tBatch: 21 \tLoss: 1.4987694025039673\n",
      "Epoch: 2 \tBatch: 22 \tLoss: 1.4632762670516968\n",
      "Epoch: 2 \tBatch: 23 \tLoss: 1.4736852645874023\n",
      "Epoch: 2 \tBatch: 24 \tLoss: 1.4863605499267578\n",
      "Epoch: 2 \tBatch: 25 \tLoss: 1.4849109649658203\n",
      "Epoch: 2 \tBatch: 26 \tLoss: 1.4814701080322266\n",
      "Epoch: 2 \tBatch: 27 \tLoss: 1.4763543605804443\n",
      "Epoch: 2 \tBatch: 28 \tLoss: 1.4794615507125854\n",
      "Epoch: 2 \tBatch: 29 \tLoss: 1.4843966960906982\n",
      "Epoch: 2 \tBatch: 30 \tLoss: 1.5011497735977173\n",
      "Epoch: 2 \tBatch: 31 \tLoss: 1.4798392057418823\n",
      "Epoch: 2 \tBatch: 32 \tLoss: 1.4783620834350586\n",
      "Epoch: 2 \tBatch: 33 \tLoss: 1.4860270023345947\n",
      "Epoch: 2 \tBatch: 34 \tLoss: 1.4818997383117676\n",
      "Epoch: 2 \tBatch: 35 \tLoss: 1.4778296947479248\n",
      "Epoch: 2 \tBatch: 36 \tLoss: 1.51210355758667\n",
      "Epoch: 2 \tBatch: 37 \tLoss: 1.4986951351165771\n",
      "Epoch: 2 \tBatch: 38 \tLoss: 1.4946988821029663\n",
      "Epoch: 2 \tBatch: 39 \tLoss: 1.5077975988388062\n",
      "Epoch: 2 \tBatch: 40 \tLoss: 1.4914069175720215\n",
      "Epoch: 2 \tBatch: 41 \tLoss: 1.4825022220611572\n",
      "Epoch: 2 \tBatch: 42 \tLoss: 1.4929648637771606\n",
      "Epoch: 2 \tBatch: 43 \tLoss: 1.4679731130599976\n",
      "Epoch: 2 \tBatch: 44 \tLoss: 1.4704689979553223\n",
      "Epoch: 2 \tBatch: 45 \tLoss: 1.494471549987793\n",
      "Epoch: 2 \tBatch: 46 \tLoss: 1.4773163795471191\n",
      "Epoch: 2 \tBatch: 47 \tLoss: 1.4803264141082764\n",
      "Epoch: 2 \tBatch: 48 \tLoss: 1.5000795125961304\n",
      "Epoch: 2 \tBatch: 49 \tLoss: 1.491986870765686\n",
      "Epoch: 2 \tBatch: 50 \tLoss: 1.4950841665267944\n",
      "Epoch: 2 \tBatch: 51 \tLoss: 1.4958586692810059\n",
      "Epoch: 2 \tBatch: 52 \tLoss: 1.5064165592193604\n",
      "Epoch: 2 \tBatch: 53 \tLoss: 1.4757634401321411\n",
      "Epoch: 2 \tBatch: 54 \tLoss: 1.504494071006775\n",
      "Epoch: 2 \tBatch: 55 \tLoss: 1.4856138229370117\n",
      "Epoch: 2 \tBatch: 56 \tLoss: 1.4746042490005493\n",
      "Epoch: 2 \tBatch: 57 \tLoss: 1.5002257823944092\n",
      "Epoch: 2 \tBatch: 58 \tLoss: 1.481627345085144\n",
      "Epoch: 2 \tBatch: 59 \tLoss: 1.4717012643814087\n",
      "Epoch: 2 \tBatch: 60 \tLoss: 1.4821935892105103\n",
      "Epoch: 2 \tBatch: 61 \tLoss: 1.4732974767684937\n",
      "Epoch: 2 \tBatch: 62 \tLoss: 1.4855198860168457\n",
      "Epoch: 2 \tBatch: 63 \tLoss: 1.4832290410995483\n",
      "Epoch: 2 \tBatch: 64 \tLoss: 1.4843847751617432\n",
      "Epoch: 2 \tBatch: 65 \tLoss: 1.4785884618759155\n",
      "Epoch: 2 \tBatch: 66 \tLoss: 1.4699962139129639\n",
      "Epoch: 2 \tBatch: 67 \tLoss: 1.4951287508010864\n",
      "Epoch: 2 \tBatch: 68 \tLoss: 1.4697258472442627\n",
      "Epoch: 2 \tBatch: 69 \tLoss: 1.4954930543899536\n",
      "Epoch: 2 \tBatch: 70 \tLoss: 1.5127512216567993\n",
      "Epoch: 2 \tBatch: 71 \tLoss: 1.4672948122024536\n",
      "Epoch: 2 \tBatch: 72 \tLoss: 1.4960167407989502\n",
      "Epoch: 2 \tBatch: 73 \tLoss: 1.4719960689544678\n",
      "Epoch: 2 \tBatch: 74 \tLoss: 1.4916009902954102\n",
      "Epoch: 2 \tBatch: 75 \tLoss: 1.4631385803222656\n",
      "Epoch: 2 \tBatch: 76 \tLoss: 1.4961766004562378\n",
      "Epoch: 2 \tBatch: 77 \tLoss: 1.4877619743347168\n",
      "Epoch: 2 \tBatch: 78 \tLoss: 1.4809298515319824\n",
      "Epoch: 2 \tBatch: 79 \tLoss: 1.5051268339157104\n",
      "Epoch: 2 \tBatch: 80 \tLoss: 1.5156704187393188\n",
      "Epoch: 2 \tBatch: 81 \tLoss: 1.4701851606369019\n",
      "Epoch: 2 \tBatch: 82 \tLoss: 1.4891711473464966\n",
      "Epoch: 2 \tBatch: 83 \tLoss: 1.4808783531188965\n",
      "Epoch: 2 \tBatch: 84 \tLoss: 1.4751805067062378\n",
      "Epoch: 2 \tBatch: 85 \tLoss: 1.502984642982483\n",
      "Epoch: 2 \tBatch: 86 \tLoss: 1.4982397556304932\n",
      "Epoch: 2 \tBatch: 87 \tLoss: 1.5140436887741089\n",
      "Epoch: 2 \tBatch: 88 \tLoss: 1.484458088874817\n",
      "Epoch: 2 \tBatch: 89 \tLoss: 1.4891334772109985\n",
      "Epoch: 2 \tBatch: 90 \tLoss: 1.4769279956817627\n",
      "Epoch: 2 \tBatch: 91 \tLoss: 1.4919941425323486\n",
      "Epoch: 2 \tBatch: 92 \tLoss: 1.477967381477356\n",
      "Epoch: 2 \tBatch: 93 \tLoss: 1.498879075050354\n",
      "Epoch: 2 \tBatch: 94 \tLoss: 1.4845116138458252\n",
      "Epoch: 2 \tBatch: 95 \tLoss: 1.4869691133499146\n",
      "Epoch: 2 \tBatch: 96 \tLoss: 1.481778860092163\n",
      "Epoch: 2 \tBatch: 97 \tLoss: 1.4744236469268799\n",
      "Epoch: 2 \tBatch: 98 \tLoss: 1.4890507459640503\n",
      "Epoch: 2 \tBatch: 99 \tLoss: 1.485468864440918\n",
      "Epoch: 2 \tBatch: 100 \tLoss: 1.4855530261993408\n",
      "Epoch: 2 \tBatch: 101 \tLoss: 1.5098254680633545\n",
      "Epoch: 2 \tBatch: 102 \tLoss: 1.4778287410736084\n",
      "Epoch: 2 \tBatch: 103 \tLoss: 1.4939181804656982\n",
      "Epoch: 2 \tBatch: 104 \tLoss: 1.4762542247772217\n",
      "Epoch: 2 \tBatch: 105 \tLoss: 1.4880493879318237\n",
      "Epoch: 2 \tBatch: 106 \tLoss: 1.4843432903289795\n",
      "Epoch: 2 \tBatch: 107 \tLoss: 1.4956707954406738\n",
      "Epoch: 2 \tBatch: 108 \tLoss: 1.4786103963851929\n",
      "Epoch: 2 \tBatch: 109 \tLoss: 1.4729946851730347\n",
      "Epoch: 2 \tBatch: 110 \tLoss: 1.4785298109054565\n",
      "Epoch: 2 \tBatch: 111 \tLoss: 1.4734069108963013\n",
      "Epoch: 2 \tBatch: 112 \tLoss: 1.497501254081726\n",
      "Epoch: 2 \tBatch: 113 \tLoss: 1.490207552909851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tBatch: 114 \tLoss: 1.4940974712371826\n",
      "Epoch: 2 \tBatch: 115 \tLoss: 1.4876835346221924\n",
      "Epoch: 2 \tBatch: 116 \tLoss: 1.4726943969726562\n",
      "Epoch: 2 \tBatch: 117 \tLoss: 1.4894124269485474\n",
      "Epoch: 2 \tBatch: 118 \tLoss: 1.4640047550201416\n",
      "Epoch: 2 \tBatch: 119 \tLoss: 1.4944877624511719\n",
      "Epoch: 2 \tBatch: 120 \tLoss: 1.4910838603973389\n",
      "Epoch: 2 \tBatch: 121 \tLoss: 1.4855592250823975\n",
      "Epoch: 2 \tBatch: 122 \tLoss: 1.5055603981018066\n",
      "Epoch: 2 \tBatch: 123 \tLoss: 1.490584373474121\n",
      "Epoch: 2 \tBatch: 124 \tLoss: 1.4693844318389893\n",
      "Epoch: 2 \tBatch: 125 \tLoss: 1.4758678674697876\n",
      "Epoch: 2 \tBatch: 126 \tLoss: 1.498379111289978\n",
      "Epoch: 2 \tBatch: 127 \tLoss: 1.4939887523651123\n",
      "Epoch: 2 \tBatch: 128 \tLoss: 1.4842689037322998\n",
      "Epoch: 2 \tBatch: 129 \tLoss: 1.5112112760543823\n",
      "Epoch: 2 \tBatch: 130 \tLoss: 1.4805067777633667\n",
      "Epoch: 2 \tBatch: 131 \tLoss: 1.500488042831421\n",
      "Epoch: 2 \tBatch: 132 \tLoss: 1.4874656200408936\n",
      "Epoch: 2 \tBatch: 133 \tLoss: 1.4723318815231323\n",
      "Epoch: 2 \tBatch: 134 \tLoss: 1.489522099494934\n",
      "Epoch: 2 \tBatch: 135 \tLoss: 1.501686453819275\n",
      "Epoch: 2 \tBatch: 136 \tLoss: 1.4754772186279297\n",
      "Epoch: 2 \tBatch: 137 \tLoss: 1.4819729328155518\n",
      "Epoch: 2 \tBatch: 138 \tLoss: 1.4714241027832031\n",
      "Epoch: 2 \tBatch: 139 \tLoss: 1.4634780883789062\n",
      "Epoch: 2 \tBatch: 140 \tLoss: 1.4697239398956299\n",
      "Epoch: 2 \tBatch: 141 \tLoss: 1.48707115650177\n",
      "Epoch: 2 \tBatch: 142 \tLoss: 1.4633188247680664\n",
      "Epoch: 2 \tBatch: 143 \tLoss: 1.4936537742614746\n",
      "Epoch: 2 \tBatch: 144 \tLoss: 1.4815391302108765\n",
      "Epoch: 2 \tBatch: 145 \tLoss: 1.4874663352966309\n",
      "Epoch: 2 \tBatch: 146 \tLoss: 1.4823862314224243\n",
      "Epoch: 2 \tBatch: 147 \tLoss: 1.480748176574707\n",
      "Epoch: 2 \tBatch: 148 \tLoss: 1.509363055229187\n",
      "Epoch: 2 \tBatch: 149 \tLoss: 1.499065637588501\n",
      "Epoch: 2 \tBatch: 150 \tLoss: 1.5101417303085327\n",
      "Epoch: 2 \tBatch: 151 \tLoss: 1.481881856918335\n",
      "Epoch: 2 \tBatch: 152 \tLoss: 1.517172932624817\n",
      "Epoch: 2 \tBatch: 153 \tLoss: 1.4849568605422974\n",
      "Epoch: 2 \tBatch: 154 \tLoss: 1.4820712804794312\n",
      "Epoch: 2 \tBatch: 155 \tLoss: 1.5047518014907837\n",
      "Epoch: 2 \tBatch: 156 \tLoss: 1.4838289022445679\n",
      "Epoch: 2 \tBatch: 157 \tLoss: 1.4884023666381836\n",
      "Epoch: 2 \tBatch: 158 \tLoss: 1.5130541324615479\n",
      "Epoch: 2 \tBatch: 159 \tLoss: 1.481698751449585\n",
      "Epoch: 2 \tBatch: 160 \tLoss: 1.4796998500823975\n",
      "Epoch: 2 \tBatch: 161 \tLoss: 1.4721300601959229\n",
      "Epoch: 2 \tBatch: 162 \tLoss: 1.4770772457122803\n",
      "Epoch: 2 \tBatch: 163 \tLoss: 1.514089584350586\n",
      "Epoch: 2 \tBatch: 164 \tLoss: 1.5146230459213257\n",
      "Epoch: 2 \tBatch: 165 \tLoss: 1.4806870222091675\n",
      "Epoch: 2 \tBatch: 166 \tLoss: 1.5107342004776\n",
      "Epoch: 2 \tBatch: 167 \tLoss: 1.4968458414077759\n",
      "Epoch: 2 \tBatch: 168 \tLoss: 1.4936599731445312\n",
      "Epoch: 2 \tBatch: 169 \tLoss: 1.4839743375778198\n",
      "Epoch: 2 \tBatch: 170 \tLoss: 1.4937183856964111\n",
      "Epoch: 2 \tBatch: 171 \tLoss: 1.4920153617858887\n",
      "Epoch: 2 \tBatch: 172 \tLoss: 1.482092022895813\n",
      "Epoch: 2 \tBatch: 173 \tLoss: 1.4682371616363525\n",
      "Epoch: 2 \tBatch: 174 \tLoss: 1.4899653196334839\n",
      "Epoch: 2 \tBatch: 175 \tLoss: 1.492450475692749\n",
      "Epoch: 2 \tBatch: 176 \tLoss: 1.503250241279602\n",
      "Epoch: 2 \tBatch: 177 \tLoss: 1.497270107269287\n",
      "Epoch: 2 \tBatch: 178 \tLoss: 1.469452977180481\n",
      "Epoch: 2 \tBatch: 179 \tLoss: 1.4663888216018677\n",
      "Epoch: 2 \tBatch: 180 \tLoss: 1.515383243560791\n",
      "Epoch: 2 \tBatch: 181 \tLoss: 1.5012272596359253\n",
      "Epoch: 2 \tBatch: 182 \tLoss: 1.4873545169830322\n",
      "Epoch: 2 \tBatch: 183 \tLoss: 1.4848525524139404\n",
      "Epoch: 2 \tBatch: 184 \tLoss: 1.4877725839614868\n",
      "Epoch: 2 \tBatch: 185 \tLoss: 1.4698985815048218\n",
      "Epoch: 2 \tBatch: 186 \tLoss: 1.493472695350647\n",
      "Epoch: 2 \tBatch: 187 \tLoss: 1.4927994012832642\n",
      "Epoch: 2 \tBatch: 188 \tLoss: 1.4644124507904053\n",
      "Epoch: 2 \tBatch: 189 \tLoss: 1.4760342836380005\n",
      "Epoch: 2 \tBatch: 190 \tLoss: 1.494668960571289\n",
      "Epoch: 2 \tBatch: 191 \tLoss: 1.4767574071884155\n",
      "Epoch: 2 \tBatch: 192 \tLoss: 1.4905054569244385\n",
      "Epoch: 2 \tBatch: 193 \tLoss: 1.4822955131530762\n",
      "Epoch: 2 \tBatch: 194 \tLoss: 1.4885164499282837\n",
      "Epoch: 2 \tBatch: 195 \tLoss: 1.4660766124725342\n",
      "Epoch: 2 \tBatch: 196 \tLoss: 1.4851258993148804\n",
      "Epoch: 2 \tBatch: 197 \tLoss: 1.4769352674484253\n",
      "Epoch: 2 \tBatch: 198 \tLoss: 1.4843968152999878\n",
      "Epoch: 2 \tBatch: 199 \tLoss: 1.4917054176330566\n",
      "Epoch: 2 \tBatch: 200 \tLoss: 1.4880609512329102\n",
      "Epoch: 2 \tBatch: 201 \tLoss: 1.4672306776046753\n",
      "Epoch: 2 \tBatch: 202 \tLoss: 1.4997390508651733\n",
      "Epoch: 2 \tBatch: 203 \tLoss: 1.475599765777588\n",
      "Epoch: 2 \tBatch: 204 \tLoss: 1.480129599571228\n",
      "Epoch: 2 \tBatch: 205 \tLoss: 1.4827682971954346\n",
      "Epoch: 2 \tBatch: 206 \tLoss: 1.4657801389694214\n",
      "Epoch: 2 \tBatch: 207 \tLoss: 1.469610571861267\n",
      "Epoch: 2 \tBatch: 208 \tLoss: 1.4813591241836548\n",
      "Epoch: 2 \tBatch: 209 \tLoss: 1.4847086668014526\n",
      "Epoch: 2 \tBatch: 210 \tLoss: 1.4820590019226074\n",
      "Epoch: 2 \tBatch: 211 \tLoss: 1.509615182876587\n",
      "Epoch: 2 \tBatch: 212 \tLoss: 1.482919454574585\n",
      "Epoch: 2 \tBatch: 213 \tLoss: 1.4746136665344238\n",
      "Epoch: 2 \tBatch: 214 \tLoss: 1.5084315538406372\n",
      "Epoch: 2 \tBatch: 215 \tLoss: 1.4891074895858765\n",
      "Epoch: 2 \tBatch: 216 \tLoss: 1.4698429107666016\n",
      "Epoch: 2 \tBatch: 217 \tLoss: 1.483015537261963\n",
      "Epoch: 2 \tBatch: 218 \tLoss: 1.476984977722168\n",
      "Epoch: 2 \tBatch: 219 \tLoss: 1.4924789667129517\n",
      "Epoch: 2 \tBatch: 220 \tLoss: 1.4814114570617676\n",
      "Epoch: 2 \tBatch: 221 \tLoss: 1.4911224842071533\n",
      "Epoch: 2 \tBatch: 222 \tLoss: 1.5084483623504639\n",
      "Epoch: 2 \tBatch: 223 \tLoss: 1.5084151029586792\n",
      "Epoch: 2 \tBatch: 224 \tLoss: 1.4997411966323853\n",
      "Epoch: 2 \tBatch: 225 \tLoss: 1.485089898109436\n",
      "Epoch: 2 \tBatch: 226 \tLoss: 1.4973459243774414\n",
      "Epoch: 2 \tBatch: 227 \tLoss: 1.500105857849121\n",
      "Epoch: 2 \tBatch: 228 \tLoss: 1.4793840646743774\n",
      "Epoch: 2 \tBatch: 229 \tLoss: 1.492299199104309\n",
      "Epoch: 2 \tBatch: 230 \tLoss: 1.4843286275863647\n",
      "Epoch: 2 \tBatch: 231 \tLoss: 1.4746806621551514\n",
      "Epoch: 2 \tBatch: 232 \tLoss: 1.4813019037246704\n",
      "Epoch: 2 \tBatch: 233 \tLoss: 1.4980461597442627\n",
      "Epoch: 2 \tBatch: 234 \tLoss: 1.492512822151184\n",
      "Epoch: 2 \tBatch: 235 \tLoss: 1.5017309188842773\n",
      "Epoch: 2 \tBatch: 236 \tLoss: 1.5022670030593872\n",
      "Epoch: 2 \tBatch: 237 \tLoss: 1.4670873880386353\n",
      "Epoch: 2 \tBatch: 238 \tLoss: 1.466709017753601\n",
      "Epoch: 2 \tBatch: 239 \tLoss: 1.4686732292175293\n",
      "Epoch: 2 \tBatch: 240 \tLoss: 1.4901102781295776\n",
      "Epoch: 2 \tBatch: 241 \tLoss: 1.4930038452148438\n",
      "Epoch: 2 \tBatch: 242 \tLoss: 1.5089386701583862\n",
      "Epoch: 2 \tBatch: 243 \tLoss: 1.482106328010559\n",
      "Epoch: 2 \tBatch: 244 \tLoss: 1.489990234375\n",
      "Epoch: 2 \tBatch: 245 \tLoss: 1.4796760082244873\n",
      "Epoch: 2 \tBatch: 246 \tLoss: 1.4999207258224487\n",
      "Epoch: 2 \tBatch: 247 \tLoss: 1.4845927953720093\n",
      "Epoch: 2 \tBatch: 248 \tLoss: 1.4781521558761597\n",
      "Epoch: 2 \tBatch: 249 \tLoss: 1.471640944480896\n",
      "Epoch: 2 \tBatch: 250 \tLoss: 1.4920010566711426\n",
      "Epoch: 2 \tBatch: 251 \tLoss: 1.4828016757965088\n",
      "Epoch: 2 \tBatch: 252 \tLoss: 1.4798860549926758\n",
      "Epoch: 2 \tBatch: 253 \tLoss: 1.4846715927124023\n",
      "Epoch: 2 \tBatch: 254 \tLoss: 1.4785674810409546\n",
      "Epoch: 2 \tBatch: 255 \tLoss: 1.5056061744689941\n",
      "Epoch: 2 \tBatch: 256 \tLoss: 1.4738517999649048\n",
      "Epoch: 2 \tBatch: 257 \tLoss: 1.4725635051727295\n",
      "Epoch: 2 \tBatch: 258 \tLoss: 1.4733532667160034\n",
      "Epoch: 2 \tBatch: 259 \tLoss: 1.4956650733947754\n",
      "Epoch: 2 \tBatch: 260 \tLoss: 1.5028115510940552\n",
      "Epoch: 2 \tBatch: 261 \tLoss: 1.5007244348526\n",
      "Epoch: 2 \tBatch: 262 \tLoss: 1.5012848377227783\n",
      "Epoch: 2 \tBatch: 263 \tLoss: 1.4697822332382202\n",
      "Epoch: 2 \tBatch: 264 \tLoss: 1.4867968559265137\n",
      "Epoch: 2 \tBatch: 265 \tLoss: 1.4817664623260498\n",
      "Epoch: 2 \tBatch: 266 \tLoss: 1.5000427961349487\n",
      "Epoch: 2 \tBatch: 267 \tLoss: 1.4864554405212402\n",
      "Epoch: 2 \tBatch: 268 \tLoss: 1.4736868143081665\n",
      "Epoch: 2 \tBatch: 269 \tLoss: 1.476747751235962\n",
      "Epoch: 2 \tBatch: 270 \tLoss: 1.4846948385238647\n",
      "Epoch: 2 \tBatch: 271 \tLoss: 1.4802041053771973\n",
      "Epoch: 2 \tBatch: 272 \tLoss: 1.4673622846603394\n",
      "Epoch: 2 \tBatch: 273 \tLoss: 1.4738153219223022\n",
      "Epoch: 2 \tBatch: 274 \tLoss: 1.4806464910507202\n",
      "Epoch: 2 \tBatch: 275 \tLoss: 1.5021991729736328\n",
      "Epoch: 2 \tBatch: 276 \tLoss: 1.4803526401519775\n",
      "Epoch: 2 \tBatch: 277 \tLoss: 1.49156653881073\n",
      "Epoch: 2 \tBatch: 278 \tLoss: 1.5031903982162476\n",
      "Epoch: 2 \tBatch: 279 \tLoss: 1.4696367979049683\n",
      "Epoch: 2 \tBatch: 280 \tLoss: 1.4859572649002075\n",
      "Epoch: 2 \tBatch: 281 \tLoss: 1.4675698280334473\n",
      "Epoch: 2 \tBatch: 282 \tLoss: 1.5136704444885254\n",
      "Epoch: 2 \tBatch: 283 \tLoss: 1.4655015468597412\n",
      "Epoch: 2 \tBatch: 284 \tLoss: 1.4922740459442139\n",
      "Epoch: 2 \tBatch: 285 \tLoss: 1.491286277770996\n",
      "Epoch: 2 \tBatch: 286 \tLoss: 1.5083034038543701\n",
      "Epoch: 2 \tBatch: 287 \tLoss: 1.4836467504501343\n",
      "Epoch: 2 \tBatch: 288 \tLoss: 1.4821665287017822\n",
      "Epoch: 2 \tBatch: 289 \tLoss: 1.4709070920944214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tBatch: 290 \tLoss: 1.4624625444412231\n",
      "Epoch: 2 \tBatch: 291 \tLoss: 1.4779925346374512\n",
      "Epoch: 2 \tBatch: 292 \tLoss: 1.482820987701416\n",
      "Epoch: 2 \tBatch: 293 \tLoss: 1.472356915473938\n",
      "Epoch: 2 \tBatch: 294 \tLoss: 1.4877655506134033\n",
      "Epoch: 2 \tBatch: 295 \tLoss: 1.486107349395752\n",
      "Epoch: 2 \tBatch: 296 \tLoss: 1.503881812095642\n",
      "Epoch: 2 \tBatch: 297 \tLoss: 1.4763755798339844\n",
      "Epoch: 2 \tBatch: 298 \tLoss: 1.4889986515045166\n",
      "Epoch: 2 \tBatch: 299 \tLoss: 1.4991083145141602\n",
      "Epoch: 2 \tBatch: 300 \tLoss: 1.4760527610778809\n",
      "Epoch: 2 \tBatch: 301 \tLoss: 1.4883044958114624\n",
      "Epoch: 2 \tBatch: 302 \tLoss: 1.5019409656524658\n",
      "Epoch: 2 \tBatch: 303 \tLoss: 1.4779942035675049\n",
      "Epoch: 2 \tBatch: 304 \tLoss: 1.4907809495925903\n",
      "Epoch: 2 \tBatch: 305 \tLoss: 1.4792728424072266\n",
      "Epoch: 2 \tBatch: 306 \tLoss: 1.4803035259246826\n",
      "Epoch: 2 \tBatch: 307 \tLoss: 1.4847394227981567\n",
      "Epoch: 2 \tBatch: 308 \tLoss: 1.4768165349960327\n",
      "Epoch: 2 \tBatch: 309 \tLoss: 1.4837076663970947\n",
      "Epoch: 2 \tBatch: 310 \tLoss: 1.4884113073349\n",
      "Epoch: 2 \tBatch: 311 \tLoss: 1.4883732795715332\n",
      "Epoch: 2 \tBatch: 312 \tLoss: 1.487436294555664\n",
      "Epoch: 2 \tBatch: 313 \tLoss: 1.4654146432876587\n",
      "Epoch: 2 \tBatch: 314 \tLoss: 1.4784119129180908\n",
      "Epoch: 2 \tBatch: 315 \tLoss: 1.4927266836166382\n",
      "Epoch: 2 \tBatch: 316 \tLoss: 1.4772531986236572\n",
      "Epoch: 2 \tBatch: 317 \tLoss: 1.4700326919555664\n",
      "Epoch: 2 \tBatch: 318 \tLoss: 1.4651278257369995\n",
      "Epoch: 2 \tBatch: 319 \tLoss: 1.4718519449234009\n",
      "Epoch: 2 \tBatch: 320 \tLoss: 1.4782341718673706\n",
      "Epoch: 2 \tBatch: 321 \tLoss: 1.486886739730835\n",
      "Epoch: 2 \tBatch: 322 \tLoss: 1.5132081508636475\n",
      "Epoch: 2 \tBatch: 323 \tLoss: 1.4959585666656494\n",
      "Epoch: 2 \tBatch: 324 \tLoss: 1.4910800457000732\n",
      "Epoch: 2 \tBatch: 325 \tLoss: 1.4743633270263672\n",
      "Epoch: 2 \tBatch: 326 \tLoss: 1.5112836360931396\n",
      "Epoch: 2 \tBatch: 327 \tLoss: 1.469796061515808\n",
      "Epoch: 2 \tBatch: 328 \tLoss: 1.4955224990844727\n",
      "Epoch: 2 \tBatch: 329 \tLoss: 1.4712399244308472\n",
      "Epoch: 2 \tBatch: 330 \tLoss: 1.468977928161621\n",
      "Epoch: 2 \tBatch: 331 \tLoss: 1.5163633823394775\n",
      "Epoch: 2 \tBatch: 332 \tLoss: 1.4848359823226929\n",
      "Epoch: 2 \tBatch: 333 \tLoss: 1.487362265586853\n",
      "Epoch: 2 \tBatch: 334 \tLoss: 1.5119876861572266\n",
      "Epoch: 2 \tBatch: 335 \tLoss: 1.5015586614608765\n",
      "Epoch: 2 \tBatch: 336 \tLoss: 1.4898215532302856\n",
      "Epoch: 2 \tBatch: 337 \tLoss: 1.4867712259292603\n",
      "Epoch: 2 \tBatch: 338 \tLoss: 1.5163711309432983\n",
      "Epoch: 2 \tBatch: 339 \tLoss: 1.4809902906417847\n",
      "Epoch: 2 \tBatch: 340 \tLoss: 1.4708484411239624\n",
      "Epoch: 2 \tBatch: 341 \tLoss: 1.5062379837036133\n",
      "Epoch: 2 \tBatch: 342 \tLoss: 1.4963393211364746\n",
      "Epoch: 2 \tBatch: 343 \tLoss: 1.4639546871185303\n",
      "Epoch: 2 \tBatch: 344 \tLoss: 1.4954500198364258\n",
      "Epoch: 2 \tBatch: 345 \tLoss: 1.4650795459747314\n",
      "Epoch: 2 \tBatch: 346 \tLoss: 1.4838488101959229\n",
      "Epoch: 2 \tBatch: 347 \tLoss: 1.4757088422775269\n",
      "Epoch: 2 \tBatch: 348 \tLoss: 1.4859192371368408\n",
      "Epoch: 2 \tBatch: 349 \tLoss: 1.4803427457809448\n",
      "Epoch: 2 \tBatch: 350 \tLoss: 1.4791758060455322\n",
      "Epoch: 2 \tBatch: 351 \tLoss: 1.4707080125808716\n",
      "Epoch: 2 \tBatch: 352 \tLoss: 1.485573172569275\n",
      "Epoch: 2 \tBatch: 353 \tLoss: 1.4777796268463135\n",
      "Epoch: 2 \tBatch: 354 \tLoss: 1.4836302995681763\n",
      "Epoch: 2 \tBatch: 355 \tLoss: 1.4721988439559937\n",
      "Epoch: 2 \tBatch: 356 \tLoss: 1.4839764833450317\n",
      "Epoch: 2 \tBatch: 357 \tLoss: 1.4765563011169434\n",
      "Epoch: 2 \tBatch: 358 \tLoss: 1.5351717472076416\n",
      "Epoch: 2 \tBatch: 359 \tLoss: 1.496830701828003\n",
      "Epoch: 2 \tBatch: 360 \tLoss: 1.501084327697754\n",
      "Epoch: 2 \tBatch: 361 \tLoss: 1.4843065738677979\n",
      "Epoch: 2 \tBatch: 362 \tLoss: 1.4780210256576538\n",
      "Epoch: 2 \tBatch: 363 \tLoss: 1.4916510581970215\n",
      "Epoch: 2 \tBatch: 364 \tLoss: 1.5121490955352783\n",
      "Epoch: 2 \tBatch: 365 \tLoss: 1.472961187362671\n",
      "Epoch: 2 \tBatch: 366 \tLoss: 1.4971790313720703\n",
      "Epoch: 2 \tBatch: 367 \tLoss: 1.4760595560073853\n",
      "Epoch: 2 \tBatch: 368 \tLoss: 1.4927184581756592\n",
      "Epoch: 2 \tBatch: 369 \tLoss: 1.5086733102798462\n",
      "Epoch: 2 \tBatch: 370 \tLoss: 1.478682279586792\n",
      "Epoch: 2 \tBatch: 371 \tLoss: 1.4895397424697876\n",
      "Epoch: 2 \tBatch: 372 \tLoss: 1.4944956302642822\n",
      "Epoch: 2 \tBatch: 373 \tLoss: 1.486061453819275\n",
      "Epoch: 2 \tBatch: 374 \tLoss: 1.485040545463562\n",
      "Epoch: 2 \tBatch: 375 \tLoss: 1.4787243604660034\n",
      "Epoch: 2 \tBatch: 376 \tLoss: 1.479581594467163\n",
      "Epoch: 2 \tBatch: 377 \tLoss: 1.4729976654052734\n",
      "Epoch: 2 \tBatch: 378 \tLoss: 1.4895621538162231\n",
      "Epoch: 2 \tBatch: 379 \tLoss: 1.4657881259918213\n",
      "Epoch: 2 \tBatch: 380 \tLoss: 1.4632710218429565\n",
      "Epoch: 2 \tBatch: 381 \tLoss: 1.4751836061477661\n",
      "Epoch: 2 \tBatch: 382 \tLoss: 1.5187352895736694\n",
      "Epoch: 2 \tBatch: 383 \tLoss: 1.505800485610962\n",
      "Epoch: 2 \tBatch: 384 \tLoss: 1.5144773721694946\n",
      "Epoch: 2 \tBatch: 385 \tLoss: 1.4658907651901245\n",
      "Epoch: 2 \tBatch: 386 \tLoss: 1.4848012924194336\n",
      "Epoch: 2 \tBatch: 387 \tLoss: 1.4759143590927124\n",
      "Epoch: 2 \tBatch: 388 \tLoss: 1.4745490550994873\n",
      "Epoch: 2 \tBatch: 389 \tLoss: 1.492645263671875\n",
      "Epoch: 2 \tBatch: 390 \tLoss: 1.4904391765594482\n",
      "Epoch: 3 \tBatch: 0 \tLoss: 1.4740592241287231\n",
      "Epoch: 3 \tBatch: 1 \tLoss: 1.4815208911895752\n",
      "Epoch: 3 \tBatch: 2 \tLoss: 1.495296835899353\n",
      "Epoch: 3 \tBatch: 3 \tLoss: 1.4730160236358643\n",
      "Epoch: 3 \tBatch: 4 \tLoss: 1.4862672090530396\n",
      "Epoch: 3 \tBatch: 5 \tLoss: 1.4867955446243286\n",
      "Epoch: 3 \tBatch: 6 \tLoss: 1.4961012601852417\n",
      "Epoch: 3 \tBatch: 7 \tLoss: 1.4870282411575317\n",
      "Epoch: 3 \tBatch: 8 \tLoss: 1.490623950958252\n",
      "Epoch: 3 \tBatch: 9 \tLoss: 1.4822748899459839\n",
      "Epoch: 3 \tBatch: 10 \tLoss: 1.4841067790985107\n",
      "Epoch: 3 \tBatch: 11 \tLoss: 1.4629236459732056\n",
      "Epoch: 3 \tBatch: 12 \tLoss: 1.4840521812438965\n",
      "Epoch: 3 \tBatch: 13 \tLoss: 1.492915391921997\n",
      "Epoch: 3 \tBatch: 14 \tLoss: 1.48380446434021\n",
      "Epoch: 3 \tBatch: 15 \tLoss: 1.4694428443908691\n",
      "Epoch: 3 \tBatch: 16 \tLoss: 1.5127671957015991\n",
      "Epoch: 3 \tBatch: 17 \tLoss: 1.4827558994293213\n",
      "Epoch: 3 \tBatch: 18 \tLoss: 1.492516040802002\n",
      "Epoch: 3 \tBatch: 19 \tLoss: 1.465437889099121\n",
      "Epoch: 3 \tBatch: 20 \tLoss: 1.4742748737335205\n",
      "Epoch: 3 \tBatch: 21 \tLoss: 1.4724552631378174\n",
      "Epoch: 3 \tBatch: 22 \tLoss: 1.4807881116867065\n",
      "Epoch: 3 \tBatch: 23 \tLoss: 1.5022028684616089\n",
      "Epoch: 3 \tBatch: 24 \tLoss: 1.480431318283081\n",
      "Epoch: 3 \tBatch: 25 \tLoss: 1.4833885431289673\n",
      "Epoch: 3 \tBatch: 26 \tLoss: 1.504496455192566\n",
      "Epoch: 3 \tBatch: 27 \tLoss: 1.4866323471069336\n",
      "Epoch: 3 \tBatch: 28 \tLoss: 1.479263424873352\n",
      "Epoch: 3 \tBatch: 29 \tLoss: 1.4758094549179077\n",
      "Epoch: 3 \tBatch: 30 \tLoss: 1.4929907321929932\n",
      "Epoch: 3 \tBatch: 31 \tLoss: 1.5088071823120117\n",
      "Epoch: 3 \tBatch: 32 \tLoss: 1.4628008604049683\n",
      "Epoch: 3 \tBatch: 33 \tLoss: 1.4882150888442993\n",
      "Epoch: 3 \tBatch: 34 \tLoss: 1.493757724761963\n",
      "Epoch: 3 \tBatch: 35 \tLoss: 1.489997386932373\n",
      "Epoch: 3 \tBatch: 36 \tLoss: 1.4782028198242188\n",
      "Epoch: 3 \tBatch: 37 \tLoss: 1.488643765449524\n",
      "Epoch: 3 \tBatch: 38 \tLoss: 1.4850507974624634\n",
      "Epoch: 3 \tBatch: 39 \tLoss: 1.5252504348754883\n",
      "Epoch: 3 \tBatch: 40 \tLoss: 1.4816964864730835\n",
      "Epoch: 3 \tBatch: 41 \tLoss: 1.494519591331482\n",
      "Epoch: 3 \tBatch: 42 \tLoss: 1.4796231985092163\n",
      "Epoch: 3 \tBatch: 43 \tLoss: 1.4841071367263794\n",
      "Epoch: 3 \tBatch: 44 \tLoss: 1.477789282798767\n",
      "Epoch: 3 \tBatch: 45 \tLoss: 1.4773446321487427\n",
      "Epoch: 3 \tBatch: 46 \tLoss: 1.4910098314285278\n",
      "Epoch: 3 \tBatch: 47 \tLoss: 1.471291422843933\n",
      "Epoch: 3 \tBatch: 48 \tLoss: 1.4791558980941772\n",
      "Epoch: 3 \tBatch: 49 \tLoss: 1.505663514137268\n",
      "Epoch: 3 \tBatch: 50 \tLoss: 1.487753987312317\n",
      "Epoch: 3 \tBatch: 51 \tLoss: 1.479312777519226\n",
      "Epoch: 3 \tBatch: 52 \tLoss: 1.4795410633087158\n",
      "Epoch: 3 \tBatch: 53 \tLoss: 1.4940563440322876\n",
      "Epoch: 3 \tBatch: 54 \tLoss: 1.4771994352340698\n",
      "Epoch: 3 \tBatch: 55 \tLoss: 1.5081286430358887\n",
      "Epoch: 3 \tBatch: 56 \tLoss: 1.486833095550537\n",
      "Epoch: 3 \tBatch: 57 \tLoss: 1.4652985334396362\n",
      "Epoch: 3 \tBatch: 58 \tLoss: 1.4711211919784546\n",
      "Epoch: 3 \tBatch: 59 \tLoss: 1.4682905673980713\n",
      "Epoch: 3 \tBatch: 60 \tLoss: 1.4664655923843384\n",
      "Epoch: 3 \tBatch: 61 \tLoss: 1.4909530878067017\n",
      "Epoch: 3 \tBatch: 62 \tLoss: 1.5012118816375732\n",
      "Epoch: 3 \tBatch: 63 \tLoss: 1.4660204648971558\n",
      "Epoch: 3 \tBatch: 64 \tLoss: 1.4860432147979736\n",
      "Epoch: 3 \tBatch: 65 \tLoss: 1.4754477739334106\n",
      "Epoch: 3 \tBatch: 66 \tLoss: 1.4855501651763916\n",
      "Epoch: 3 \tBatch: 67 \tLoss: 1.476028323173523\n",
      "Epoch: 3 \tBatch: 68 \tLoss: 1.4886587858200073\n",
      "Epoch: 3 \tBatch: 69 \tLoss: 1.4704279899597168\n",
      "Epoch: 3 \tBatch: 70 \tLoss: 1.4751137495040894\n",
      "Epoch: 3 \tBatch: 71 \tLoss: 1.486689805984497\n",
      "Epoch: 3 \tBatch: 72 \tLoss: 1.4778735637664795\n",
      "Epoch: 3 \tBatch: 73 \tLoss: 1.4956609010696411\n",
      "Epoch: 3 \tBatch: 74 \tLoss: 1.504956603050232\n",
      "Epoch: 3 \tBatch: 75 \tLoss: 1.46723210811615\n",
      "Epoch: 3 \tBatch: 76 \tLoss: 1.4765169620513916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tBatch: 77 \tLoss: 1.4835822582244873\n",
      "Epoch: 3 \tBatch: 78 \tLoss: 1.4731813669204712\n",
      "Epoch: 3 \tBatch: 79 \tLoss: 1.4840123653411865\n",
      "Epoch: 3 \tBatch: 80 \tLoss: 1.4635374546051025\n",
      "Epoch: 3 \tBatch: 81 \tLoss: 1.4623444080352783\n",
      "Epoch: 3 \tBatch: 82 \tLoss: 1.4794994592666626\n",
      "Epoch: 3 \tBatch: 83 \tLoss: 1.4758365154266357\n",
      "Epoch: 3 \tBatch: 84 \tLoss: 1.4746313095092773\n",
      "Epoch: 3 \tBatch: 85 \tLoss: 1.5048127174377441\n",
      "Epoch: 3 \tBatch: 86 \tLoss: 1.4616318941116333\n",
      "Epoch: 3 \tBatch: 87 \tLoss: 1.4735254049301147\n",
      "Epoch: 3 \tBatch: 88 \tLoss: 1.4836244583129883\n",
      "Epoch: 3 \tBatch: 89 \tLoss: 1.4811146259307861\n",
      "Epoch: 3 \tBatch: 90 \tLoss: 1.474069595336914\n",
      "Epoch: 3 \tBatch: 91 \tLoss: 1.4773695468902588\n",
      "Epoch: 3 \tBatch: 92 \tLoss: 1.4948575496673584\n",
      "Epoch: 3 \tBatch: 93 \tLoss: 1.4655685424804688\n",
      "Epoch: 3 \tBatch: 94 \tLoss: 1.4792098999023438\n",
      "Epoch: 3 \tBatch: 95 \tLoss: 1.4921727180480957\n",
      "Epoch: 3 \tBatch: 96 \tLoss: 1.4701615571975708\n",
      "Epoch: 3 \tBatch: 97 \tLoss: 1.4666805267333984\n",
      "Epoch: 3 \tBatch: 98 \tLoss: 1.4614145755767822\n",
      "Epoch: 3 \tBatch: 99 \tLoss: 1.489938497543335\n",
      "Epoch: 3 \tBatch: 100 \tLoss: 1.4768543243408203\n",
      "Epoch: 3 \tBatch: 101 \tLoss: 1.497087001800537\n",
      "Epoch: 3 \tBatch: 102 \tLoss: 1.475756287574768\n",
      "Epoch: 3 \tBatch: 103 \tLoss: 1.4927749633789062\n",
      "Epoch: 3 \tBatch: 104 \tLoss: 1.503782868385315\n",
      "Epoch: 3 \tBatch: 105 \tLoss: 1.469490647315979\n",
      "Epoch: 3 \tBatch: 106 \tLoss: 1.4775346517562866\n",
      "Epoch: 3 \tBatch: 107 \tLoss: 1.4681154489517212\n",
      "Epoch: 3 \tBatch: 108 \tLoss: 1.5003151893615723\n",
      "Epoch: 3 \tBatch: 109 \tLoss: 1.4869228601455688\n",
      "Epoch: 3 \tBatch: 110 \tLoss: 1.4905304908752441\n",
      "Epoch: 3 \tBatch: 111 \tLoss: 1.4930477142333984\n",
      "Epoch: 3 \tBatch: 112 \tLoss: 1.4668453931808472\n",
      "Epoch: 3 \tBatch: 113 \tLoss: 1.4877640008926392\n",
      "Epoch: 3 \tBatch: 114 \tLoss: 1.4711635112762451\n",
      "Epoch: 3 \tBatch: 115 \tLoss: 1.471637487411499\n",
      "Epoch: 3 \tBatch: 116 \tLoss: 1.4754656553268433\n",
      "Epoch: 3 \tBatch: 117 \tLoss: 1.4838253259658813\n",
      "Epoch: 3 \tBatch: 118 \tLoss: 1.475592851638794\n",
      "Epoch: 3 \tBatch: 119 \tLoss: 1.4832016229629517\n",
      "Epoch: 3 \tBatch: 120 \tLoss: 1.4698199033737183\n",
      "Epoch: 3 \tBatch: 121 \tLoss: 1.5054093599319458\n",
      "Epoch: 3 \tBatch: 122 \tLoss: 1.4911489486694336\n",
      "Epoch: 3 \tBatch: 123 \tLoss: 1.4731080532073975\n",
      "Epoch: 3 \tBatch: 124 \tLoss: 1.4666279554367065\n",
      "Epoch: 3 \tBatch: 125 \tLoss: 1.4765477180480957\n",
      "Epoch: 3 \tBatch: 126 \tLoss: 1.4988776445388794\n",
      "Epoch: 3 \tBatch: 127 \tLoss: 1.476931095123291\n",
      "Epoch: 3 \tBatch: 128 \tLoss: 1.4837418794631958\n",
      "Epoch: 3 \tBatch: 129 \tLoss: 1.4864609241485596\n",
      "Epoch: 3 \tBatch: 130 \tLoss: 1.4700714349746704\n",
      "Epoch: 3 \tBatch: 131 \tLoss: 1.4914741516113281\n",
      "Epoch: 3 \tBatch: 132 \tLoss: 1.488799810409546\n",
      "Epoch: 3 \tBatch: 133 \tLoss: 1.4688905477523804\n",
      "Epoch: 3 \tBatch: 134 \tLoss: 1.469340205192566\n",
      "Epoch: 3 \tBatch: 135 \tLoss: 1.4690439701080322\n",
      "Epoch: 3 \tBatch: 136 \tLoss: 1.4820107221603394\n",
      "Epoch: 3 \tBatch: 137 \tLoss: 1.480483055114746\n",
      "Epoch: 3 \tBatch: 138 \tLoss: 1.4805725812911987\n",
      "Epoch: 3 \tBatch: 139 \tLoss: 1.4643211364746094\n",
      "Epoch: 3 \tBatch: 140 \tLoss: 1.469800591468811\n",
      "Epoch: 3 \tBatch: 141 \tLoss: 1.4737377166748047\n",
      "Epoch: 3 \tBatch: 142 \tLoss: 1.4894678592681885\n",
      "Epoch: 3 \tBatch: 143 \tLoss: 1.4768649339675903\n",
      "Epoch: 3 \tBatch: 144 \tLoss: 1.4646012783050537\n",
      "Epoch: 3 \tBatch: 145 \tLoss: 1.4856007099151611\n",
      "Epoch: 3 \tBatch: 146 \tLoss: 1.49478280544281\n",
      "Epoch: 3 \tBatch: 147 \tLoss: 1.477628469467163\n",
      "Epoch: 3 \tBatch: 148 \tLoss: 1.4738972187042236\n",
      "Epoch: 3 \tBatch: 149 \tLoss: 1.4684820175170898\n",
      "Epoch: 3 \tBatch: 150 \tLoss: 1.4760150909423828\n",
      "Epoch: 3 \tBatch: 151 \tLoss: 1.4725021123886108\n",
      "Epoch: 3 \tBatch: 152 \tLoss: 1.5033268928527832\n",
      "Epoch: 3 \tBatch: 153 \tLoss: 1.477938175201416\n",
      "Epoch: 3 \tBatch: 154 \tLoss: 1.4715168476104736\n",
      "Epoch: 3 \tBatch: 155 \tLoss: 1.4956481456756592\n",
      "Epoch: 3 \tBatch: 156 \tLoss: 1.4846222400665283\n",
      "Epoch: 3 \tBatch: 157 \tLoss: 1.4780610799789429\n",
      "Epoch: 3 \tBatch: 158 \tLoss: 1.4913285970687866\n",
      "Epoch: 3 \tBatch: 159 \tLoss: 1.4831502437591553\n",
      "Epoch: 3 \tBatch: 160 \tLoss: 1.4823411703109741\n",
      "Epoch: 3 \tBatch: 161 \tLoss: 1.4743773937225342\n",
      "Epoch: 3 \tBatch: 162 \tLoss: 1.4832353591918945\n",
      "Epoch: 3 \tBatch: 163 \tLoss: 1.489333987236023\n",
      "Epoch: 3 \tBatch: 164 \tLoss: 1.4760370254516602\n",
      "Epoch: 3 \tBatch: 165 \tLoss: 1.493548035621643\n",
      "Epoch: 3 \tBatch: 166 \tLoss: 1.484240174293518\n",
      "Epoch: 3 \tBatch: 167 \tLoss: 1.4714198112487793\n",
      "Epoch: 3 \tBatch: 168 \tLoss: 1.5017496347427368\n",
      "Epoch: 3 \tBatch: 169 \tLoss: 1.4778820276260376\n",
      "Epoch: 3 \tBatch: 170 \tLoss: 1.4791896343231201\n",
      "Epoch: 3 \tBatch: 171 \tLoss: 1.487254023551941\n",
      "Epoch: 3 \tBatch: 172 \tLoss: 1.4725477695465088\n",
      "Epoch: 3 \tBatch: 173 \tLoss: 1.4640814065933228\n",
      "Epoch: 3 \tBatch: 174 \tLoss: 1.4757364988327026\n",
      "Epoch: 3 \tBatch: 175 \tLoss: 1.474564790725708\n",
      "Epoch: 3 \tBatch: 176 \tLoss: 1.4832911491394043\n",
      "Epoch: 3 \tBatch: 177 \tLoss: 1.4894485473632812\n",
      "Epoch: 3 \tBatch: 178 \tLoss: 1.4746237993240356\n",
      "Epoch: 3 \tBatch: 179 \tLoss: 1.4655793905258179\n",
      "Epoch: 3 \tBatch: 180 \tLoss: 1.4651962518692017\n",
      "Epoch: 3 \tBatch: 181 \tLoss: 1.487675428390503\n",
      "Epoch: 3 \tBatch: 182 \tLoss: 1.498095989227295\n",
      "Epoch: 3 \tBatch: 183 \tLoss: 1.4634943008422852\n",
      "Epoch: 3 \tBatch: 184 \tLoss: 1.47060227394104\n",
      "Epoch: 3 \tBatch: 185 \tLoss: 1.479074478149414\n",
      "Epoch: 3 \tBatch: 186 \tLoss: 1.4710357189178467\n",
      "Epoch: 3 \tBatch: 187 \tLoss: 1.4838958978652954\n",
      "Epoch: 3 \tBatch: 188 \tLoss: 1.468705654144287\n",
      "Epoch: 3 \tBatch: 189 \tLoss: 1.477251648902893\n",
      "Epoch: 3 \tBatch: 190 \tLoss: 1.475262999534607\n",
      "Epoch: 3 \tBatch: 191 \tLoss: 1.4853956699371338\n",
      "Epoch: 3 \tBatch: 192 \tLoss: 1.4884828329086304\n",
      "Epoch: 3 \tBatch: 193 \tLoss: 1.465442419052124\n",
      "Epoch: 3 \tBatch: 194 \tLoss: 1.4733233451843262\n",
      "Epoch: 3 \tBatch: 195 \tLoss: 1.4757640361785889\n",
      "Epoch: 3 \tBatch: 196 \tLoss: 1.5027134418487549\n",
      "Epoch: 3 \tBatch: 197 \tLoss: 1.4680161476135254\n",
      "Epoch: 3 \tBatch: 198 \tLoss: 1.4941866397857666\n",
      "Epoch: 3 \tBatch: 199 \tLoss: 1.4830894470214844\n",
      "Epoch: 3 \tBatch: 200 \tLoss: 1.4768072366714478\n",
      "Epoch: 3 \tBatch: 201 \tLoss: 1.4924663305282593\n",
      "Epoch: 3 \tBatch: 202 \tLoss: 1.4880458116531372\n",
      "Epoch: 3 \tBatch: 203 \tLoss: 1.5034027099609375\n",
      "Epoch: 3 \tBatch: 204 \tLoss: 1.50441312789917\n",
      "Epoch: 3 \tBatch: 205 \tLoss: 1.469053030014038\n",
      "Epoch: 3 \tBatch: 206 \tLoss: 1.486465573310852\n",
      "Epoch: 3 \tBatch: 207 \tLoss: 1.478054165840149\n",
      "Epoch: 3 \tBatch: 208 \tLoss: 1.4721375703811646\n",
      "Epoch: 3 \tBatch: 209 \tLoss: 1.4702889919281006\n",
      "Epoch: 3 \tBatch: 210 \tLoss: 1.4976143836975098\n",
      "Epoch: 3 \tBatch: 211 \tLoss: 1.4870283603668213\n",
      "Epoch: 3 \tBatch: 212 \tLoss: 1.4835948944091797\n",
      "Epoch: 3 \tBatch: 213 \tLoss: 1.491431713104248\n",
      "Epoch: 3 \tBatch: 214 \tLoss: 1.4758635759353638\n",
      "Epoch: 3 \tBatch: 215 \tLoss: 1.4846187829971313\n",
      "Epoch: 3 \tBatch: 216 \tLoss: 1.4912714958190918\n",
      "Epoch: 3 \tBatch: 217 \tLoss: 1.4939075708389282\n",
      "Epoch: 3 \tBatch: 218 \tLoss: 1.4698154926300049\n",
      "Epoch: 3 \tBatch: 219 \tLoss: 1.4875431060791016\n",
      "Epoch: 3 \tBatch: 220 \tLoss: 1.4786698818206787\n",
      "Epoch: 3 \tBatch: 221 \tLoss: 1.4930025339126587\n",
      "Epoch: 3 \tBatch: 222 \tLoss: 1.4670319557189941\n",
      "Epoch: 3 \tBatch: 223 \tLoss: 1.4952037334442139\n",
      "Epoch: 3 \tBatch: 224 \tLoss: 1.4719047546386719\n",
      "Epoch: 3 \tBatch: 225 \tLoss: 1.4817218780517578\n",
      "Epoch: 3 \tBatch: 226 \tLoss: 1.5071552991867065\n",
      "Epoch: 3 \tBatch: 227 \tLoss: 1.4896247386932373\n",
      "Epoch: 3 \tBatch: 228 \tLoss: 1.496777892112732\n",
      "Epoch: 3 \tBatch: 229 \tLoss: 1.4839050769805908\n",
      "Epoch: 3 \tBatch: 230 \tLoss: 1.483257532119751\n",
      "Epoch: 3 \tBatch: 231 \tLoss: 1.4697717428207397\n",
      "Epoch: 3 \tBatch: 232 \tLoss: 1.48537278175354\n",
      "Epoch: 3 \tBatch: 233 \tLoss: 1.4804766178131104\n",
      "Epoch: 3 \tBatch: 234 \tLoss: 1.4843484163284302\n",
      "Epoch: 3 \tBatch: 235 \tLoss: 1.4852358102798462\n",
      "Epoch: 3 \tBatch: 236 \tLoss: 1.4842102527618408\n",
      "Epoch: 3 \tBatch: 237 \tLoss: 1.4708095788955688\n",
      "Epoch: 3 \tBatch: 238 \tLoss: 1.4877684116363525\n",
      "Epoch: 3 \tBatch: 239 \tLoss: 1.4780285358428955\n",
      "Epoch: 3 \tBatch: 240 \tLoss: 1.470924735069275\n",
      "Epoch: 3 \tBatch: 241 \tLoss: 1.4854387044906616\n",
      "Epoch: 3 \tBatch: 242 \tLoss: 1.5101319551467896\n",
      "Epoch: 3 \tBatch: 243 \tLoss: 1.5115649700164795\n",
      "Epoch: 3 \tBatch: 244 \tLoss: 1.4808381795883179\n",
      "Epoch: 3 \tBatch: 245 \tLoss: 1.4737863540649414\n",
      "Epoch: 3 \tBatch: 246 \tLoss: 1.48024320602417\n",
      "Epoch: 3 \tBatch: 247 \tLoss: 1.4690051078796387\n",
      "Epoch: 3 \tBatch: 248 \tLoss: 1.4783589839935303\n",
      "Epoch: 3 \tBatch: 249 \tLoss: 1.4984911680221558\n",
      "Epoch: 3 \tBatch: 250 \tLoss: 1.4828410148620605\n",
      "Epoch: 3 \tBatch: 251 \tLoss: 1.4687256813049316\n",
      "Epoch: 3 \tBatch: 252 \tLoss: 1.463329553604126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tBatch: 253 \tLoss: 1.478721261024475\n",
      "Epoch: 3 \tBatch: 254 \tLoss: 1.4856499433517456\n",
      "Epoch: 3 \tBatch: 255 \tLoss: 1.4713112115859985\n",
      "Epoch: 3 \tBatch: 256 \tLoss: 1.4962962865829468\n",
      "Epoch: 3 \tBatch: 257 \tLoss: 1.4779576063156128\n",
      "Epoch: 3 \tBatch: 258 \tLoss: 1.4856760501861572\n",
      "Epoch: 3 \tBatch: 259 \tLoss: 1.4953951835632324\n",
      "Epoch: 3 \tBatch: 260 \tLoss: 1.475582480430603\n",
      "Epoch: 3 \tBatch: 261 \tLoss: 1.4907587766647339\n",
      "Epoch: 3 \tBatch: 262 \tLoss: 1.4891170263290405\n",
      "Epoch: 3 \tBatch: 263 \tLoss: 1.4913411140441895\n",
      "Epoch: 3 \tBatch: 264 \tLoss: 1.4708808660507202\n",
      "Epoch: 3 \tBatch: 265 \tLoss: 1.4984009265899658\n",
      "Epoch: 3 \tBatch: 266 \tLoss: 1.4764704704284668\n",
      "Epoch: 3 \tBatch: 267 \tLoss: 1.4807562828063965\n",
      "Epoch: 3 \tBatch: 268 \tLoss: 1.475467562675476\n",
      "Epoch: 3 \tBatch: 269 \tLoss: 1.4837231636047363\n",
      "Epoch: 3 \tBatch: 270 \tLoss: 1.4657264947891235\n",
      "Epoch: 3 \tBatch: 271 \tLoss: 1.4939727783203125\n",
      "Epoch: 3 \tBatch: 272 \tLoss: 1.480033040046692\n",
      "Epoch: 3 \tBatch: 273 \tLoss: 1.4893916845321655\n",
      "Epoch: 3 \tBatch: 274 \tLoss: 1.4799541234970093\n",
      "Epoch: 3 \tBatch: 275 \tLoss: 1.5010453462600708\n",
      "Epoch: 3 \tBatch: 276 \tLoss: 1.4878134727478027\n",
      "Epoch: 3 \tBatch: 277 \tLoss: 1.487129807472229\n",
      "Epoch: 3 \tBatch: 278 \tLoss: 1.4672038555145264\n",
      "Epoch: 3 \tBatch: 279 \tLoss: 1.475767731666565\n",
      "Epoch: 3 \tBatch: 280 \tLoss: 1.4864463806152344\n",
      "Epoch: 3 \tBatch: 281 \tLoss: 1.4976614713668823\n",
      "Epoch: 3 \tBatch: 282 \tLoss: 1.484595537185669\n",
      "Epoch: 3 \tBatch: 283 \tLoss: 1.4843333959579468\n",
      "Epoch: 3 \tBatch: 284 \tLoss: 1.4732469320297241\n",
      "Epoch: 3 \tBatch: 285 \tLoss: 1.4700359106063843\n",
      "Epoch: 3 \tBatch: 286 \tLoss: 1.4735925197601318\n",
      "Epoch: 3 \tBatch: 287 \tLoss: 1.4702389240264893\n",
      "Epoch: 3 \tBatch: 288 \tLoss: 1.4888497591018677\n",
      "Epoch: 3 \tBatch: 289 \tLoss: 1.4931354522705078\n",
      "Epoch: 3 \tBatch: 290 \tLoss: 1.4803308248519897\n",
      "Epoch: 3 \tBatch: 291 \tLoss: 1.4694411754608154\n",
      "Epoch: 3 \tBatch: 292 \tLoss: 1.472903847694397\n",
      "Epoch: 3 \tBatch: 293 \tLoss: 1.4867266416549683\n",
      "Epoch: 3 \tBatch: 294 \tLoss: 1.480891466140747\n",
      "Epoch: 3 \tBatch: 295 \tLoss: 1.4694721698760986\n",
      "Epoch: 3 \tBatch: 296 \tLoss: 1.4781755208969116\n",
      "Epoch: 3 \tBatch: 297 \tLoss: 1.4643585681915283\n",
      "Epoch: 3 \tBatch: 298 \tLoss: 1.4954099655151367\n",
      "Epoch: 3 \tBatch: 299 \tLoss: 1.4743280410766602\n",
      "Epoch: 3 \tBatch: 300 \tLoss: 1.487050175666809\n",
      "Epoch: 3 \tBatch: 301 \tLoss: 1.4659122228622437\n",
      "Epoch: 3 \tBatch: 302 \tLoss: 1.4742881059646606\n",
      "Epoch: 3 \tBatch: 303 \tLoss: 1.49087655544281\n",
      "Epoch: 3 \tBatch: 304 \tLoss: 1.4807980060577393\n",
      "Epoch: 3 \tBatch: 305 \tLoss: 1.4830961227416992\n",
      "Epoch: 3 \tBatch: 306 \tLoss: 1.4776856899261475\n",
      "Epoch: 3 \tBatch: 307 \tLoss: 1.4934251308441162\n",
      "Epoch: 3 \tBatch: 308 \tLoss: 1.4853813648223877\n",
      "Epoch: 3 \tBatch: 309 \tLoss: 1.4981248378753662\n",
      "Epoch: 3 \tBatch: 310 \tLoss: 1.4822423458099365\n",
      "Epoch: 3 \tBatch: 311 \tLoss: 1.4965623617172241\n",
      "Epoch: 3 \tBatch: 312 \tLoss: 1.4848964214324951\n",
      "Epoch: 3 \tBatch: 313 \tLoss: 1.4803084135055542\n",
      "Epoch: 3 \tBatch: 314 \tLoss: 1.4808951616287231\n",
      "Epoch: 3 \tBatch: 315 \tLoss: 1.4833978414535522\n",
      "Epoch: 3 \tBatch: 316 \tLoss: 1.462479591369629\n",
      "Epoch: 3 \tBatch: 317 \tLoss: 1.4818158149719238\n",
      "Epoch: 3 \tBatch: 318 \tLoss: 1.4621920585632324\n",
      "Epoch: 3 \tBatch: 319 \tLoss: 1.4615092277526855\n",
      "Epoch: 3 \tBatch: 320 \tLoss: 1.4642189741134644\n",
      "Epoch: 3 \tBatch: 321 \tLoss: 1.4734773635864258\n",
      "Epoch: 3 \tBatch: 322 \tLoss: 1.478316068649292\n",
      "Epoch: 3 \tBatch: 323 \tLoss: 1.5027767419815063\n",
      "Epoch: 3 \tBatch: 324 \tLoss: 1.4881175756454468\n",
      "Epoch: 3 \tBatch: 325 \tLoss: 1.4698680639266968\n",
      "Epoch: 3 \tBatch: 326 \tLoss: 1.4676594734191895\n",
      "Epoch: 3 \tBatch: 327 \tLoss: 1.468332052230835\n",
      "Epoch: 3 \tBatch: 328 \tLoss: 1.4957022666931152\n",
      "Epoch: 3 \tBatch: 329 \tLoss: 1.4756962060928345\n",
      "Epoch: 3 \tBatch: 330 \tLoss: 1.4722703695297241\n",
      "Epoch: 3 \tBatch: 331 \tLoss: 1.4691716432571411\n",
      "Epoch: 3 \tBatch: 332 \tLoss: 1.4773863554000854\n",
      "Epoch: 3 \tBatch: 333 \tLoss: 1.4941306114196777\n",
      "Epoch: 3 \tBatch: 334 \tLoss: 1.4719440937042236\n",
      "Epoch: 3 \tBatch: 335 \tLoss: 1.4792057275772095\n",
      "Epoch: 3 \tBatch: 336 \tLoss: 1.4822523593902588\n",
      "Epoch: 3 \tBatch: 337 \tLoss: 1.4991095066070557\n",
      "Epoch: 3 \tBatch: 338 \tLoss: 1.4665483236312866\n",
      "Epoch: 3 \tBatch: 339 \tLoss: 1.4828431606292725\n",
      "Epoch: 3 \tBatch: 340 \tLoss: 1.4660816192626953\n",
      "Epoch: 3 \tBatch: 341 \tLoss: 1.486747145652771\n",
      "Epoch: 3 \tBatch: 342 \tLoss: 1.4773920774459839\n",
      "Epoch: 3 \tBatch: 343 \tLoss: 1.4667366743087769\n",
      "Epoch: 3 \tBatch: 344 \tLoss: 1.4733750820159912\n",
      "Epoch: 3 \tBatch: 345 \tLoss: 1.4793856143951416\n",
      "Epoch: 3 \tBatch: 346 \tLoss: 1.464653730392456\n",
      "Epoch: 3 \tBatch: 347 \tLoss: 1.489102840423584\n",
      "Epoch: 3 \tBatch: 348 \tLoss: 1.4907684326171875\n",
      "Epoch: 3 \tBatch: 349 \tLoss: 1.4763567447662354\n",
      "Epoch: 3 \tBatch: 350 \tLoss: 1.5013484954833984\n",
      "Epoch: 3 \tBatch: 351 \tLoss: 1.4816687107086182\n",
      "Epoch: 3 \tBatch: 352 \tLoss: 1.491276502609253\n",
      "Epoch: 3 \tBatch: 353 \tLoss: 1.5005085468292236\n",
      "Epoch: 3 \tBatch: 354 \tLoss: 1.4768441915512085\n",
      "Epoch: 3 \tBatch: 355 \tLoss: 1.4835784435272217\n",
      "Epoch: 3 \tBatch: 356 \tLoss: 1.4749913215637207\n",
      "Epoch: 3 \tBatch: 357 \tLoss: 1.4874738454818726\n",
      "Epoch: 3 \tBatch: 358 \tLoss: 1.4659067392349243\n",
      "Epoch: 3 \tBatch: 359 \tLoss: 1.4914124011993408\n",
      "Epoch: 3 \tBatch: 360 \tLoss: 1.4910916090011597\n",
      "Epoch: 3 \tBatch: 361 \tLoss: 1.4635623693466187\n",
      "Epoch: 3 \tBatch: 362 \tLoss: 1.4918897151947021\n",
      "Epoch: 3 \tBatch: 363 \tLoss: 1.4771232604980469\n",
      "Epoch: 3 \tBatch: 364 \tLoss: 1.4799729585647583\n",
      "Epoch: 3 \tBatch: 365 \tLoss: 1.471519947052002\n",
      "Epoch: 3 \tBatch: 366 \tLoss: 1.469671607017517\n",
      "Epoch: 3 \tBatch: 367 \tLoss: 1.5050486326217651\n",
      "Epoch: 3 \tBatch: 368 \tLoss: 1.4865750074386597\n",
      "Epoch: 3 \tBatch: 369 \tLoss: 1.4776207208633423\n",
      "Epoch: 3 \tBatch: 370 \tLoss: 1.4783269166946411\n",
      "Epoch: 3 \tBatch: 371 \tLoss: 1.4773147106170654\n",
      "Epoch: 3 \tBatch: 372 \tLoss: 1.4775336980819702\n",
      "Epoch: 3 \tBatch: 373 \tLoss: 1.4811896085739136\n",
      "Epoch: 3 \tBatch: 374 \tLoss: 1.499489426612854\n",
      "Epoch: 3 \tBatch: 375 \tLoss: 1.4754287004470825\n",
      "Epoch: 3 \tBatch: 376 \tLoss: 1.4779226779937744\n",
      "Epoch: 3 \tBatch: 377 \tLoss: 1.4769898653030396\n",
      "Epoch: 3 \tBatch: 378 \tLoss: 1.4845434427261353\n",
      "Epoch: 3 \tBatch: 379 \tLoss: 1.4710116386413574\n",
      "Epoch: 3 \tBatch: 380 \tLoss: 1.4853051900863647\n",
      "Epoch: 3 \tBatch: 381 \tLoss: 1.4774643182754517\n",
      "Epoch: 3 \tBatch: 382 \tLoss: 1.4802955389022827\n",
      "Epoch: 3 \tBatch: 383 \tLoss: 1.4832056760787964\n",
      "Epoch: 3 \tBatch: 384 \tLoss: 1.4784125089645386\n",
      "Epoch: 3 \tBatch: 385 \tLoss: 1.4937312602996826\n",
      "Epoch: 3 \tBatch: 386 \tLoss: 1.5159711837768555\n",
      "Epoch: 3 \tBatch: 387 \tLoss: 1.4772833585739136\n",
      "Epoch: 3 \tBatch: 388 \tLoss: 1.474104642868042\n",
      "Epoch: 3 \tBatch: 389 \tLoss: 1.461648941040039\n",
      "Epoch: 3 \tBatch: 390 \tLoss: 1.4713075160980225\n",
      "Epoch: 4 \tBatch: 0 \tLoss: 1.47016441822052\n",
      "Epoch: 4 \tBatch: 1 \tLoss: 1.4621206521987915\n",
      "Epoch: 4 \tBatch: 2 \tLoss: 1.4723732471466064\n",
      "Epoch: 4 \tBatch: 3 \tLoss: 1.4693710803985596\n",
      "Epoch: 4 \tBatch: 4 \tLoss: 1.4768102169036865\n",
      "Epoch: 4 \tBatch: 5 \tLoss: 1.496056318283081\n",
      "Epoch: 4 \tBatch: 6 \tLoss: 1.4955812692642212\n",
      "Epoch: 4 \tBatch: 7 \tLoss: 1.4792988300323486\n",
      "Epoch: 4 \tBatch: 8 \tLoss: 1.487610936164856\n",
      "Epoch: 4 \tBatch: 9 \tLoss: 1.4876629114151\n",
      "Epoch: 4 \tBatch: 10 \tLoss: 1.4779444932937622\n",
      "Epoch: 4 \tBatch: 11 \tLoss: 1.4708056449890137\n",
      "Epoch: 4 \tBatch: 12 \tLoss: 1.4771835803985596\n",
      "Epoch: 4 \tBatch: 13 \tLoss: 1.4625258445739746\n",
      "Epoch: 4 \tBatch: 14 \tLoss: 1.4811537265777588\n",
      "Epoch: 4 \tBatch: 15 \tLoss: 1.4705666303634644\n",
      "Epoch: 4 \tBatch: 16 \tLoss: 1.4850035905838013\n",
      "Epoch: 4 \tBatch: 17 \tLoss: 1.489007592201233\n",
      "Epoch: 4 \tBatch: 18 \tLoss: 1.4950597286224365\n",
      "Epoch: 4 \tBatch: 19 \tLoss: 1.4684488773345947\n",
      "Epoch: 4 \tBatch: 20 \tLoss: 1.478035569190979\n",
      "Epoch: 4 \tBatch: 21 \tLoss: 1.463220238685608\n",
      "Epoch: 4 \tBatch: 22 \tLoss: 1.4758765697479248\n",
      "Epoch: 4 \tBatch: 23 \tLoss: 1.4792226552963257\n",
      "Epoch: 4 \tBatch: 24 \tLoss: 1.4615774154663086\n",
      "Epoch: 4 \tBatch: 25 \tLoss: 1.5066816806793213\n",
      "Epoch: 4 \tBatch: 26 \tLoss: 1.473878026008606\n",
      "Epoch: 4 \tBatch: 27 \tLoss: 1.4788422584533691\n",
      "Epoch: 4 \tBatch: 28 \tLoss: 1.4642566442489624\n",
      "Epoch: 4 \tBatch: 29 \tLoss: 1.4834048748016357\n",
      "Epoch: 4 \tBatch: 30 \tLoss: 1.4752169847488403\n",
      "Epoch: 4 \tBatch: 31 \tLoss: 1.4747651815414429\n",
      "Epoch: 4 \tBatch: 32 \tLoss: 1.479062557220459\n",
      "Epoch: 4 \tBatch: 33 \tLoss: 1.4709173440933228\n",
      "Epoch: 4 \tBatch: 34 \tLoss: 1.4796727895736694\n",
      "Epoch: 4 \tBatch: 35 \tLoss: 1.4961450099945068\n",
      "Epoch: 4 \tBatch: 36 \tLoss: 1.4632973670959473\n",
      "Epoch: 4 \tBatch: 37 \tLoss: 1.4785292148590088\n",
      "Epoch: 4 \tBatch: 38 \tLoss: 1.4804877042770386\n",
      "Epoch: 4 \tBatch: 39 \tLoss: 1.5053374767303467\n",
      "Epoch: 4 \tBatch: 40 \tLoss: 1.4826774597167969\n",
      "Epoch: 4 \tBatch: 41 \tLoss: 1.4731332063674927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 42 \tLoss: 1.4673470258712769\n",
      "Epoch: 4 \tBatch: 43 \tLoss: 1.479129433631897\n",
      "Epoch: 4 \tBatch: 44 \tLoss: 1.4943292140960693\n",
      "Epoch: 4 \tBatch: 45 \tLoss: 1.4635579586029053\n",
      "Epoch: 4 \tBatch: 46 \tLoss: 1.4753371477127075\n",
      "Epoch: 4 \tBatch: 47 \tLoss: 1.4926426410675049\n",
      "Epoch: 4 \tBatch: 48 \tLoss: 1.466221570968628\n",
      "Epoch: 4 \tBatch: 49 \tLoss: 1.4692574739456177\n",
      "Epoch: 4 \tBatch: 50 \tLoss: 1.4747871160507202\n",
      "Epoch: 4 \tBatch: 51 \tLoss: 1.4929887056350708\n",
      "Epoch: 4 \tBatch: 52 \tLoss: 1.4636467695236206\n",
      "Epoch: 4 \tBatch: 53 \tLoss: 1.4782041311264038\n",
      "Epoch: 4 \tBatch: 54 \tLoss: 1.4857820272445679\n",
      "Epoch: 4 \tBatch: 55 \tLoss: 1.4740041494369507\n",
      "Epoch: 4 \tBatch: 56 \tLoss: 1.483005404472351\n",
      "Epoch: 4 \tBatch: 57 \tLoss: 1.4828077554702759\n",
      "Epoch: 4 \tBatch: 58 \tLoss: 1.4735666513442993\n",
      "Epoch: 4 \tBatch: 59 \tLoss: 1.4854178428649902\n",
      "Epoch: 4 \tBatch: 60 \tLoss: 1.4942947626113892\n",
      "Epoch: 4 \tBatch: 61 \tLoss: 1.4848582744598389\n",
      "Epoch: 4 \tBatch: 62 \tLoss: 1.488938570022583\n",
      "Epoch: 4 \tBatch: 63 \tLoss: 1.4650685787200928\n",
      "Epoch: 4 \tBatch: 64 \tLoss: 1.4736833572387695\n",
      "Epoch: 4 \tBatch: 65 \tLoss: 1.4700334072113037\n",
      "Epoch: 4 \tBatch: 66 \tLoss: 1.464560866355896\n",
      "Epoch: 4 \tBatch: 67 \tLoss: 1.4998116493225098\n",
      "Epoch: 4 \tBatch: 68 \tLoss: 1.4743694067001343\n",
      "Epoch: 4 \tBatch: 69 \tLoss: 1.4947518110275269\n",
      "Epoch: 4 \tBatch: 70 \tLoss: 1.4937220811843872\n",
      "Epoch: 4 \tBatch: 71 \tLoss: 1.4654682874679565\n",
      "Epoch: 4 \tBatch: 72 \tLoss: 1.4803491830825806\n",
      "Epoch: 4 \tBatch: 73 \tLoss: 1.4700852632522583\n",
      "Epoch: 4 \tBatch: 74 \tLoss: 1.4648759365081787\n",
      "Epoch: 4 \tBatch: 75 \tLoss: 1.4643083810806274\n",
      "Epoch: 4 \tBatch: 76 \tLoss: 1.4712363481521606\n",
      "Epoch: 4 \tBatch: 77 \tLoss: 1.4913575649261475\n",
      "Epoch: 4 \tBatch: 78 \tLoss: 1.4871071577072144\n",
      "Epoch: 4 \tBatch: 79 \tLoss: 1.4824440479278564\n",
      "Epoch: 4 \tBatch: 80 \tLoss: 1.4783333539962769\n",
      "Epoch: 4 \tBatch: 81 \tLoss: 1.4726734161376953\n",
      "Epoch: 4 \tBatch: 82 \tLoss: 1.4795442819595337\n",
      "Epoch: 4 \tBatch: 83 \tLoss: 1.487443208694458\n",
      "Epoch: 4 \tBatch: 84 \tLoss: 1.4755829572677612\n",
      "Epoch: 4 \tBatch: 85 \tLoss: 1.4891842603683472\n",
      "Epoch: 4 \tBatch: 86 \tLoss: 1.4936378002166748\n",
      "Epoch: 4 \tBatch: 87 \tLoss: 1.4767910242080688\n",
      "Epoch: 4 \tBatch: 88 \tLoss: 1.4835433959960938\n",
      "Epoch: 4 \tBatch: 89 \tLoss: 1.469285488128662\n",
      "Epoch: 4 \tBatch: 90 \tLoss: 1.497531533241272\n",
      "Epoch: 4 \tBatch: 91 \tLoss: 1.4767249822616577\n",
      "Epoch: 4 \tBatch: 92 \tLoss: 1.4908745288848877\n",
      "Epoch: 4 \tBatch: 93 \tLoss: 1.4689139127731323\n",
      "Epoch: 4 \tBatch: 94 \tLoss: 1.474686861038208\n",
      "Epoch: 4 \tBatch: 95 \tLoss: 1.4702143669128418\n",
      "Epoch: 4 \tBatch: 96 \tLoss: 1.474875807762146\n",
      "Epoch: 4 \tBatch: 97 \tLoss: 1.4823520183563232\n",
      "Epoch: 4 \tBatch: 98 \tLoss: 1.4840142726898193\n",
      "Epoch: 4 \tBatch: 99 \tLoss: 1.4746147394180298\n",
      "Epoch: 4 \tBatch: 100 \tLoss: 1.4662731885910034\n",
      "Epoch: 4 \tBatch: 101 \tLoss: 1.4971611499786377\n",
      "Epoch: 4 \tBatch: 102 \tLoss: 1.49799644947052\n",
      "Epoch: 4 \tBatch: 103 \tLoss: 1.4819602966308594\n",
      "Epoch: 4 \tBatch: 104 \tLoss: 1.4821542501449585\n",
      "Epoch: 4 \tBatch: 105 \tLoss: 1.4616223573684692\n",
      "Epoch: 4 \tBatch: 106 \tLoss: 1.465301275253296\n",
      "Epoch: 4 \tBatch: 107 \tLoss: 1.50763738155365\n",
      "Epoch: 4 \tBatch: 108 \tLoss: 1.4857561588287354\n",
      "Epoch: 4 \tBatch: 109 \tLoss: 1.4812222719192505\n",
      "Epoch: 4 \tBatch: 110 \tLoss: 1.4756838083267212\n",
      "Epoch: 4 \tBatch: 111 \tLoss: 1.4757764339447021\n",
      "Epoch: 4 \tBatch: 112 \tLoss: 1.485313892364502\n",
      "Epoch: 4 \tBatch: 113 \tLoss: 1.4907838106155396\n",
      "Epoch: 4 \tBatch: 114 \tLoss: 1.4715609550476074\n",
      "Epoch: 4 \tBatch: 115 \tLoss: 1.4744082689285278\n",
      "Epoch: 4 \tBatch: 116 \tLoss: 1.4756720066070557\n",
      "Epoch: 4 \tBatch: 117 \tLoss: 1.4666392803192139\n",
      "Epoch: 4 \tBatch: 118 \tLoss: 1.4696441888809204\n",
      "Epoch: 4 \tBatch: 119 \tLoss: 1.4830766916275024\n",
      "Epoch: 4 \tBatch: 120 \tLoss: 1.4665247201919556\n",
      "Epoch: 4 \tBatch: 121 \tLoss: 1.4984912872314453\n",
      "Epoch: 4 \tBatch: 122 \tLoss: 1.4642651081085205\n",
      "Epoch: 4 \tBatch: 123 \tLoss: 1.4850937128067017\n",
      "Epoch: 4 \tBatch: 124 \tLoss: 1.4894551038742065\n",
      "Epoch: 4 \tBatch: 125 \tLoss: 1.4977102279663086\n",
      "Epoch: 4 \tBatch: 126 \tLoss: 1.5023722648620605\n",
      "Epoch: 4 \tBatch: 127 \tLoss: 1.4764811992645264\n",
      "Epoch: 4 \tBatch: 128 \tLoss: 1.4834413528442383\n",
      "Epoch: 4 \tBatch: 129 \tLoss: 1.4669547080993652\n",
      "Epoch: 4 \tBatch: 130 \tLoss: 1.474970817565918\n",
      "Epoch: 4 \tBatch: 131 \tLoss: 1.478070616722107\n",
      "Epoch: 4 \tBatch: 132 \tLoss: 1.4755536317825317\n",
      "Epoch: 4 \tBatch: 133 \tLoss: 1.4771531820297241\n",
      "Epoch: 4 \tBatch: 134 \tLoss: 1.4692810773849487\n",
      "Epoch: 4 \tBatch: 135 \tLoss: 1.474729299545288\n",
      "Epoch: 4 \tBatch: 136 \tLoss: 1.4905495643615723\n",
      "Epoch: 4 \tBatch: 137 \tLoss: 1.4954280853271484\n",
      "Epoch: 4 \tBatch: 138 \tLoss: 1.4694334268569946\n",
      "Epoch: 4 \tBatch: 139 \tLoss: 1.4841481447219849\n",
      "Epoch: 4 \tBatch: 140 \tLoss: 1.4725645780563354\n",
      "Epoch: 4 \tBatch: 141 \tLoss: 1.46846604347229\n",
      "Epoch: 4 \tBatch: 142 \tLoss: 1.4714179039001465\n",
      "Epoch: 4 \tBatch: 143 \tLoss: 1.471142292022705\n",
      "Epoch: 4 \tBatch: 144 \tLoss: 1.4821083545684814\n",
      "Epoch: 4 \tBatch: 145 \tLoss: 1.4821771383285522\n",
      "Epoch: 4 \tBatch: 146 \tLoss: 1.4722061157226562\n",
      "Epoch: 4 \tBatch: 147 \tLoss: 1.471311330795288\n",
      "Epoch: 4 \tBatch: 148 \tLoss: 1.4629130363464355\n",
      "Epoch: 4 \tBatch: 149 \tLoss: 1.506109356880188\n",
      "Epoch: 4 \tBatch: 150 \tLoss: 1.4876763820648193\n",
      "Epoch: 4 \tBatch: 151 \tLoss: 1.4759929180145264\n",
      "Epoch: 4 \tBatch: 152 \tLoss: 1.4637539386749268\n",
      "Epoch: 4 \tBatch: 153 \tLoss: 1.4930760860443115\n",
      "Epoch: 4 \tBatch: 154 \tLoss: 1.4833744764328003\n",
      "Epoch: 4 \tBatch: 155 \tLoss: 1.4791758060455322\n",
      "Epoch: 4 \tBatch: 156 \tLoss: 1.4780638217926025\n",
      "Epoch: 4 \tBatch: 157 \tLoss: 1.4757007360458374\n",
      "Epoch: 4 \tBatch: 158 \tLoss: 1.4659594297409058\n",
      "Epoch: 4 \tBatch: 159 \tLoss: 1.4855700731277466\n",
      "Epoch: 4 \tBatch: 160 \tLoss: 1.4692450761795044\n",
      "Epoch: 4 \tBatch: 161 \tLoss: 1.5014609098434448\n",
      "Epoch: 4 \tBatch: 162 \tLoss: 1.4700530767440796\n",
      "Epoch: 4 \tBatch: 163 \tLoss: 1.4933768510818481\n",
      "Epoch: 4 \tBatch: 164 \tLoss: 1.4656974077224731\n",
      "Epoch: 4 \tBatch: 165 \tLoss: 1.4721412658691406\n",
      "Epoch: 4 \tBatch: 166 \tLoss: 1.461264967918396\n",
      "Epoch: 4 \tBatch: 167 \tLoss: 1.4987117052078247\n",
      "Epoch: 4 \tBatch: 168 \tLoss: 1.4782843589782715\n",
      "Epoch: 4 \tBatch: 169 \tLoss: 1.4690996408462524\n",
      "Epoch: 4 \tBatch: 170 \tLoss: 1.4765949249267578\n",
      "Epoch: 4 \tBatch: 171 \tLoss: 1.4815402030944824\n",
      "Epoch: 4 \tBatch: 172 \tLoss: 1.4965689182281494\n",
      "Epoch: 4 \tBatch: 173 \tLoss: 1.4785221815109253\n",
      "Epoch: 4 \tBatch: 174 \tLoss: 1.483377456665039\n",
      "Epoch: 4 \tBatch: 175 \tLoss: 1.4779906272888184\n",
      "Epoch: 4 \tBatch: 176 \tLoss: 1.471388816833496\n",
      "Epoch: 4 \tBatch: 177 \tLoss: 1.4705426692962646\n",
      "Epoch: 4 \tBatch: 178 \tLoss: 1.491513729095459\n",
      "Epoch: 4 \tBatch: 179 \tLoss: 1.477000117301941\n",
      "Epoch: 4 \tBatch: 180 \tLoss: 1.4616676568984985\n",
      "Epoch: 4 \tBatch: 181 \tLoss: 1.5161951780319214\n",
      "Epoch: 4 \tBatch: 182 \tLoss: 1.4947043657302856\n",
      "Epoch: 4 \tBatch: 183 \tLoss: 1.4763646125793457\n",
      "Epoch: 4 \tBatch: 184 \tLoss: 1.4857616424560547\n",
      "Epoch: 4 \tBatch: 185 \tLoss: 1.4716373682022095\n",
      "Epoch: 4 \tBatch: 186 \tLoss: 1.4796061515808105\n",
      "Epoch: 4 \tBatch: 187 \tLoss: 1.4871208667755127\n",
      "Epoch: 4 \tBatch: 188 \tLoss: 1.4668349027633667\n",
      "Epoch: 4 \tBatch: 189 \tLoss: 1.4858767986297607\n",
      "Epoch: 4 \tBatch: 190 \tLoss: 1.4879745244979858\n",
      "Epoch: 4 \tBatch: 191 \tLoss: 1.4732099771499634\n",
      "Epoch: 4 \tBatch: 192 \tLoss: 1.468922734260559\n",
      "Epoch: 4 \tBatch: 193 \tLoss: 1.4620846509933472\n",
      "Epoch: 4 \tBatch: 194 \tLoss: 1.4668161869049072\n",
      "Epoch: 4 \tBatch: 195 \tLoss: 1.4821420907974243\n",
      "Epoch: 4 \tBatch: 196 \tLoss: 1.4629037380218506\n",
      "Epoch: 4 \tBatch: 197 \tLoss: 1.4797911643981934\n",
      "Epoch: 4 \tBatch: 198 \tLoss: 1.4686683416366577\n",
      "Epoch: 4 \tBatch: 199 \tLoss: 1.4888534545898438\n",
      "Epoch: 4 \tBatch: 200 \tLoss: 1.4694949388504028\n",
      "Epoch: 4 \tBatch: 201 \tLoss: 1.4847421646118164\n",
      "Epoch: 4 \tBatch: 202 \tLoss: 1.4772076606750488\n",
      "Epoch: 4 \tBatch: 203 \tLoss: 1.4882615804672241\n",
      "Epoch: 4 \tBatch: 204 \tLoss: 1.4650450944900513\n",
      "Epoch: 4 \tBatch: 205 \tLoss: 1.475279450416565\n",
      "Epoch: 4 \tBatch: 206 \tLoss: 1.48529052734375\n",
      "Epoch: 4 \tBatch: 207 \tLoss: 1.4772671461105347\n",
      "Epoch: 4 \tBatch: 208 \tLoss: 1.4771807193756104\n",
      "Epoch: 4 \tBatch: 209 \tLoss: 1.4869433641433716\n",
      "Epoch: 4 \tBatch: 210 \tLoss: 1.4862940311431885\n",
      "Epoch: 4 \tBatch: 211 \tLoss: 1.51168954372406\n",
      "Epoch: 4 \tBatch: 212 \tLoss: 1.5046080350875854\n",
      "Epoch: 4 \tBatch: 213 \tLoss: 1.470435380935669\n",
      "Epoch: 4 \tBatch: 214 \tLoss: 1.4782270193099976\n",
      "Epoch: 4 \tBatch: 215 \tLoss: 1.4894367456436157\n",
      "Epoch: 4 \tBatch: 216 \tLoss: 1.4771695137023926\n",
      "Epoch: 4 \tBatch: 217 \tLoss: 1.5011107921600342\n",
      "Epoch: 4 \tBatch: 218 \tLoss: 1.4691449403762817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 219 \tLoss: 1.4795089960098267\n",
      "Epoch: 4 \tBatch: 220 \tLoss: 1.488156795501709\n",
      "Epoch: 4 \tBatch: 221 \tLoss: 1.4839582443237305\n",
      "Epoch: 4 \tBatch: 222 \tLoss: 1.479638934135437\n",
      "Epoch: 4 \tBatch: 223 \tLoss: 1.477520227432251\n",
      "Epoch: 4 \tBatch: 224 \tLoss: 1.4686086177825928\n",
      "Epoch: 4 \tBatch: 225 \tLoss: 1.5001215934753418\n",
      "Epoch: 4 \tBatch: 226 \tLoss: 1.4940319061279297\n",
      "Epoch: 4 \tBatch: 227 \tLoss: 1.48404860496521\n",
      "Epoch: 4 \tBatch: 228 \tLoss: 1.4743232727050781\n",
      "Epoch: 4 \tBatch: 229 \tLoss: 1.4619231224060059\n",
      "Epoch: 4 \tBatch: 230 \tLoss: 1.499787449836731\n",
      "Epoch: 4 \tBatch: 231 \tLoss: 1.4770660400390625\n",
      "Epoch: 4 \tBatch: 232 \tLoss: 1.4942458868026733\n",
      "Epoch: 4 \tBatch: 233 \tLoss: 1.5083916187286377\n",
      "Epoch: 4 \tBatch: 234 \tLoss: 1.4682570695877075\n",
      "Epoch: 4 \tBatch: 235 \tLoss: 1.483481526374817\n",
      "Epoch: 4 \tBatch: 236 \tLoss: 1.4724639654159546\n",
      "Epoch: 4 \tBatch: 237 \tLoss: 1.475326657295227\n",
      "Epoch: 4 \tBatch: 238 \tLoss: 1.4854639768600464\n",
      "Epoch: 4 \tBatch: 239 \tLoss: 1.4829813241958618\n",
      "Epoch: 4 \tBatch: 240 \tLoss: 1.4740104675292969\n",
      "Epoch: 4 \tBatch: 241 \tLoss: 1.4795074462890625\n",
      "Epoch: 4 \tBatch: 242 \tLoss: 1.4782824516296387\n",
      "Epoch: 4 \tBatch: 243 \tLoss: 1.4867897033691406\n",
      "Epoch: 4 \tBatch: 244 \tLoss: 1.4639631509780884\n",
      "Epoch: 4 \tBatch: 245 \tLoss: 1.4768071174621582\n",
      "Epoch: 4 \tBatch: 246 \tLoss: 1.4819362163543701\n",
      "Epoch: 4 \tBatch: 247 \tLoss: 1.4852031469345093\n",
      "Epoch: 4 \tBatch: 248 \tLoss: 1.5066410303115845\n",
      "Epoch: 4 \tBatch: 249 \tLoss: 1.4833710193634033\n",
      "Epoch: 4 \tBatch: 250 \tLoss: 1.4972991943359375\n",
      "Epoch: 4 \tBatch: 251 \tLoss: 1.462812900543213\n",
      "Epoch: 4 \tBatch: 252 \tLoss: 1.4945505857467651\n",
      "Epoch: 4 \tBatch: 253 \tLoss: 1.4771162271499634\n",
      "Epoch: 4 \tBatch: 254 \tLoss: 1.4789425134658813\n",
      "Epoch: 4 \tBatch: 255 \tLoss: 1.4645675420761108\n",
      "Epoch: 4 \tBatch: 256 \tLoss: 1.4716895818710327\n",
      "Epoch: 4 \tBatch: 257 \tLoss: 1.487320065498352\n",
      "Epoch: 4 \tBatch: 258 \tLoss: 1.4890536069869995\n",
      "Epoch: 4 \tBatch: 259 \tLoss: 1.4846638441085815\n",
      "Epoch: 4 \tBatch: 260 \tLoss: 1.4618117809295654\n",
      "Epoch: 4 \tBatch: 261 \tLoss: 1.471175193786621\n",
      "Epoch: 4 \tBatch: 262 \tLoss: 1.473334550857544\n",
      "Epoch: 4 \tBatch: 263 \tLoss: 1.4724160432815552\n",
      "Epoch: 4 \tBatch: 264 \tLoss: 1.4919837713241577\n",
      "Epoch: 4 \tBatch: 265 \tLoss: 1.4775621891021729\n",
      "Epoch: 4 \tBatch: 266 \tLoss: 1.4854505062103271\n",
      "Epoch: 4 \tBatch: 267 \tLoss: 1.4760653972625732\n",
      "Epoch: 4 \tBatch: 268 \tLoss: 1.4619555473327637\n",
      "Epoch: 4 \tBatch: 269 \tLoss: 1.4632368087768555\n",
      "Epoch: 4 \tBatch: 270 \tLoss: 1.4801143407821655\n",
      "Epoch: 4 \tBatch: 271 \tLoss: 1.470750331878662\n",
      "Epoch: 4 \tBatch: 272 \tLoss: 1.5115561485290527\n",
      "Epoch: 4 \tBatch: 273 \tLoss: 1.4788788557052612\n",
      "Epoch: 4 \tBatch: 274 \tLoss: 1.4945857524871826\n",
      "Epoch: 4 \tBatch: 275 \tLoss: 1.5102503299713135\n",
      "Epoch: 4 \tBatch: 276 \tLoss: 1.4740468263626099\n",
      "Epoch: 4 \tBatch: 277 \tLoss: 1.481026530265808\n",
      "Epoch: 4 \tBatch: 278 \tLoss: 1.485739827156067\n",
      "Epoch: 4 \tBatch: 279 \tLoss: 1.4859548807144165\n",
      "Epoch: 4 \tBatch: 280 \tLoss: 1.4778043031692505\n",
      "Epoch: 4 \tBatch: 281 \tLoss: 1.4912500381469727\n",
      "Epoch: 4 \tBatch: 282 \tLoss: 1.4625351428985596\n",
      "Epoch: 4 \tBatch: 283 \tLoss: 1.47896146774292\n",
      "Epoch: 4 \tBatch: 284 \tLoss: 1.466385841369629\n",
      "Epoch: 4 \tBatch: 285 \tLoss: 1.4858638048171997\n",
      "Epoch: 4 \tBatch: 286 \tLoss: 1.4617042541503906\n",
      "Epoch: 4 \tBatch: 287 \tLoss: 1.476436734199524\n",
      "Epoch: 4 \tBatch: 288 \tLoss: 1.4698340892791748\n",
      "Epoch: 4 \tBatch: 289 \tLoss: 1.4736661911010742\n",
      "Epoch: 4 \tBatch: 290 \tLoss: 1.4689165353775024\n",
      "Epoch: 4 \tBatch: 291 \tLoss: 1.4884226322174072\n",
      "Epoch: 4 \tBatch: 292 \tLoss: 1.469631314277649\n",
      "Epoch: 4 \tBatch: 293 \tLoss: 1.4814250469207764\n",
      "Epoch: 4 \tBatch: 294 \tLoss: 1.4651646614074707\n",
      "Epoch: 4 \tBatch: 295 \tLoss: 1.486466407775879\n",
      "Epoch: 4 \tBatch: 296 \tLoss: 1.4779455661773682\n",
      "Epoch: 4 \tBatch: 297 \tLoss: 1.470589518547058\n",
      "Epoch: 4 \tBatch: 298 \tLoss: 1.4731855392456055\n",
      "Epoch: 4 \tBatch: 299 \tLoss: 1.4701184034347534\n",
      "Epoch: 4 \tBatch: 300 \tLoss: 1.4774283170700073\n",
      "Epoch: 4 \tBatch: 301 \tLoss: 1.4811593294143677\n",
      "Epoch: 4 \tBatch: 302 \tLoss: 1.493017554283142\n",
      "Epoch: 4 \tBatch: 303 \tLoss: 1.4801069498062134\n",
      "Epoch: 4 \tBatch: 304 \tLoss: 1.4744261503219604\n",
      "Epoch: 4 \tBatch: 305 \tLoss: 1.484645128250122\n",
      "Epoch: 4 \tBatch: 306 \tLoss: 1.4786746501922607\n",
      "Epoch: 4 \tBatch: 307 \tLoss: 1.4813010692596436\n",
      "Epoch: 4 \tBatch: 308 \tLoss: 1.4822579622268677\n",
      "Epoch: 4 \tBatch: 309 \tLoss: 1.47945237159729\n",
      "Epoch: 4 \tBatch: 310 \tLoss: 1.4870256185531616\n",
      "Epoch: 4 \tBatch: 311 \tLoss: 1.4621509313583374\n",
      "Epoch: 4 \tBatch: 312 \tLoss: 1.4825557470321655\n",
      "Epoch: 4 \tBatch: 313 \tLoss: 1.4831726551055908\n",
      "Epoch: 4 \tBatch: 314 \tLoss: 1.4827910661697388\n",
      "Epoch: 4 \tBatch: 315 \tLoss: 1.4701733589172363\n",
      "Epoch: 4 \tBatch: 316 \tLoss: 1.4773213863372803\n",
      "Epoch: 4 \tBatch: 317 \tLoss: 1.4872174263000488\n",
      "Epoch: 4 \tBatch: 318 \tLoss: 1.4632320404052734\n",
      "Epoch: 4 \tBatch: 319 \tLoss: 1.4735352993011475\n",
      "Epoch: 4 \tBatch: 320 \tLoss: 1.4688860177993774\n",
      "Epoch: 4 \tBatch: 321 \tLoss: 1.476520299911499\n",
      "Epoch: 4 \tBatch: 322 \tLoss: 1.4896368980407715\n",
      "Epoch: 4 \tBatch: 323 \tLoss: 1.4893027544021606\n",
      "Epoch: 4 \tBatch: 324 \tLoss: 1.4792020320892334\n",
      "Epoch: 4 \tBatch: 325 \tLoss: 1.480724573135376\n",
      "Epoch: 4 \tBatch: 326 \tLoss: 1.4957189559936523\n",
      "Epoch: 4 \tBatch: 327 \tLoss: 1.4900729656219482\n",
      "Epoch: 4 \tBatch: 328 \tLoss: 1.4891605377197266\n",
      "Epoch: 4 \tBatch: 329 \tLoss: 1.4715336561203003\n",
      "Epoch: 4 \tBatch: 330 \tLoss: 1.4624414443969727\n",
      "Epoch: 4 \tBatch: 331 \tLoss: 1.5028233528137207\n",
      "Epoch: 4 \tBatch: 332 \tLoss: 1.486438274383545\n",
      "Epoch: 4 \tBatch: 333 \tLoss: 1.4705601930618286\n",
      "Epoch: 4 \tBatch: 334 \tLoss: 1.4964696168899536\n",
      "Epoch: 4 \tBatch: 335 \tLoss: 1.4886589050292969\n",
      "Epoch: 4 \tBatch: 336 \tLoss: 1.4933743476867676\n",
      "Epoch: 4 \tBatch: 337 \tLoss: 1.461896300315857\n",
      "Epoch: 4 \tBatch: 338 \tLoss: 1.4922010898590088\n",
      "Epoch: 4 \tBatch: 339 \tLoss: 1.4685769081115723\n",
      "Epoch: 4 \tBatch: 340 \tLoss: 1.5064617395401\n",
      "Epoch: 4 \tBatch: 341 \tLoss: 1.4832013845443726\n",
      "Epoch: 4 \tBatch: 342 \tLoss: 1.4937769174575806\n",
      "Epoch: 4 \tBatch: 343 \tLoss: 1.4820810556411743\n",
      "Epoch: 4 \tBatch: 344 \tLoss: 1.4722490310668945\n",
      "Epoch: 4 \tBatch: 345 \tLoss: 1.4725888967514038\n",
      "Epoch: 4 \tBatch: 346 \tLoss: 1.4637902975082397\n",
      "Epoch: 4 \tBatch: 347 \tLoss: 1.4826362133026123\n",
      "Epoch: 4 \tBatch: 348 \tLoss: 1.4791754484176636\n",
      "Epoch: 4 \tBatch: 349 \tLoss: 1.4730567932128906\n",
      "Epoch: 4 \tBatch: 350 \tLoss: 1.4818520545959473\n",
      "Epoch: 4 \tBatch: 351 \tLoss: 1.4777600765228271\n",
      "Epoch: 4 \tBatch: 352 \tLoss: 1.4912701845169067\n",
      "Epoch: 4 \tBatch: 353 \tLoss: 1.4971044063568115\n",
      "Epoch: 4 \tBatch: 354 \tLoss: 1.478380799293518\n",
      "Epoch: 4 \tBatch: 355 \tLoss: 1.4647847414016724\n",
      "Epoch: 4 \tBatch: 356 \tLoss: 1.4693169593811035\n",
      "Epoch: 4 \tBatch: 357 \tLoss: 1.484544038772583\n",
      "Epoch: 4 \tBatch: 358 \tLoss: 1.469215750694275\n",
      "Epoch: 4 \tBatch: 359 \tLoss: 1.469728946685791\n",
      "Epoch: 4 \tBatch: 360 \tLoss: 1.4931446313858032\n",
      "Epoch: 4 \tBatch: 361 \tLoss: 1.4819142818450928\n",
      "Epoch: 4 \tBatch: 362 \tLoss: 1.478521704673767\n",
      "Epoch: 4 \tBatch: 363 \tLoss: 1.4712340831756592\n",
      "Epoch: 4 \tBatch: 364 \tLoss: 1.4679945707321167\n",
      "Epoch: 4 \tBatch: 365 \tLoss: 1.4693412780761719\n",
      "Epoch: 4 \tBatch: 366 \tLoss: 1.462541937828064\n",
      "Epoch: 4 \tBatch: 367 \tLoss: 1.478973150253296\n",
      "Epoch: 4 \tBatch: 368 \tLoss: 1.4796570539474487\n",
      "Epoch: 4 \tBatch: 369 \tLoss: 1.4798582792282104\n",
      "Epoch: 4 \tBatch: 370 \tLoss: 1.4717283248901367\n",
      "Epoch: 4 \tBatch: 371 \tLoss: 1.4744247198104858\n",
      "Epoch: 4 \tBatch: 372 \tLoss: 1.491227388381958\n",
      "Epoch: 4 \tBatch: 373 \tLoss: 1.475404977798462\n",
      "Epoch: 4 \tBatch: 374 \tLoss: 1.4825997352600098\n",
      "Epoch: 4 \tBatch: 375 \tLoss: 1.486480474472046\n",
      "Epoch: 4 \tBatch: 376 \tLoss: 1.4684739112854004\n",
      "Epoch: 4 \tBatch: 377 \tLoss: 1.4648699760437012\n",
      "Epoch: 4 \tBatch: 378 \tLoss: 1.4737446308135986\n",
      "Epoch: 4 \tBatch: 379 \tLoss: 1.4809621572494507\n",
      "Epoch: 4 \tBatch: 380 \tLoss: 1.4920566082000732\n",
      "Epoch: 4 \tBatch: 381 \tLoss: 1.4691681861877441\n",
      "Epoch: 4 \tBatch: 382 \tLoss: 1.476628303527832\n",
      "Epoch: 4 \tBatch: 383 \tLoss: 1.4714770317077637\n",
      "Epoch: 4 \tBatch: 384 \tLoss: 1.5069066286087036\n",
      "Epoch: 4 \tBatch: 385 \tLoss: 1.4616190195083618\n",
      "Epoch: 4 \tBatch: 386 \tLoss: 1.462393879890442\n",
      "Epoch: 4 \tBatch: 387 \tLoss: 1.464563250541687\n",
      "Epoch: 4 \tBatch: 388 \tLoss: 1.47118079662323\n",
      "Epoch: 4 \tBatch: 389 \tLoss: 1.4627476930618286\n",
      "Epoch: 4 \tBatch: 390 \tLoss: 1.4849488735198975\n",
      "Training Complete. Final loss = 1.4849488735198975\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "train(cnn, epochs)\n",
    "print('time taken:', time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def calc_accuracy(model, dataloader):\n",
    "    num_correct = 0\n",
    "    num_examples = len(dataloader.dataset)                       # test DATA not test LOADER\n",
    "    for inputs, labels in dataloader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        predictions = model(inputs)\n",
    "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
    "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Accuracy:', calc_accuracy(cnn, train_loader))\n",
    "print('Validation Accuracy:', calc_accuracy(cnn, val_loader))\n",
    "print('Test Accuracy:', calc_accuracy(cnn, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Benchmarking of Fully Connected vs Convolutional Architecture on MNIST\n",
    "\n",
    "Lets empirically test the translation invariance of the two architectures. \n",
    "\n",
    "The training set now has a center crop transformation which crops the central pixels of the image to a given size. However, the test set has a random crop transformation which crops a random region of the image to a given size.\n",
    "\n",
    "So we are training the neural networks with a centrally cropped image but testing it on cropped images with translations applied.\n",
    "\n",
    "The network that has more translation invariant features should performs better on this test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "crop_size = 22\n",
    "\n",
    "traintransforms = []\n",
    "traintransforms.append(transforms.CenterCrop(crop_size))\n",
    "traintransforms.append(transforms.ToTensor())\n",
    "traintransforms = transforms.Compose(traintransforms)\n",
    "\n",
    "# GET THE TRAINING DATASET\n",
    "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
    "                            transform=traintransforms,          # transform the data from a PIL image to a tensor\n",
    "                            train=True,                               # is this training data?\n",
    "                            download=True                             # should i download it if it's not already here?\n",
    "                           )\n",
    "\n",
    "testtransforms = []\n",
    "testtransforms.append(transforms.RandomCrop(crop_size))\n",
    "testtransforms.append(transforms.ToTensor())\n",
    "testtransforms = transforms.Compose(testtransforms)\n",
    "\n",
    "# GET THE TEST DATASET\n",
    "test_data = datasets.MNIST(root='MNIST-data',\n",
    "                           transform=testtransforms,\n",
    "                           train=False,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# PRINT THEIR LENGTHS AND VISUALISE AN EXAMPLE\n",
    "x = train_data[np.random.randint(0, len(train_data))][0]    # get a random training example\n",
    "print('Train Example')\n",
    "plt.imshow(x[0].numpy())\n",
    "plt.show()\n",
    "x = test_data[np.random.randint(0, len(test_data))][0]    # get a random test example\n",
    "print('Test Example')\n",
    "plt.imshow(x[0].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# MAKE TRAINING DATALOADER\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE VALIDATION DATALOADER\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE TEST DATALOADER\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the architectures of our fully connected and convolutional networks as well as a function which returns the number of parameters in each model. Since the number of parameters in a model is a rough measure of its capacity, the networks should have an approximately equal number of parameters to make it a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "class FullyConnectedNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()    # initialise parent module\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(crop_size*crop_size, 225),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(225, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(50, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, crop_size*crop_size)\n",
    "        x = self.fc_layers(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = torch.nn.Sequential( # put your convolutional architecture here using torch.nn.Sequential \n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64*(crop_size-12)*(crop_size-12), 10) # put your convolutional architecture here using torch.nn.Sequential \n",
    "            \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)# pass through conv layers\n",
    "        x = x.view(x.shape[0], -1)# flatten output ready for fully connected layer\n",
    "        x = self.fc_layers(x)# pass through fully connected layer\n",
    "        x = F.softmax(x, dim=1)# softmax activation function on outputs\n",
    "        return x\n",
    "\n",
    "def get_n_params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005 # set learning rate\n",
    "epochs = 5 # set number of epochs\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = ConvNet().to(device)#instantiate model\n",
    "print('Number of parameters in cnn:', get_n_params(cnn))\n",
    "criterion = torch.nn.CrossEntropyLoss() #use cross entropy loss function\n",
    "optimiser = torch.optim.Adam(cnn.parameters(), lr=learning_rate) # use Adam optimizer, passing it the parameters of your model and the learning rate\n",
    "train(cnn, epochs, verbose=False, tag='CNN Loss/Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn = FullyConnectedNet().to(device)\n",
    "print('Number of parameters in fnn:', get_n_params(fnn))\n",
    "optimiser = torch.optim.Adam(fnn.parameters(), lr=learning_rate) # use Adam optimizer, passing it the parameters of your model and the learning rate\n",
    "train(fnn, epochs, verbose=False, tag='FNN Loss/Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CNN Train Accuracy:', calc_accuracy(cnn, train_loader))\n",
    "print('CNN Validation Accuracy:', calc_accuracy(cnn, val_loader))\n",
    "print('CNN Test Accuracy:', calc_accuracy(cnn, test_loader), '\\n')\n",
    "\n",
    "print('FNN Train Accuracy:', calc_accuracy(fnn, train_loader))\n",
    "print('FNN Validation Accuracy:', calc_accuracy(fnn, val_loader))\n",
    "print('FNN Test Accuracy:', calc_accuracy(fnn, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, there is a significant disparity between the test accuracy of the two architectures, with the CNN have ~+20% test accuracy\n",
    "\n",
    "__Next steps__\n",
    "\n",
    "- [Custom Datasets](https://github.com/AICore/ConvolutionalNeuralNetworks/blob/master/Custom%20Datasets.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "The fully connected neural network we looked at in the previous lesson takes in a vector as input thus a flattened image could be passed in as input and used for classification problems successfully. But this is not the best way to do it. When trying to interpret an image, the spatial relations between the different pixels is crucial to our understanding. When we flatten the image, we lose this information.\n",
    "\n",
    "Convolutional neural networks solve this very problem. Rather than performing a matrix multiplication, a convolution operation is performed which can take in a 2d input and give a 2d output hence keep the information about the spatial relations of the pixels. This greatly increases their performance on image and video processing tasks.\n",
    "\n",
    "In the convolution proccess, you have a filter which you start on the top left side of the image and slide across the whole image, taking a dot product between the values of the filter and pixel values of the image. Bear in mind that colour images have three colour channels so your filter may be 3d so you take the dot product across a 3d volume. Each dot product corresponds to a single activation value in a 2d matrix of neurons which corresponds to a single layer in the output.\n",
    "\n",
    "![image](images/CNN_RGB.JPG)\n",
    "\n",
    "The animation below shows how a 1x3x3 filter is applied to a 1x5x5 image. Notice how the output has high  values when the filter is passed over locations where there is an X shape in the input image. This is because the values of the filter are such that it is performing pattern matching for the X shape.\n",
    "\n",
    "![image](images/convolution_animation.gif)\n",
    "\n",
    "Convolution operations have 3 parameters, the kernel size, which is the width and height of our filter, the stride, which is the number of pixels we translate our kernel by to compute the next feature, and the padding which is how many layers of 0 padding we add to the input image. We som\n",
    "\n",
    "![image](images/CNN_diagram.JPG)\n",
    "\n",
    "Each computed feature is a linear function of the pixels in a local region as opposed to fully connected nets where each computed feature is a linear function of all the pixels in the image.\n",
    "\n",
    "We have some prior understanding of how image data should be processed. We apply the same set of weights at different locations all over the image because we know that features are repeated at different locations throughout the image. This makes the learned features translation invariant.\n",
    "\n",
    "\n",
    "![image](images/CNN_FNN_comparison.JPG)\n",
    "\n",
    "\n",
    "For a long time, operations like this were used in computer vision to find different patterns in images with the engineers having to manually tune the values of the filters to perform the required function. The only difference now is that we apply an activation such as Relu or Sigmoid at each layer and after setting up the structure of the network, we initialize the filter values randomly before using gradient descent to automatically tune the values of the filters. We can also apply max pooling operations to subsample the output at each layer therefore reducing the number of parameters that need to be learned for the succeeding convolution operation.\n",
    "\n",
    "\n",
    "Just like before, each layer in the whole network learns higher level abstract features from the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# GET THE TRAINING DATASET\n",
    "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
    "                            transform=transforms.ToTensor(),          # transform the data from a PIL image to a tensor\n",
    "                            train=True,                               # is this training data?\n",
    "                            download=True                             # should i download it if it's not already here?\n",
    "                           )\n",
    "\n",
    "# GET THE TEST DATASET\n",
    "test_data = datasets.MNIST(root='MNIST-data',\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           train=False,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = train_data[np.random.randint(0, 300)][0]    # get a random example\n",
    "#print(x)\n",
    "plt.imshow(x[0].numpy(),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# MAKE TRAINING DATALOADER\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE VALIDATION DATALOADER\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE TEST DATALOADER\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            # conv2d(in_channels, out_channels, kernel_size)\n",
    "            # in_channels is the number of layers which it takes in (i.e.num color channels in 1st layer)\n",
    "            # out_channels is the number of different filters that we use\n",
    "            # kernel_size is the depthxwidthxheight of the kernel#\n",
    "            # stride is how many pixels we shift the kernel by each time\n",
    "        self.conv_layers = torch.nn.Sequential( # put your convolutional architecture here using torch.nn.Sequential \n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32*20*20, 10) # put your convolutional architecture here using torch.nn.Sequential \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)# pass through conv layers\n",
    "        x = x.view(x.shape[0], -1)# flatten output ready for fully connected layer\n",
    "        x = self.fc_layers(x)# pass through fully connected layer\n",
    "        x = F.softmax(x, dim=1)# softmax activation function on outputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available() #checks if gpu is available\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005 # set learning rate\n",
    "epochs = 5# set number of epochs\n",
    "\n",
    "cnn = ConvNet().to(device)#instantiate model\n",
    "criterion = torch.nn.CrossEntropyLoss() #use cross entropy loss function\n",
    "optimiser = torch.optim.Adam(cnn.parameters(), lr=learning_rate) # use Adam optimizer, passing it the parameters of your model and the learning rate\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, verbose=True, tag='Loss/Train'):\n",
    "    for epoch in range(epochs):\n",
    "        for idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # pass x through your model to get a prediction\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the cost\n",
    "            if verbose: print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss.item())\n",
    "            \n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of all of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and store all of the model's param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            \n",
    "            writer.add_scalar(tag, loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "    print('Training Complete. Final loss =',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 0 \tLoss: 2.301351547241211\n",
      "Epoch: 0 \tBatch: 1 \tLoss: 2.2933883666992188\n",
      "Epoch: 0 \tBatch: 2 \tLoss: 2.2848145961761475\n",
      "Epoch: 0 \tBatch: 3 \tLoss: 2.2663705348968506\n",
      "Epoch: 0 \tBatch: 4 \tLoss: 2.2432281970977783\n",
      "Epoch: 0 \tBatch: 5 \tLoss: 2.246927261352539\n",
      "Epoch: 0 \tBatch: 6 \tLoss: 2.2052159309387207\n",
      "Epoch: 0 \tBatch: 7 \tLoss: 2.1649117469787598\n",
      "Epoch: 0 \tBatch: 8 \tLoss: 2.1620545387268066\n",
      "Epoch: 0 \tBatch: 9 \tLoss: 2.1345627307891846\n",
      "Epoch: 0 \tBatch: 10 \tLoss: 2.1192588806152344\n",
      "Epoch: 0 \tBatch: 11 \tLoss: 2.0726356506347656\n",
      "Epoch: 0 \tBatch: 12 \tLoss: 2.042109489440918\n",
      "Epoch: 0 \tBatch: 13 \tLoss: 2.0222980976104736\n",
      "Epoch: 0 \tBatch: 14 \tLoss: 1.9543883800506592\n",
      "Epoch: 0 \tBatch: 15 \tLoss: 1.894980788230896\n",
      "Epoch: 0 \tBatch: 16 \tLoss: 1.9101344347000122\n",
      "Epoch: 0 \tBatch: 17 \tLoss: 1.9213521480560303\n",
      "Epoch: 0 \tBatch: 18 \tLoss: 1.9399389028549194\n",
      "Epoch: 0 \tBatch: 19 \tLoss: 1.8844553232192993\n",
      "Epoch: 0 \tBatch: 20 \tLoss: 1.834992527961731\n",
      "Epoch: 0 \tBatch: 21 \tLoss: 1.8786529302597046\n",
      "Epoch: 0 \tBatch: 22 \tLoss: 1.8539484739303589\n",
      "Epoch: 0 \tBatch: 23 \tLoss: 1.8782845735549927\n",
      "Epoch: 0 \tBatch: 24 \tLoss: 1.8104835748672485\n",
      "Epoch: 0 \tBatch: 25 \tLoss: 1.8564727306365967\n",
      "Epoch: 0 \tBatch: 26 \tLoss: 1.7975680828094482\n",
      "Epoch: 0 \tBatch: 27 \tLoss: 1.7930887937545776\n",
      "Epoch: 0 \tBatch: 28 \tLoss: 1.8318284749984741\n",
      "Epoch: 0 \tBatch: 29 \tLoss: 1.8465607166290283\n",
      "Epoch: 0 \tBatch: 30 \tLoss: 1.776671290397644\n",
      "Epoch: 0 \tBatch: 31 \tLoss: 1.8984454870224\n",
      "Epoch: 0 \tBatch: 32 \tLoss: 1.7543373107910156\n",
      "Epoch: 0 \tBatch: 33 \tLoss: 1.8526318073272705\n",
      "Epoch: 0 \tBatch: 34 \tLoss: 1.8349567651748657\n",
      "Epoch: 0 \tBatch: 35 \tLoss: 1.890095829963684\n",
      "Epoch: 0 \tBatch: 36 \tLoss: 1.847267746925354\n",
      "Epoch: 0 \tBatch: 37 \tLoss: 1.779279112815857\n",
      "Epoch: 0 \tBatch: 38 \tLoss: 1.731417179107666\n",
      "Epoch: 0 \tBatch: 39 \tLoss: 1.7263154983520508\n",
      "Epoch: 0 \tBatch: 40 \tLoss: 1.7533303499221802\n",
      "Epoch: 0 \tBatch: 41 \tLoss: 1.7581284046173096\n",
      "Epoch: 0 \tBatch: 42 \tLoss: 1.6890699863433838\n",
      "Epoch: 0 \tBatch: 43 \tLoss: 1.750502109527588\n",
      "Epoch: 0 \tBatch: 44 \tLoss: 1.7394065856933594\n",
      "Epoch: 0 \tBatch: 45 \tLoss: 1.7847261428833008\n",
      "Epoch: 0 \tBatch: 46 \tLoss: 1.7343106269836426\n",
      "Epoch: 0 \tBatch: 47 \tLoss: 1.7487739324569702\n",
      "Epoch: 0 \tBatch: 48 \tLoss: 1.7771745920181274\n",
      "Epoch: 0 \tBatch: 49 \tLoss: 1.7380493879318237\n",
      "Epoch: 0 \tBatch: 50 \tLoss: 1.7665339708328247\n",
      "Epoch: 0 \tBatch: 51 \tLoss: 1.7507050037384033\n",
      "Epoch: 0 \tBatch: 52 \tLoss: 1.824931025505066\n",
      "Epoch: 0 \tBatch: 53 \tLoss: 1.7755882740020752\n",
      "Epoch: 0 \tBatch: 54 \tLoss: 1.8023704290390015\n",
      "Epoch: 0 \tBatch: 55 \tLoss: 1.7349750995635986\n",
      "Epoch: 0 \tBatch: 56 \tLoss: 1.690971851348877\n",
      "Epoch: 0 \tBatch: 57 \tLoss: 1.7628158330917358\n",
      "Epoch: 0 \tBatch: 58 \tLoss: 1.7357792854309082\n",
      "Epoch: 0 \tBatch: 59 \tLoss: 1.6489336490631104\n",
      "Epoch: 0 \tBatch: 60 \tLoss: 1.7578462362289429\n",
      "Epoch: 0 \tBatch: 61 \tLoss: 1.7165573835372925\n",
      "Epoch: 0 \tBatch: 62 \tLoss: 1.7356830835342407\n",
      "Epoch: 0 \tBatch: 63 \tLoss: 1.6953519582748413\n",
      "Epoch: 0 \tBatch: 64 \tLoss: 1.6929837465286255\n",
      "Epoch: 0 \tBatch: 65 \tLoss: 1.6416289806365967\n",
      "Epoch: 0 \tBatch: 66 \tLoss: 1.735581874847412\n",
      "Epoch: 0 \tBatch: 67 \tLoss: 1.7105646133422852\n",
      "Epoch: 0 \tBatch: 68 \tLoss: 1.7130566835403442\n",
      "Epoch: 0 \tBatch: 69 \tLoss: 1.7132478952407837\n",
      "Epoch: 0 \tBatch: 70 \tLoss: 1.7623388767242432\n",
      "Epoch: 0 \tBatch: 71 \tLoss: 1.7503114938735962\n",
      "Epoch: 0 \tBatch: 72 \tLoss: 1.6904239654541016\n",
      "Epoch: 0 \tBatch: 73 \tLoss: 1.7254390716552734\n",
      "Epoch: 0 \tBatch: 74 \tLoss: 1.6967005729675293\n",
      "Epoch: 0 \tBatch: 75 \tLoss: 1.690961241722107\n",
      "Epoch: 0 \tBatch: 76 \tLoss: 1.697696566581726\n",
      "Epoch: 0 \tBatch: 77 \tLoss: 1.7496083974838257\n",
      "Epoch: 0 \tBatch: 78 \tLoss: 1.674696922302246\n",
      "Epoch: 0 \tBatch: 79 \tLoss: 1.6668322086334229\n",
      "Epoch: 0 \tBatch: 80 \tLoss: 1.7107957601547241\n",
      "Epoch: 0 \tBatch: 81 \tLoss: 1.6897526979446411\n",
      "Epoch: 0 \tBatch: 82 \tLoss: 1.7749477624893188\n",
      "Epoch: 0 \tBatch: 83 \tLoss: 1.7340108156204224\n",
      "Epoch: 0 \tBatch: 84 \tLoss: 1.7824229001998901\n",
      "Epoch: 0 \tBatch: 85 \tLoss: 1.7312500476837158\n",
      "Epoch: 0 \tBatch: 86 \tLoss: 1.6706640720367432\n",
      "Epoch: 0 \tBatch: 87 \tLoss: 1.7582145929336548\n",
      "Epoch: 0 \tBatch: 88 \tLoss: 1.759381890296936\n",
      "Epoch: 0 \tBatch: 89 \tLoss: 1.767259955406189\n",
      "Epoch: 0 \tBatch: 90 \tLoss: 1.6766785383224487\n",
      "Epoch: 0 \tBatch: 91 \tLoss: 1.755341649055481\n",
      "Epoch: 0 \tBatch: 92 \tLoss: 1.7125102281570435\n",
      "Epoch: 0 \tBatch: 93 \tLoss: 1.6953462362289429\n",
      "Epoch: 0 \tBatch: 94 \tLoss: 1.6774280071258545\n",
      "Epoch: 0 \tBatch: 95 \tLoss: 1.7571768760681152\n",
      "Epoch: 0 \tBatch: 96 \tLoss: 1.6999632120132446\n",
      "Epoch: 0 \tBatch: 97 \tLoss: 1.7502036094665527\n",
      "Epoch: 0 \tBatch: 98 \tLoss: 1.706799030303955\n",
      "Epoch: 0 \tBatch: 99 \tLoss: 1.7450628280639648\n",
      "Epoch: 0 \tBatch: 100 \tLoss: 1.6647088527679443\n",
      "Epoch: 0 \tBatch: 101 \tLoss: 1.8130048513412476\n",
      "Epoch: 0 \tBatch: 102 \tLoss: 1.7364445924758911\n",
      "Epoch: 0 \tBatch: 103 \tLoss: 1.7455745935440063\n",
      "Epoch: 0 \tBatch: 104 \tLoss: 1.706058382987976\n",
      "Epoch: 0 \tBatch: 105 \tLoss: 1.7079254388809204\n",
      "Epoch: 0 \tBatch: 106 \tLoss: 1.7052751779556274\n",
      "Epoch: 0 \tBatch: 107 \tLoss: 1.747513771057129\n",
      "Epoch: 0 \tBatch: 108 \tLoss: 1.6607635021209717\n",
      "Epoch: 0 \tBatch: 109 \tLoss: 1.6817365884780884\n",
      "Epoch: 0 \tBatch: 110 \tLoss: 1.7436457872390747\n",
      "Epoch: 0 \tBatch: 111 \tLoss: 1.7153574228286743\n",
      "Epoch: 0 \tBatch: 112 \tLoss: 1.6830657720565796\n",
      "Epoch: 0 \tBatch: 113 \tLoss: 1.7260010242462158\n",
      "Epoch: 0 \tBatch: 114 \tLoss: 1.7039669752120972\n",
      "Epoch: 0 \tBatch: 115 \tLoss: 1.6853229999542236\n",
      "Epoch: 0 \tBatch: 116 \tLoss: 1.706802248954773\n",
      "Epoch: 0 \tBatch: 117 \tLoss: 1.695158839225769\n",
      "Epoch: 0 \tBatch: 118 \tLoss: 1.7182400226593018\n",
      "Epoch: 0 \tBatch: 119 \tLoss: 1.7616394758224487\n",
      "Epoch: 0 \tBatch: 120 \tLoss: 1.7477449178695679\n",
      "Epoch: 0 \tBatch: 121 \tLoss: 1.7050367593765259\n",
      "Epoch: 0 \tBatch: 122 \tLoss: 1.717558741569519\n",
      "Epoch: 0 \tBatch: 123 \tLoss: 1.7251309156417847\n",
      "Epoch: 0 \tBatch: 124 \tLoss: 1.7362221479415894\n",
      "Epoch: 0 \tBatch: 125 \tLoss: 1.7191818952560425\n",
      "Epoch: 0 \tBatch: 126 \tLoss: 1.7010180950164795\n",
      "Epoch: 0 \tBatch: 127 \tLoss: 1.6925303936004639\n",
      "Epoch: 0 \tBatch: 128 \tLoss: 1.6810933351516724\n",
      "Epoch: 0 \tBatch: 129 \tLoss: 1.7017862796783447\n",
      "Epoch: 0 \tBatch: 130 \tLoss: 1.740942358970642\n",
      "Epoch: 0 \tBatch: 131 \tLoss: 1.673572063446045\n",
      "Epoch: 0 \tBatch: 132 \tLoss: 1.6823723316192627\n",
      "Epoch: 0 \tBatch: 133 \tLoss: 1.720507264137268\n",
      "Epoch: 0 \tBatch: 134 \tLoss: 1.7090758085250854\n",
      "Epoch: 0 \tBatch: 135 \tLoss: 1.7891911268234253\n",
      "Epoch: 0 \tBatch: 136 \tLoss: 1.7007334232330322\n",
      "Epoch: 0 \tBatch: 137 \tLoss: 1.6687510013580322\n",
      "Epoch: 0 \tBatch: 138 \tLoss: 1.7373988628387451\n",
      "Epoch: 0 \tBatch: 139 \tLoss: 1.7182365655899048\n",
      "Epoch: 0 \tBatch: 140 \tLoss: 1.6214866638183594\n",
      "Epoch: 0 \tBatch: 141 \tLoss: 1.749751091003418\n",
      "Epoch: 0 \tBatch: 142 \tLoss: 1.7282699346542358\n",
      "Epoch: 0 \tBatch: 143 \tLoss: 1.7143079042434692\n",
      "Epoch: 0 \tBatch: 144 \tLoss: 1.7557859420776367\n",
      "Epoch: 0 \tBatch: 145 \tLoss: 1.7288421392440796\n",
      "Epoch: 0 \tBatch: 146 \tLoss: 1.706628441810608\n",
      "Epoch: 0 \tBatch: 147 \tLoss: 1.7722833156585693\n",
      "Epoch: 0 \tBatch: 148 \tLoss: 1.6975034475326538\n",
      "Epoch: 0 \tBatch: 149 \tLoss: 1.7234770059585571\n",
      "Epoch: 0 \tBatch: 150 \tLoss: 1.6819875240325928\n",
      "Epoch: 0 \tBatch: 151 \tLoss: 1.6964778900146484\n",
      "Epoch: 0 \tBatch: 152 \tLoss: 1.7188291549682617\n",
      "Epoch: 0 \tBatch: 153 \tLoss: 1.7749665975570679\n",
      "Epoch: 0 \tBatch: 154 \tLoss: 1.7089126110076904\n",
      "Epoch: 0 \tBatch: 155 \tLoss: 1.6346420049667358\n",
      "Epoch: 0 \tBatch: 156 \tLoss: 1.6723140478134155\n",
      "Epoch: 0 \tBatch: 157 \tLoss: 1.6613134145736694\n",
      "Epoch: 0 \tBatch: 158 \tLoss: 1.6474323272705078\n",
      "Epoch: 0 \tBatch: 159 \tLoss: 1.7315326929092407\n",
      "Epoch: 0 \tBatch: 160 \tLoss: 1.6698789596557617\n",
      "Epoch: 0 \tBatch: 161 \tLoss: 1.7031941413879395\n",
      "Epoch: 0 \tBatch: 162 \tLoss: 1.7012596130371094\n",
      "Epoch: 0 \tBatch: 163 \tLoss: 1.6554996967315674\n",
      "Epoch: 0 \tBatch: 164 \tLoss: 1.664186954498291\n",
      "Epoch: 0 \tBatch: 165 \tLoss: 1.7321560382843018\n",
      "Epoch: 0 \tBatch: 166 \tLoss: 1.7625133991241455\n",
      "Epoch: 0 \tBatch: 167 \tLoss: 1.6985117197036743\n",
      "Epoch: 0 \tBatch: 168 \tLoss: 1.6995937824249268\n",
      "Epoch: 0 \tBatch: 169 \tLoss: 1.6763345003128052\n",
      "Epoch: 0 \tBatch: 170 \tLoss: 1.6847667694091797\n",
      "Epoch: 0 \tBatch: 171 \tLoss: 1.7198083400726318\n",
      "Epoch: 0 \tBatch: 172 \tLoss: 1.6872962713241577\n",
      "Epoch: 0 \tBatch: 173 \tLoss: 1.6546233892440796\n",
      "Epoch: 0 \tBatch: 174 \tLoss: 1.6978193521499634\n",
      "Epoch: 0 \tBatch: 175 \tLoss: 1.7082886695861816\n",
      "Epoch: 0 \tBatch: 176 \tLoss: 1.7112523317337036\n",
      "Epoch: 0 \tBatch: 177 \tLoss: 1.6791666746139526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 178 \tLoss: 1.712011456489563\n",
      "Epoch: 0 \tBatch: 179 \tLoss: 1.7029533386230469\n",
      "Epoch: 0 \tBatch: 180 \tLoss: 1.7415062189102173\n",
      "Epoch: 0 \tBatch: 181 \tLoss: 1.7968254089355469\n",
      "Epoch: 0 \tBatch: 182 \tLoss: 1.6798198223114014\n",
      "Epoch: 0 \tBatch: 183 \tLoss: 1.69523286819458\n",
      "Epoch: 0 \tBatch: 184 \tLoss: 1.734512209892273\n",
      "Epoch: 0 \tBatch: 185 \tLoss: 1.6441036462783813\n",
      "Epoch: 0 \tBatch: 186 \tLoss: 1.682090401649475\n",
      "Epoch: 0 \tBatch: 187 \tLoss: 1.6791023015975952\n",
      "Epoch: 0 \tBatch: 188 \tLoss: 1.7172726392745972\n",
      "Epoch: 0 \tBatch: 189 \tLoss: 1.65831458568573\n",
      "Epoch: 0 \tBatch: 190 \tLoss: 1.7514275312423706\n",
      "Epoch: 0 \tBatch: 191 \tLoss: 1.6570382118225098\n",
      "Epoch: 0 \tBatch: 192 \tLoss: 1.7559977769851685\n",
      "Epoch: 0 \tBatch: 193 \tLoss: 1.7008540630340576\n",
      "Epoch: 0 \tBatch: 194 \tLoss: 1.665772557258606\n",
      "Epoch: 0 \tBatch: 195 \tLoss: 1.7631592750549316\n",
      "Epoch: 0 \tBatch: 196 \tLoss: 1.695044994354248\n",
      "Epoch: 0 \tBatch: 197 \tLoss: 1.6862194538116455\n",
      "Epoch: 0 \tBatch: 198 \tLoss: 1.757136583328247\n",
      "Epoch: 0 \tBatch: 199 \tLoss: 1.7520097494125366\n",
      "Epoch: 0 \tBatch: 200 \tLoss: 1.7403066158294678\n",
      "Epoch: 0 \tBatch: 201 \tLoss: 1.7259730100631714\n",
      "Epoch: 0 \tBatch: 202 \tLoss: 1.7140250205993652\n",
      "Epoch: 0 \tBatch: 203 \tLoss: 1.649329423904419\n",
      "Epoch: 0 \tBatch: 204 \tLoss: 1.6806135177612305\n",
      "Epoch: 0 \tBatch: 205 \tLoss: 1.6885685920715332\n",
      "Epoch: 0 \tBatch: 206 \tLoss: 1.741145372390747\n",
      "Epoch: 0 \tBatch: 207 \tLoss: 1.6890755891799927\n",
      "Epoch: 0 \tBatch: 208 \tLoss: 1.6722530126571655\n",
      "Epoch: 0 \tBatch: 209 \tLoss: 1.6989123821258545\n",
      "Epoch: 0 \tBatch: 210 \tLoss: 1.733953595161438\n",
      "Epoch: 0 \tBatch: 211 \tLoss: 1.7182835340499878\n",
      "Epoch: 0 \tBatch: 212 \tLoss: 1.724080204963684\n",
      "Epoch: 0 \tBatch: 213 \tLoss: 1.742594599723816\n",
      "Epoch: 0 \tBatch: 214 \tLoss: 1.7555913925170898\n",
      "Epoch: 0 \tBatch: 215 \tLoss: 1.7136567831039429\n",
      "Epoch: 0 \tBatch: 216 \tLoss: 1.7305138111114502\n",
      "Epoch: 0 \tBatch: 217 \tLoss: 1.704607605934143\n",
      "Epoch: 0 \tBatch: 218 \tLoss: 1.6673846244812012\n",
      "Epoch: 0 \tBatch: 219 \tLoss: 1.7607511281967163\n",
      "Epoch: 0 \tBatch: 220 \tLoss: 1.7140620946884155\n",
      "Epoch: 0 \tBatch: 221 \tLoss: 1.709043264389038\n",
      "Epoch: 0 \tBatch: 222 \tLoss: 1.7061418294906616\n",
      "Epoch: 0 \tBatch: 223 \tLoss: 1.7188704013824463\n",
      "Epoch: 0 \tBatch: 224 \tLoss: 1.697163701057434\n",
      "Epoch: 0 \tBatch: 225 \tLoss: 1.6716147661209106\n",
      "Epoch: 0 \tBatch: 226 \tLoss: 1.6738965511322021\n",
      "Epoch: 0 \tBatch: 227 \tLoss: 1.7527955770492554\n",
      "Epoch: 0 \tBatch: 228 \tLoss: 1.7719188928604126\n",
      "Epoch: 0 \tBatch: 229 \tLoss: 1.6932438611984253\n",
      "Epoch: 0 \tBatch: 230 \tLoss: 1.6874953508377075\n",
      "Epoch: 0 \tBatch: 231 \tLoss: 1.656377911567688\n",
      "Epoch: 0 \tBatch: 232 \tLoss: 1.6776058673858643\n",
      "Epoch: 0 \tBatch: 233 \tLoss: 1.7613753080368042\n",
      "Epoch: 0 \tBatch: 234 \tLoss: 1.7036755084991455\n",
      "Epoch: 0 \tBatch: 235 \tLoss: 1.7657486200332642\n",
      "Epoch: 0 \tBatch: 236 \tLoss: 1.734061360359192\n",
      "Epoch: 0 \tBatch: 237 \tLoss: 1.719106912612915\n",
      "Epoch: 0 \tBatch: 238 \tLoss: 1.710425853729248\n",
      "Epoch: 0 \tBatch: 239 \tLoss: 1.703535556793213\n",
      "Epoch: 0 \tBatch: 240 \tLoss: 1.7342579364776611\n",
      "Epoch: 0 \tBatch: 241 \tLoss: 1.6636470556259155\n",
      "Epoch: 0 \tBatch: 242 \tLoss: 1.7378087043762207\n",
      "Epoch: 0 \tBatch: 243 \tLoss: 1.7337065935134888\n",
      "Epoch: 0 \tBatch: 244 \tLoss: 1.6610915660858154\n",
      "Epoch: 0 \tBatch: 245 \tLoss: 1.7268288135528564\n",
      "Epoch: 0 \tBatch: 246 \tLoss: 1.6348077058792114\n",
      "Epoch: 0 \tBatch: 247 \tLoss: 1.7280032634735107\n",
      "Epoch: 0 \tBatch: 248 \tLoss: 1.6668671369552612\n",
      "Epoch: 0 \tBatch: 249 \tLoss: 1.718017816543579\n",
      "Epoch: 0 \tBatch: 250 \tLoss: 1.7085047960281372\n",
      "Epoch: 0 \tBatch: 251 \tLoss: 1.6719436645507812\n",
      "Epoch: 0 \tBatch: 252 \tLoss: 1.741995096206665\n",
      "Epoch: 0 \tBatch: 253 \tLoss: 1.7551623582839966\n",
      "Epoch: 0 \tBatch: 254 \tLoss: 1.6645787954330444\n",
      "Epoch: 0 \tBatch: 255 \tLoss: 1.706610918045044\n",
      "Epoch: 0 \tBatch: 256 \tLoss: 1.6415016651153564\n",
      "Epoch: 0 \tBatch: 257 \tLoss: 1.728709101676941\n",
      "Epoch: 0 \tBatch: 258 \tLoss: 1.6662535667419434\n",
      "Epoch: 0 \tBatch: 259 \tLoss: 1.6413288116455078\n",
      "Epoch: 0 \tBatch: 260 \tLoss: 1.5773372650146484\n",
      "Epoch: 0 \tBatch: 261 \tLoss: 1.8004082441329956\n",
      "Epoch: 0 \tBatch: 262 \tLoss: 1.6692777872085571\n",
      "Epoch: 0 \tBatch: 263 \tLoss: 1.6955208778381348\n",
      "Epoch: 0 \tBatch: 264 \tLoss: 1.6511437892913818\n",
      "Epoch: 0 \tBatch: 265 \tLoss: 1.6688809394836426\n",
      "Epoch: 0 \tBatch: 266 \tLoss: 1.6477771997451782\n",
      "Epoch: 0 \tBatch: 267 \tLoss: 1.7148215770721436\n",
      "Epoch: 0 \tBatch: 268 \tLoss: 1.7217415571212769\n",
      "Epoch: 0 \tBatch: 269 \tLoss: 1.6694059371948242\n",
      "Epoch: 0 \tBatch: 270 \tLoss: 1.689966082572937\n",
      "Epoch: 0 \tBatch: 271 \tLoss: 1.7577744722366333\n",
      "Epoch: 0 \tBatch: 272 \tLoss: 1.6215792894363403\n",
      "Epoch: 0 \tBatch: 273 \tLoss: 1.7236981391906738\n",
      "Epoch: 0 \tBatch: 274 \tLoss: 1.6495769023895264\n",
      "Epoch: 0 \tBatch: 275 \tLoss: 1.6387572288513184\n",
      "Epoch: 0 \tBatch: 276 \tLoss: 1.7238500118255615\n",
      "Epoch: 0 \tBatch: 277 \tLoss: 1.65337073802948\n",
      "Epoch: 0 \tBatch: 278 \tLoss: 1.660325050354004\n",
      "Epoch: 0 \tBatch: 279 \tLoss: 1.7423852682113647\n",
      "Epoch: 0 \tBatch: 280 \tLoss: 1.7387535572052002\n",
      "Epoch: 0 \tBatch: 281 \tLoss: 1.7086299657821655\n",
      "Epoch: 0 \tBatch: 282 \tLoss: 1.681283712387085\n",
      "Epoch: 0 \tBatch: 283 \tLoss: 1.6643248796463013\n",
      "Epoch: 0 \tBatch: 284 \tLoss: 1.709458351135254\n",
      "Epoch: 0 \tBatch: 285 \tLoss: 1.7266017198562622\n",
      "Epoch: 0 \tBatch: 286 \tLoss: 1.6873530149459839\n",
      "Epoch: 0 \tBatch: 287 \tLoss: 1.6654160022735596\n",
      "Epoch: 0 \tBatch: 288 \tLoss: 1.7012877464294434\n",
      "Epoch: 0 \tBatch: 289 \tLoss: 1.7633671760559082\n",
      "Epoch: 0 \tBatch: 290 \tLoss: 1.714851975440979\n",
      "Epoch: 0 \tBatch: 291 \tLoss: 1.6942071914672852\n",
      "Epoch: 0 \tBatch: 292 \tLoss: 1.7562296390533447\n",
      "Epoch: 0 \tBatch: 293 \tLoss: 1.6782504320144653\n",
      "Epoch: 0 \tBatch: 294 \tLoss: 1.6829134225845337\n",
      "Epoch: 0 \tBatch: 295 \tLoss: 1.656699538230896\n",
      "Epoch: 0 \tBatch: 296 \tLoss: 1.7041586637496948\n",
      "Epoch: 0 \tBatch: 297 \tLoss: 1.6647309064865112\n",
      "Epoch: 0 \tBatch: 298 \tLoss: 1.6630250215530396\n",
      "Epoch: 0 \tBatch: 299 \tLoss: 1.675102949142456\n",
      "Epoch: 0 \tBatch: 300 \tLoss: 1.736633062362671\n",
      "Epoch: 0 \tBatch: 301 \tLoss: 1.7228832244873047\n",
      "Epoch: 0 \tBatch: 302 \tLoss: 1.7035104036331177\n",
      "Epoch: 0 \tBatch: 303 \tLoss: 1.7055071592330933\n",
      "Epoch: 0 \tBatch: 304 \tLoss: 1.7575576305389404\n",
      "Epoch: 0 \tBatch: 305 \tLoss: 1.6705553531646729\n",
      "Epoch: 0 \tBatch: 306 \tLoss: 1.6639769077301025\n",
      "Epoch: 0 \tBatch: 307 \tLoss: 1.659815788269043\n",
      "Epoch: 0 \tBatch: 308 \tLoss: 1.7147462368011475\n",
      "Epoch: 0 \tBatch: 309 \tLoss: 1.7099626064300537\n",
      "Epoch: 0 \tBatch: 310 \tLoss: 1.678246021270752\n",
      "Epoch: 0 \tBatch: 311 \tLoss: 1.673519253730774\n",
      "Epoch: 0 \tBatch: 312 \tLoss: 1.673156976699829\n",
      "Epoch: 0 \tBatch: 313 \tLoss: 1.728628158569336\n",
      "Epoch: 0 \tBatch: 314 \tLoss: 1.651548147201538\n",
      "Epoch: 0 \tBatch: 315 \tLoss: 1.7288860082626343\n",
      "Epoch: 0 \tBatch: 316 \tLoss: 1.7093409299850464\n",
      "Epoch: 0 \tBatch: 317 \tLoss: 1.7411078214645386\n",
      "Epoch: 0 \tBatch: 318 \tLoss: 1.7154819965362549\n",
      "Epoch: 0 \tBatch: 319 \tLoss: 1.756284236907959\n",
      "Epoch: 0 \tBatch: 320 \tLoss: 1.752943992614746\n",
      "Epoch: 0 \tBatch: 321 \tLoss: 1.6785253286361694\n",
      "Epoch: 0 \tBatch: 322 \tLoss: 1.7205649614334106\n",
      "Epoch: 0 \tBatch: 323 \tLoss: 1.7695306539535522\n",
      "Epoch: 0 \tBatch: 324 \tLoss: 1.7390575408935547\n",
      "Epoch: 0 \tBatch: 325 \tLoss: 1.6940667629241943\n",
      "Epoch: 0 \tBatch: 326 \tLoss: 1.6956584453582764\n",
      "Epoch: 0 \tBatch: 327 \tLoss: 1.6823724508285522\n",
      "Epoch: 0 \tBatch: 328 \tLoss: 1.7941089868545532\n",
      "Epoch: 0 \tBatch: 329 \tLoss: 1.7086005210876465\n",
      "Epoch: 0 \tBatch: 330 \tLoss: 1.693036675453186\n",
      "Epoch: 0 \tBatch: 331 \tLoss: 1.7037357091903687\n",
      "Epoch: 0 \tBatch: 332 \tLoss: 1.7254722118377686\n",
      "Epoch: 0 \tBatch: 333 \tLoss: 1.6972178220748901\n",
      "Epoch: 0 \tBatch: 334 \tLoss: 1.7174824476242065\n",
      "Epoch: 0 \tBatch: 335 \tLoss: 1.7341293096542358\n",
      "Epoch: 0 \tBatch: 336 \tLoss: 1.6902670860290527\n",
      "Epoch: 0 \tBatch: 337 \tLoss: 1.6841379404067993\n",
      "Epoch: 0 \tBatch: 338 \tLoss: 1.709952712059021\n",
      "Epoch: 0 \tBatch: 339 \tLoss: 1.657862901687622\n",
      "Epoch: 0 \tBatch: 340 \tLoss: 1.6628801822662354\n",
      "Epoch: 0 \tBatch: 341 \tLoss: 1.6543896198272705\n",
      "Epoch: 0 \tBatch: 342 \tLoss: 1.676935076713562\n",
      "Epoch: 0 \tBatch: 343 \tLoss: 1.6500556468963623\n",
      "Epoch: 0 \tBatch: 344 \tLoss: 1.7158080339431763\n",
      "Epoch: 0 \tBatch: 345 \tLoss: 1.72865629196167\n",
      "Epoch: 0 \tBatch: 346 \tLoss: 1.6970776319503784\n",
      "Epoch: 0 \tBatch: 347 \tLoss: 1.7161141633987427\n",
      "Epoch: 0 \tBatch: 348 \tLoss: 1.738051414489746\n",
      "Epoch: 0 \tBatch: 349 \tLoss: 1.6921055316925049\n",
      "Epoch: 0 \tBatch: 350 \tLoss: 1.6923218965530396\n",
      "Epoch: 0 \tBatch: 351 \tLoss: 1.7364236116409302\n",
      "Epoch: 0 \tBatch: 352 \tLoss: 1.6905165910720825\n",
      "Epoch: 0 \tBatch: 353 \tLoss: 1.676878571510315\n",
      "Epoch: 0 \tBatch: 354 \tLoss: 1.661185383796692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 355 \tLoss: 1.6580848693847656\n",
      "Epoch: 0 \tBatch: 356 \tLoss: 1.6853642463684082\n",
      "Epoch: 0 \tBatch: 357 \tLoss: 1.7218983173370361\n",
      "Epoch: 0 \tBatch: 358 \tLoss: 1.746641993522644\n",
      "Epoch: 0 \tBatch: 359 \tLoss: 1.6861648559570312\n",
      "Epoch: 0 \tBatch: 360 \tLoss: 1.655429720878601\n",
      "Epoch: 0 \tBatch: 361 \tLoss: 1.6721875667572021\n",
      "Epoch: 0 \tBatch: 362 \tLoss: 1.6687839031219482\n",
      "Epoch: 0 \tBatch: 363 \tLoss: 1.6676933765411377\n",
      "Epoch: 0 \tBatch: 364 \tLoss: 1.676353096961975\n",
      "Epoch: 0 \tBatch: 365 \tLoss: 1.6418248414993286\n",
      "Epoch: 0 \tBatch: 366 \tLoss: 1.754818320274353\n",
      "Epoch: 0 \tBatch: 367 \tLoss: 1.7301437854766846\n",
      "Epoch: 0 \tBatch: 368 \tLoss: 1.6386100053787231\n",
      "Epoch: 0 \tBatch: 369 \tLoss: 1.6824809312820435\n",
      "Epoch: 0 \tBatch: 370 \tLoss: 1.6779649257659912\n",
      "Epoch: 0 \tBatch: 371 \tLoss: 1.785969614982605\n",
      "Epoch: 0 \tBatch: 372 \tLoss: 1.6612064838409424\n",
      "Epoch: 0 \tBatch: 373 \tLoss: 1.6388949155807495\n",
      "Epoch: 0 \tBatch: 374 \tLoss: 1.6843469142913818\n",
      "Epoch: 0 \tBatch: 375 \tLoss: 1.6863138675689697\n",
      "Epoch: 0 \tBatch: 376 \tLoss: 1.7429829835891724\n",
      "Epoch: 0 \tBatch: 377 \tLoss: 1.735726237297058\n",
      "Epoch: 0 \tBatch: 378 \tLoss: 1.6520787477493286\n",
      "Epoch: 0 \tBatch: 379 \tLoss: 1.5966933965682983\n",
      "Epoch: 0 \tBatch: 380 \tLoss: 1.6564500331878662\n",
      "Epoch: 0 \tBatch: 381 \tLoss: 1.603326678276062\n",
      "Epoch: 0 \tBatch: 382 \tLoss: 1.655171275138855\n",
      "Epoch: 0 \tBatch: 383 \tLoss: 1.5698286294937134\n",
      "Epoch: 0 \tBatch: 384 \tLoss: 1.6217849254608154\n",
      "Epoch: 0 \tBatch: 385 \tLoss: 1.5812195539474487\n",
      "Epoch: 0 \tBatch: 386 \tLoss: 1.632887840270996\n",
      "Epoch: 0 \tBatch: 387 \tLoss: 1.6426217555999756\n",
      "Epoch: 0 \tBatch: 388 \tLoss: 1.6133357286453247\n",
      "Epoch: 0 \tBatch: 389 \tLoss: 1.604506492614746\n",
      "Epoch: 0 \tBatch: 390 \tLoss: 1.562381625175476\n",
      "Epoch: 1 \tBatch: 0 \tLoss: 1.646553635597229\n",
      "Epoch: 1 \tBatch: 1 \tLoss: 1.6382434368133545\n",
      "Epoch: 1 \tBatch: 2 \tLoss: 1.5794439315795898\n",
      "Epoch: 1 \tBatch: 3 \tLoss: 1.6062030792236328\n",
      "Epoch: 1 \tBatch: 4 \tLoss: 1.6044814586639404\n",
      "Epoch: 1 \tBatch: 5 \tLoss: 1.593578577041626\n",
      "Epoch: 1 \tBatch: 6 \tLoss: 1.6143028736114502\n",
      "Epoch: 1 \tBatch: 7 \tLoss: 1.6319482326507568\n",
      "Epoch: 1 \tBatch: 8 \tLoss: 1.6792378425598145\n",
      "Epoch: 1 \tBatch: 9 \tLoss: 1.5517910718917847\n",
      "Epoch: 1 \tBatch: 10 \tLoss: 1.5789718627929688\n",
      "Epoch: 1 \tBatch: 11 \tLoss: 1.586665391921997\n",
      "Epoch: 1 \tBatch: 12 \tLoss: 1.6474839448928833\n",
      "Epoch: 1 \tBatch: 13 \tLoss: 1.6029164791107178\n",
      "Epoch: 1 \tBatch: 14 \tLoss: 1.6318610906600952\n",
      "Epoch: 1 \tBatch: 15 \tLoss: 1.613838791847229\n",
      "Epoch: 1 \tBatch: 16 \tLoss: 1.626784086227417\n",
      "Epoch: 1 \tBatch: 17 \tLoss: 1.6327705383300781\n",
      "Epoch: 1 \tBatch: 18 \tLoss: 1.6108633279800415\n",
      "Epoch: 1 \tBatch: 19 \tLoss: 1.5982221364974976\n",
      "Epoch: 1 \tBatch: 20 \tLoss: 1.6298563480377197\n",
      "Epoch: 1 \tBatch: 21 \tLoss: 1.5491836071014404\n",
      "Epoch: 1 \tBatch: 22 \tLoss: 1.625596284866333\n",
      "Epoch: 1 \tBatch: 23 \tLoss: 1.6131798028945923\n",
      "Epoch: 1 \tBatch: 24 \tLoss: 1.5844844579696655\n",
      "Epoch: 1 \tBatch: 25 \tLoss: 1.5571415424346924\n",
      "Epoch: 1 \tBatch: 26 \tLoss: 1.6248767375946045\n",
      "Epoch: 1 \tBatch: 27 \tLoss: 1.5379959344863892\n",
      "Epoch: 1 \tBatch: 28 \tLoss: 1.6233851909637451\n",
      "Epoch: 1 \tBatch: 29 \tLoss: 1.5800427198410034\n",
      "Epoch: 1 \tBatch: 30 \tLoss: 1.6227247714996338\n",
      "Epoch: 1 \tBatch: 31 \tLoss: 1.5782274007797241\n",
      "Epoch: 1 \tBatch: 32 \tLoss: 1.5882188081741333\n",
      "Epoch: 1 \tBatch: 33 \tLoss: 1.592301845550537\n",
      "Epoch: 1 \tBatch: 34 \tLoss: 1.5963695049285889\n",
      "Epoch: 1 \tBatch: 35 \tLoss: 1.5958893299102783\n",
      "Epoch: 1 \tBatch: 36 \tLoss: 1.6251463890075684\n",
      "Epoch: 1 \tBatch: 37 \tLoss: 1.5697615146636963\n",
      "Epoch: 1 \tBatch: 38 \tLoss: 1.594617247581482\n",
      "Epoch: 1 \tBatch: 39 \tLoss: 1.597318410873413\n",
      "Epoch: 1 \tBatch: 40 \tLoss: 1.5766806602478027\n",
      "Epoch: 1 \tBatch: 41 \tLoss: 1.5682334899902344\n",
      "Epoch: 1 \tBatch: 42 \tLoss: 1.5730094909667969\n",
      "Epoch: 1 \tBatch: 43 \tLoss: 1.5944520235061646\n",
      "Epoch: 1 \tBatch: 44 \tLoss: 1.6677570343017578\n",
      "Epoch: 1 \tBatch: 45 \tLoss: 1.5639041662216187\n",
      "Epoch: 1 \tBatch: 46 \tLoss: 1.5534347295761108\n",
      "Epoch: 1 \tBatch: 47 \tLoss: 1.5968955755233765\n",
      "Epoch: 1 \tBatch: 48 \tLoss: 1.610395073890686\n",
      "Epoch: 1 \tBatch: 49 \tLoss: 1.5649992227554321\n",
      "Epoch: 1 \tBatch: 50 \tLoss: 1.6200141906738281\n",
      "Epoch: 1 \tBatch: 51 \tLoss: 1.5972814559936523\n",
      "Epoch: 1 \tBatch: 52 \tLoss: 1.6039717197418213\n",
      "Epoch: 1 \tBatch: 53 \tLoss: 1.560603141784668\n",
      "Epoch: 1 \tBatch: 54 \tLoss: 1.571913719177246\n",
      "Epoch: 1 \tBatch: 55 \tLoss: 1.6097959280014038\n",
      "Epoch: 1 \tBatch: 56 \tLoss: 1.6406867504119873\n",
      "Epoch: 1 \tBatch: 57 \tLoss: 1.5805360078811646\n",
      "Epoch: 1 \tBatch: 58 \tLoss: 1.597672939300537\n",
      "Epoch: 1 \tBatch: 59 \tLoss: 1.5867619514465332\n",
      "Epoch: 1 \tBatch: 60 \tLoss: 1.5881321430206299\n",
      "Epoch: 1 \tBatch: 61 \tLoss: 1.6356587409973145\n",
      "Epoch: 1 \tBatch: 62 \tLoss: 1.5701922178268433\n",
      "Epoch: 1 \tBatch: 63 \tLoss: 1.5804847478866577\n",
      "Epoch: 1 \tBatch: 64 \tLoss: 1.613763451576233\n",
      "Epoch: 1 \tBatch: 65 \tLoss: 1.6228426694869995\n",
      "Epoch: 1 \tBatch: 66 \tLoss: 1.6432796716690063\n",
      "Epoch: 1 \tBatch: 67 \tLoss: 1.6273518800735474\n",
      "Epoch: 1 \tBatch: 68 \tLoss: 1.589726448059082\n",
      "Epoch: 1 \tBatch: 69 \tLoss: 1.5855623483657837\n",
      "Epoch: 1 \tBatch: 70 \tLoss: 1.6183233261108398\n",
      "Epoch: 1 \tBatch: 71 \tLoss: 1.5565797090530396\n",
      "Epoch: 1 \tBatch: 72 \tLoss: 1.5431185960769653\n",
      "Epoch: 1 \tBatch: 73 \tLoss: 1.598044991493225\n",
      "Epoch: 1 \tBatch: 74 \tLoss: 1.550736904144287\n",
      "Epoch: 1 \tBatch: 75 \tLoss: 1.5937113761901855\n",
      "Epoch: 1 \tBatch: 76 \tLoss: 1.5901347398757935\n",
      "Epoch: 1 \tBatch: 77 \tLoss: 1.6023123264312744\n",
      "Epoch: 1 \tBatch: 78 \tLoss: 1.6449545621871948\n",
      "Epoch: 1 \tBatch: 79 \tLoss: 1.6073346138000488\n",
      "Epoch: 1 \tBatch: 80 \tLoss: 1.577744483947754\n",
      "Epoch: 1 \tBatch: 81 \tLoss: 1.5758427381515503\n",
      "Epoch: 1 \tBatch: 82 \tLoss: 1.5873918533325195\n",
      "Epoch: 1 \tBatch: 83 \tLoss: 1.559450626373291\n",
      "Epoch: 1 \tBatch: 84 \tLoss: 1.6050416231155396\n",
      "Epoch: 1 \tBatch: 85 \tLoss: 1.604317545890808\n",
      "Epoch: 1 \tBatch: 86 \tLoss: 1.5956411361694336\n",
      "Epoch: 1 \tBatch: 87 \tLoss: 1.5893676280975342\n",
      "Epoch: 1 \tBatch: 88 \tLoss: 1.6197936534881592\n",
      "Epoch: 1 \tBatch: 89 \tLoss: 1.588865041732788\n",
      "Epoch: 1 \tBatch: 90 \tLoss: 1.570103645324707\n",
      "Epoch: 1 \tBatch: 91 \tLoss: 1.5741890668869019\n",
      "Epoch: 1 \tBatch: 92 \tLoss: 1.6323426961898804\n",
      "Epoch: 1 \tBatch: 93 \tLoss: 1.6116783618927002\n",
      "Epoch: 1 \tBatch: 94 \tLoss: 1.6190110445022583\n",
      "Epoch: 1 \tBatch: 95 \tLoss: 1.6096781492233276\n",
      "Epoch: 1 \tBatch: 96 \tLoss: 1.6505886316299438\n",
      "Epoch: 1 \tBatch: 97 \tLoss: 1.596117615699768\n",
      "Epoch: 1 \tBatch: 98 \tLoss: 1.5705379247665405\n",
      "Epoch: 1 \tBatch: 99 \tLoss: 1.6167186498641968\n",
      "Epoch: 1 \tBatch: 100 \tLoss: 1.5794216394424438\n",
      "Epoch: 1 \tBatch: 101 \tLoss: 1.6227716207504272\n",
      "Epoch: 1 \tBatch: 102 \tLoss: 1.5969336032867432\n",
      "Epoch: 1 \tBatch: 103 \tLoss: 1.621666669845581\n",
      "Epoch: 1 \tBatch: 104 \tLoss: 1.5447665452957153\n",
      "Epoch: 1 \tBatch: 105 \tLoss: 1.5891510248184204\n",
      "Epoch: 1 \tBatch: 106 \tLoss: 1.6323240995407104\n",
      "Epoch: 1 \tBatch: 107 \tLoss: 1.5901503562927246\n",
      "Epoch: 1 \tBatch: 108 \tLoss: 1.5942974090576172\n",
      "Epoch: 1 \tBatch: 109 \tLoss: 1.5635006427764893\n",
      "Epoch: 1 \tBatch: 110 \tLoss: 1.5670192241668701\n",
      "Epoch: 1 \tBatch: 111 \tLoss: 1.5362473726272583\n",
      "Epoch: 1 \tBatch: 112 \tLoss: 1.6087489128112793\n",
      "Epoch: 1 \tBatch: 113 \tLoss: 1.607121467590332\n",
      "Epoch: 1 \tBatch: 114 \tLoss: 1.6021132469177246\n",
      "Epoch: 1 \tBatch: 115 \tLoss: 1.57719087600708\n",
      "Epoch: 1 \tBatch: 116 \tLoss: 1.5715217590332031\n",
      "Epoch: 1 \tBatch: 117 \tLoss: 1.5896201133728027\n",
      "Epoch: 1 \tBatch: 118 \tLoss: 1.6184366941452026\n",
      "Epoch: 1 \tBatch: 119 \tLoss: 1.5796974897384644\n",
      "Epoch: 1 \tBatch: 120 \tLoss: 1.599085807800293\n",
      "Epoch: 1 \tBatch: 121 \tLoss: 1.5970637798309326\n",
      "Epoch: 1 \tBatch: 122 \tLoss: 1.6068857908248901\n",
      "Epoch: 1 \tBatch: 123 \tLoss: 1.5846457481384277\n",
      "Epoch: 1 \tBatch: 124 \tLoss: 1.5999798774719238\n",
      "Epoch: 1 \tBatch: 125 \tLoss: 1.591048002243042\n",
      "Epoch: 1 \tBatch: 126 \tLoss: 1.5778329372406006\n",
      "Epoch: 1 \tBatch: 127 \tLoss: 1.5848881006240845\n",
      "Epoch: 1 \tBatch: 128 \tLoss: 1.6016271114349365\n",
      "Epoch: 1 \tBatch: 129 \tLoss: 1.5950210094451904\n",
      "Epoch: 1 \tBatch: 130 \tLoss: 1.591990351676941\n",
      "Epoch: 1 \tBatch: 131 \tLoss: 1.5842723846435547\n",
      "Epoch: 1 \tBatch: 132 \tLoss: 1.6360889673233032\n",
      "Epoch: 1 \tBatch: 133 \tLoss: 1.667156457901001\n",
      "Epoch: 1 \tBatch: 134 \tLoss: 1.5763479471206665\n",
      "Epoch: 1 \tBatch: 135 \tLoss: 1.543485403060913\n",
      "Epoch: 1 \tBatch: 136 \tLoss: 1.5658214092254639\n",
      "Epoch: 1 \tBatch: 137 \tLoss: 1.581501841545105\n",
      "Epoch: 1 \tBatch: 138 \tLoss: 1.6037334203720093\n",
      "Epoch: 1 \tBatch: 139 \tLoss: 1.5952577590942383\n",
      "Epoch: 1 \tBatch: 140 \tLoss: 1.6228621006011963\n",
      "Epoch: 1 \tBatch: 141 \tLoss: 1.588307499885559\n",
      "Epoch: 1 \tBatch: 142 \tLoss: 1.5892179012298584\n",
      "Epoch: 1 \tBatch: 143 \tLoss: 1.6323977708816528\n",
      "Epoch: 1 \tBatch: 144 \tLoss: 1.5953476428985596\n",
      "Epoch: 1 \tBatch: 145 \tLoss: 1.6215791702270508\n",
      "Epoch: 1 \tBatch: 146 \tLoss: 1.6032311916351318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 147 \tLoss: 1.5921069383621216\n",
      "Epoch: 1 \tBatch: 148 \tLoss: 1.6384214162826538\n",
      "Epoch: 1 \tBatch: 149 \tLoss: 1.5411295890808105\n",
      "Epoch: 1 \tBatch: 150 \tLoss: 1.5729243755340576\n",
      "Epoch: 1 \tBatch: 151 \tLoss: 1.5696966648101807\n",
      "Epoch: 1 \tBatch: 152 \tLoss: 1.5668412446975708\n",
      "Epoch: 1 \tBatch: 153 \tLoss: 1.5484623908996582\n",
      "Epoch: 1 \tBatch: 154 \tLoss: 1.5354479551315308\n",
      "Epoch: 1 \tBatch: 155 \tLoss: 1.5312258005142212\n",
      "Epoch: 1 \tBatch: 156 \tLoss: 1.5471304655075073\n",
      "Epoch: 1 \tBatch: 157 \tLoss: 1.5441445112228394\n",
      "Epoch: 1 \tBatch: 158 \tLoss: 1.5058255195617676\n",
      "Epoch: 1 \tBatch: 159 \tLoss: 1.548006296157837\n",
      "Epoch: 1 \tBatch: 160 \tLoss: 1.5212833881378174\n",
      "Epoch: 1 \tBatch: 161 \tLoss: 1.5120383501052856\n",
      "Epoch: 1 \tBatch: 162 \tLoss: 1.5218051671981812\n",
      "Epoch: 1 \tBatch: 163 \tLoss: 1.5378259420394897\n",
      "Epoch: 1 \tBatch: 164 \tLoss: 1.5358142852783203\n",
      "Epoch: 1 \tBatch: 165 \tLoss: 1.528180480003357\n",
      "Epoch: 1 \tBatch: 166 \tLoss: 1.4996373653411865\n",
      "Epoch: 1 \tBatch: 167 \tLoss: 1.5324138402938843\n",
      "Epoch: 1 \tBatch: 168 \tLoss: 1.5012882947921753\n",
      "Epoch: 1 \tBatch: 169 \tLoss: 1.5396859645843506\n",
      "Epoch: 1 \tBatch: 170 \tLoss: 1.5354546308517456\n",
      "Epoch: 1 \tBatch: 171 \tLoss: 1.5030312538146973\n",
      "Epoch: 1 \tBatch: 172 \tLoss: 1.5059767961502075\n",
      "Epoch: 1 \tBatch: 173 \tLoss: 1.5309149026870728\n",
      "Epoch: 1 \tBatch: 174 \tLoss: 1.5315607786178589\n",
      "Epoch: 1 \tBatch: 175 \tLoss: 1.4996310472488403\n",
      "Epoch: 1 \tBatch: 176 \tLoss: 1.4928178787231445\n",
      "Epoch: 1 \tBatch: 177 \tLoss: 1.5092171430587769\n",
      "Epoch: 1 \tBatch: 178 \tLoss: 1.494672417640686\n",
      "Epoch: 1 \tBatch: 179 \tLoss: 1.5402191877365112\n",
      "Epoch: 1 \tBatch: 180 \tLoss: 1.536804437637329\n",
      "Epoch: 1 \tBatch: 181 \tLoss: 1.5077364444732666\n",
      "Epoch: 1 \tBatch: 182 \tLoss: 1.543411135673523\n",
      "Epoch: 1 \tBatch: 183 \tLoss: 1.500327706336975\n",
      "Epoch: 1 \tBatch: 184 \tLoss: 1.5129625797271729\n",
      "Epoch: 1 \tBatch: 185 \tLoss: 1.4739084243774414\n",
      "Epoch: 1 \tBatch: 186 \tLoss: 1.5426045656204224\n",
      "Epoch: 1 \tBatch: 187 \tLoss: 1.497758388519287\n",
      "Epoch: 1 \tBatch: 188 \tLoss: 1.5147217512130737\n",
      "Epoch: 1 \tBatch: 189 \tLoss: 1.5299052000045776\n",
      "Epoch: 1 \tBatch: 190 \tLoss: 1.5220483541488647\n",
      "Epoch: 1 \tBatch: 191 \tLoss: 1.5409350395202637\n",
      "Epoch: 1 \tBatch: 192 \tLoss: 1.5155091285705566\n",
      "Epoch: 1 \tBatch: 193 \tLoss: 1.5274193286895752\n",
      "Epoch: 1 \tBatch: 194 \tLoss: 1.5325084924697876\n",
      "Epoch: 1 \tBatch: 195 \tLoss: 1.5004042387008667\n",
      "Epoch: 1 \tBatch: 196 \tLoss: 1.5365906953811646\n",
      "Epoch: 1 \tBatch: 197 \tLoss: 1.5573338270187378\n",
      "Epoch: 1 \tBatch: 198 \tLoss: 1.494344711303711\n",
      "Epoch: 1 \tBatch: 199 \tLoss: 1.5020625591278076\n",
      "Epoch: 1 \tBatch: 200 \tLoss: 1.5009396076202393\n",
      "Epoch: 1 \tBatch: 201 \tLoss: 1.5102704763412476\n",
      "Epoch: 1 \tBatch: 202 \tLoss: 1.5157747268676758\n",
      "Epoch: 1 \tBatch: 203 \tLoss: 1.5189273357391357\n",
      "Epoch: 1 \tBatch: 204 \tLoss: 1.5035332441329956\n",
      "Epoch: 1 \tBatch: 205 \tLoss: 1.5062161684036255\n",
      "Epoch: 1 \tBatch: 206 \tLoss: 1.5410611629486084\n",
      "Epoch: 1 \tBatch: 207 \tLoss: 1.5151243209838867\n",
      "Epoch: 1 \tBatch: 208 \tLoss: 1.5051590204238892\n",
      "Epoch: 1 \tBatch: 209 \tLoss: 1.537057876586914\n",
      "Epoch: 1 \tBatch: 210 \tLoss: 1.4842164516448975\n",
      "Epoch: 1 \tBatch: 211 \tLoss: 1.5191709995269775\n",
      "Epoch: 1 \tBatch: 212 \tLoss: 1.53481924533844\n",
      "Epoch: 1 \tBatch: 213 \tLoss: 1.5230445861816406\n",
      "Epoch: 1 \tBatch: 214 \tLoss: 1.5285557508468628\n",
      "Epoch: 1 \tBatch: 215 \tLoss: 1.5303306579589844\n",
      "Epoch: 1 \tBatch: 216 \tLoss: 1.5012174844741821\n",
      "Epoch: 1 \tBatch: 217 \tLoss: 1.5146650075912476\n",
      "Epoch: 1 \tBatch: 218 \tLoss: 1.5264623165130615\n",
      "Epoch: 1 \tBatch: 219 \tLoss: 1.5000920295715332\n",
      "Epoch: 1 \tBatch: 220 \tLoss: 1.5227620601654053\n",
      "Epoch: 1 \tBatch: 221 \tLoss: 1.5063754320144653\n",
      "Epoch: 1 \tBatch: 222 \tLoss: 1.4895626306533813\n",
      "Epoch: 1 \tBatch: 223 \tLoss: 1.526069164276123\n",
      "Epoch: 1 \tBatch: 224 \tLoss: 1.5297532081604004\n",
      "Epoch: 1 \tBatch: 225 \tLoss: 1.5005186796188354\n",
      "Epoch: 1 \tBatch: 226 \tLoss: 1.5486685037612915\n",
      "Epoch: 1 \tBatch: 227 \tLoss: 1.523759365081787\n",
      "Epoch: 1 \tBatch: 228 \tLoss: 1.4962948560714722\n",
      "Epoch: 1 \tBatch: 229 \tLoss: 1.5155417919158936\n",
      "Epoch: 1 \tBatch: 230 \tLoss: 1.4916611909866333\n",
      "Epoch: 1 \tBatch: 231 \tLoss: 1.4859466552734375\n",
      "Epoch: 1 \tBatch: 232 \tLoss: 1.5078686475753784\n",
      "Epoch: 1 \tBatch: 233 \tLoss: 1.5009703636169434\n",
      "Epoch: 1 \tBatch: 234 \tLoss: 1.4917320013046265\n",
      "Epoch: 1 \tBatch: 235 \tLoss: 1.5397601127624512\n",
      "Epoch: 1 \tBatch: 236 \tLoss: 1.491200566291809\n",
      "Epoch: 1 \tBatch: 237 \tLoss: 1.5111557245254517\n",
      "Epoch: 1 \tBatch: 238 \tLoss: 1.5130926370620728\n",
      "Epoch: 1 \tBatch: 239 \tLoss: 1.509460687637329\n",
      "Epoch: 1 \tBatch: 240 \tLoss: 1.4821488857269287\n",
      "Epoch: 1 \tBatch: 241 \tLoss: 1.4885790348052979\n",
      "Epoch: 1 \tBatch: 242 \tLoss: 1.476698875427246\n",
      "Epoch: 1 \tBatch: 243 \tLoss: 1.5115808248519897\n",
      "Epoch: 1 \tBatch: 244 \tLoss: 1.507340431213379\n",
      "Epoch: 1 \tBatch: 245 \tLoss: 1.5031535625457764\n",
      "Epoch: 1 \tBatch: 246 \tLoss: 1.4654760360717773\n",
      "Epoch: 1 \tBatch: 247 \tLoss: 1.499224305152893\n",
      "Epoch: 1 \tBatch: 248 \tLoss: 1.5101344585418701\n",
      "Epoch: 1 \tBatch: 249 \tLoss: 1.509522557258606\n",
      "Epoch: 1 \tBatch: 250 \tLoss: 1.4989410638809204\n",
      "Epoch: 1 \tBatch: 251 \tLoss: 1.5066680908203125\n",
      "Epoch: 1 \tBatch: 252 \tLoss: 1.502590537071228\n",
      "Epoch: 1 \tBatch: 253 \tLoss: 1.5043309926986694\n",
      "Epoch: 1 \tBatch: 254 \tLoss: 1.5052746534347534\n",
      "Epoch: 1 \tBatch: 255 \tLoss: 1.4995009899139404\n",
      "Epoch: 1 \tBatch: 256 \tLoss: 1.5013363361358643\n",
      "Epoch: 1 \tBatch: 257 \tLoss: 1.5147351026535034\n",
      "Epoch: 1 \tBatch: 258 \tLoss: 1.5001368522644043\n",
      "Epoch: 1 \tBatch: 259 \tLoss: 1.483046054840088\n",
      "Epoch: 1 \tBatch: 260 \tLoss: 1.5226762294769287\n",
      "Epoch: 1 \tBatch: 261 \tLoss: 1.5047396421432495\n",
      "Epoch: 1 \tBatch: 262 \tLoss: 1.5059629678726196\n",
      "Epoch: 1 \tBatch: 263 \tLoss: 1.4989944696426392\n",
      "Epoch: 1 \tBatch: 264 \tLoss: 1.485747218132019\n",
      "Epoch: 1 \tBatch: 265 \tLoss: 1.5088943243026733\n",
      "Epoch: 1 \tBatch: 266 \tLoss: 1.5042624473571777\n",
      "Epoch: 1 \tBatch: 267 \tLoss: 1.5309141874313354\n",
      "Epoch: 1 \tBatch: 268 \tLoss: 1.5000211000442505\n",
      "Epoch: 1 \tBatch: 269 \tLoss: 1.4831680059432983\n",
      "Epoch: 1 \tBatch: 270 \tLoss: 1.4830150604248047\n",
      "Epoch: 1 \tBatch: 271 \tLoss: 1.4850482940673828\n",
      "Epoch: 1 \tBatch: 272 \tLoss: 1.4955644607543945\n",
      "Epoch: 1 \tBatch: 273 \tLoss: 1.5149563550949097\n",
      "Epoch: 1 \tBatch: 274 \tLoss: 1.4814943075180054\n",
      "Epoch: 1 \tBatch: 275 \tLoss: 1.4898210763931274\n",
      "Epoch: 1 \tBatch: 276 \tLoss: 1.4924721717834473\n",
      "Epoch: 1 \tBatch: 277 \tLoss: 1.5064187049865723\n",
      "Epoch: 1 \tBatch: 278 \tLoss: 1.5051511526107788\n",
      "Epoch: 1 \tBatch: 279 \tLoss: 1.5420970916748047\n",
      "Epoch: 1 \tBatch: 280 \tLoss: 1.5042818784713745\n",
      "Epoch: 1 \tBatch: 281 \tLoss: 1.4858951568603516\n",
      "Epoch: 1 \tBatch: 282 \tLoss: 1.4867967367172241\n",
      "Epoch: 1 \tBatch: 283 \tLoss: 1.488587737083435\n",
      "Epoch: 1 \tBatch: 284 \tLoss: 1.485575556755066\n",
      "Epoch: 1 \tBatch: 285 \tLoss: 1.5104342699050903\n",
      "Epoch: 1 \tBatch: 286 \tLoss: 1.4880874156951904\n",
      "Epoch: 1 \tBatch: 287 \tLoss: 1.4998440742492676\n",
      "Epoch: 1 \tBatch: 288 \tLoss: 1.514844298362732\n",
      "Epoch: 1 \tBatch: 289 \tLoss: 1.5058530569076538\n",
      "Epoch: 1 \tBatch: 290 \tLoss: 1.5210413932800293\n",
      "Epoch: 1 \tBatch: 291 \tLoss: 1.524395227432251\n",
      "Epoch: 1 \tBatch: 292 \tLoss: 1.4980254173278809\n",
      "Epoch: 1 \tBatch: 293 \tLoss: 1.504338264465332\n",
      "Epoch: 1 \tBatch: 294 \tLoss: 1.479915738105774\n",
      "Epoch: 1 \tBatch: 295 \tLoss: 1.5037641525268555\n",
      "Epoch: 1 \tBatch: 296 \tLoss: 1.4997670650482178\n",
      "Epoch: 1 \tBatch: 297 \tLoss: 1.496419072151184\n",
      "Epoch: 1 \tBatch: 298 \tLoss: 1.5027120113372803\n",
      "Epoch: 1 \tBatch: 299 \tLoss: 1.4882217645645142\n",
      "Epoch: 1 \tBatch: 300 \tLoss: 1.492633581161499\n",
      "Epoch: 1 \tBatch: 301 \tLoss: 1.493269443511963\n",
      "Epoch: 1 \tBatch: 302 \tLoss: 1.4960449934005737\n",
      "Epoch: 1 \tBatch: 303 \tLoss: 1.4835312366485596\n",
      "Epoch: 1 \tBatch: 304 \tLoss: 1.4825738668441772\n",
      "Epoch: 1 \tBatch: 305 \tLoss: 1.491479754447937\n",
      "Epoch: 1 \tBatch: 306 \tLoss: 1.5124295949935913\n",
      "Epoch: 1 \tBatch: 307 \tLoss: 1.4951834678649902\n",
      "Epoch: 1 \tBatch: 308 \tLoss: 1.499822735786438\n",
      "Epoch: 1 \tBatch: 309 \tLoss: 1.539613962173462\n",
      "Epoch: 1 \tBatch: 310 \tLoss: 1.5335463285446167\n",
      "Epoch: 1 \tBatch: 311 \tLoss: 1.5065356492996216\n",
      "Epoch: 1 \tBatch: 312 \tLoss: 1.4746776819229126\n",
      "Epoch: 1 \tBatch: 313 \tLoss: 1.5160415172576904\n",
      "Epoch: 1 \tBatch: 314 \tLoss: 1.4810502529144287\n",
      "Epoch: 1 \tBatch: 315 \tLoss: 1.5027943849563599\n",
      "Epoch: 1 \tBatch: 316 \tLoss: 1.4846508502960205\n",
      "Epoch: 1 \tBatch: 317 \tLoss: 1.5004699230194092\n",
      "Epoch: 1 \tBatch: 318 \tLoss: 1.4947404861450195\n",
      "Epoch: 1 \tBatch: 319 \tLoss: 1.5316787958145142\n",
      "Epoch: 1 \tBatch: 320 \tLoss: 1.4887961149215698\n",
      "Epoch: 1 \tBatch: 321 \tLoss: 1.4879395961761475\n",
      "Epoch: 1 \tBatch: 322 \tLoss: 1.4817042350769043\n",
      "Epoch: 1 \tBatch: 323 \tLoss: 1.5077433586120605\n",
      "Epoch: 1 \tBatch: 324 \tLoss: 1.4935758113861084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 325 \tLoss: 1.486905813217163\n",
      "Epoch: 1 \tBatch: 326 \tLoss: 1.5237061977386475\n",
      "Epoch: 1 \tBatch: 327 \tLoss: 1.4881561994552612\n",
      "Epoch: 1 \tBatch: 328 \tLoss: 1.4829708337783813\n",
      "Epoch: 1 \tBatch: 329 \tLoss: 1.4827646017074585\n",
      "Epoch: 1 \tBatch: 330 \tLoss: 1.472924828529358\n",
      "Epoch: 1 \tBatch: 331 \tLoss: 1.5005457401275635\n",
      "Epoch: 1 \tBatch: 332 \tLoss: 1.5250062942504883\n",
      "Epoch: 1 \tBatch: 333 \tLoss: 1.4782994985580444\n",
      "Epoch: 1 \tBatch: 334 \tLoss: 1.4849872589111328\n",
      "Epoch: 1 \tBatch: 335 \tLoss: 1.514311671257019\n",
      "Epoch: 1 \tBatch: 336 \tLoss: 1.4712289571762085\n",
      "Epoch: 1 \tBatch: 337 \tLoss: 1.4997824430465698\n",
      "Epoch: 1 \tBatch: 338 \tLoss: 1.4848170280456543\n",
      "Epoch: 1 \tBatch: 339 \tLoss: 1.49793541431427\n",
      "Epoch: 1 \tBatch: 340 \tLoss: 1.4945625066757202\n",
      "Epoch: 1 \tBatch: 341 \tLoss: 1.4999622106552124\n",
      "Epoch: 1 \tBatch: 342 \tLoss: 1.5155525207519531\n",
      "Epoch: 1 \tBatch: 343 \tLoss: 1.5075474977493286\n",
      "Epoch: 1 \tBatch: 344 \tLoss: 1.4889510869979858\n",
      "Epoch: 1 \tBatch: 345 \tLoss: 1.4989540576934814\n",
      "Epoch: 1 \tBatch: 346 \tLoss: 1.4903713464736938\n",
      "Epoch: 1 \tBatch: 347 \tLoss: 1.4839776754379272\n",
      "Epoch: 1 \tBatch: 348 \tLoss: 1.4938077926635742\n",
      "Epoch: 1 \tBatch: 349 \tLoss: 1.5039575099945068\n",
      "Epoch: 1 \tBatch: 350 \tLoss: 1.4716217517852783\n",
      "Epoch: 1 \tBatch: 351 \tLoss: 1.4790107011795044\n",
      "Epoch: 1 \tBatch: 352 \tLoss: 1.5016084909439087\n",
      "Epoch: 1 \tBatch: 353 \tLoss: 1.5013034343719482\n",
      "Epoch: 1 \tBatch: 354 \tLoss: 1.4957127571105957\n",
      "Epoch: 1 \tBatch: 355 \tLoss: 1.4970554113388062\n",
      "Epoch: 1 \tBatch: 356 \tLoss: 1.5083867311477661\n",
      "Epoch: 1 \tBatch: 357 \tLoss: 1.4995343685150146\n",
      "Epoch: 1 \tBatch: 358 \tLoss: 1.5004149675369263\n",
      "Epoch: 1 \tBatch: 359 \tLoss: 1.486291766166687\n",
      "Epoch: 1 \tBatch: 360 \tLoss: 1.5469470024108887\n",
      "Epoch: 1 \tBatch: 361 \tLoss: 1.5117448568344116\n",
      "Epoch: 1 \tBatch: 362 \tLoss: 1.4974957704544067\n",
      "Epoch: 1 \tBatch: 363 \tLoss: 1.4949476718902588\n",
      "Epoch: 1 \tBatch: 364 \tLoss: 1.4862627983093262\n",
      "Epoch: 1 \tBatch: 365 \tLoss: 1.5055557489395142\n",
      "Epoch: 1 \tBatch: 366 \tLoss: 1.5096548795700073\n",
      "Epoch: 1 \tBatch: 367 \tLoss: 1.524223804473877\n",
      "Epoch: 1 \tBatch: 368 \tLoss: 1.4935623407363892\n",
      "Epoch: 1 \tBatch: 369 \tLoss: 1.4944449663162231\n",
      "Epoch: 1 \tBatch: 370 \tLoss: 1.4834591150283813\n",
      "Epoch: 1 \tBatch: 371 \tLoss: 1.50629723072052\n",
      "Epoch: 1 \tBatch: 372 \tLoss: 1.4799911975860596\n",
      "Epoch: 1 \tBatch: 373 \tLoss: 1.5019930601119995\n",
      "Epoch: 1 \tBatch: 374 \tLoss: 1.493535041809082\n",
      "Epoch: 1 \tBatch: 375 \tLoss: 1.4856256246566772\n",
      "Epoch: 1 \tBatch: 376 \tLoss: 1.4859868288040161\n",
      "Epoch: 1 \tBatch: 377 \tLoss: 1.5101509094238281\n",
      "Epoch: 1 \tBatch: 378 \tLoss: 1.465687870979309\n",
      "Epoch: 1 \tBatch: 379 \tLoss: 1.4835339784622192\n",
      "Epoch: 1 \tBatch: 380 \tLoss: 1.4821116924285889\n",
      "Epoch: 1 \tBatch: 381 \tLoss: 1.494959831237793\n",
      "Epoch: 1 \tBatch: 382 \tLoss: 1.4961475133895874\n",
      "Epoch: 1 \tBatch: 383 \tLoss: 1.5167720317840576\n",
      "Epoch: 1 \tBatch: 384 \tLoss: 1.5211501121520996\n",
      "Epoch: 1 \tBatch: 385 \tLoss: 1.5069084167480469\n",
      "Epoch: 1 \tBatch: 386 \tLoss: 1.4804425239562988\n",
      "Epoch: 1 \tBatch: 387 \tLoss: 1.5119118690490723\n",
      "Epoch: 1 \tBatch: 388 \tLoss: 1.513170838356018\n",
      "Epoch: 1 \tBatch: 389 \tLoss: 1.507139801979065\n",
      "Epoch: 1 \tBatch: 390 \tLoss: 1.554651141166687\n",
      "Epoch: 2 \tBatch: 0 \tLoss: 1.4819153547286987\n",
      "Epoch: 2 \tBatch: 1 \tLoss: 1.4888259172439575\n",
      "Epoch: 2 \tBatch: 2 \tLoss: 1.4964224100112915\n",
      "Epoch: 2 \tBatch: 3 \tLoss: 1.4647222757339478\n",
      "Epoch: 2 \tBatch: 4 \tLoss: 1.4727272987365723\n",
      "Epoch: 2 \tBatch: 5 \tLoss: 1.5049041509628296\n",
      "Epoch: 2 \tBatch: 6 \tLoss: 1.492325782775879\n",
      "Epoch: 2 \tBatch: 7 \tLoss: 1.4970660209655762\n",
      "Epoch: 2 \tBatch: 8 \tLoss: 1.4912223815917969\n",
      "Epoch: 2 \tBatch: 9 \tLoss: 1.4816149473190308\n",
      "Epoch: 2 \tBatch: 10 \tLoss: 1.508238434791565\n",
      "Epoch: 2 \tBatch: 11 \tLoss: 1.4830365180969238\n",
      "Epoch: 2 \tBatch: 12 \tLoss: 1.5024266242980957\n",
      "Epoch: 2 \tBatch: 13 \tLoss: 1.497078776359558\n",
      "Epoch: 2 \tBatch: 14 \tLoss: 1.5076719522476196\n",
      "Epoch: 2 \tBatch: 15 \tLoss: 1.509006381034851\n",
      "Epoch: 2 \tBatch: 16 \tLoss: 1.5068308115005493\n",
      "Epoch: 2 \tBatch: 17 \tLoss: 1.4900431632995605\n",
      "Epoch: 2 \tBatch: 18 \tLoss: 1.4808522462844849\n",
      "Epoch: 2 \tBatch: 19 \tLoss: 1.5229405164718628\n",
      "Epoch: 2 \tBatch: 20 \tLoss: 1.489158272743225\n",
      "Epoch: 2 \tBatch: 21 \tLoss: 1.4823036193847656\n",
      "Epoch: 2 \tBatch: 22 \tLoss: 1.497740626335144\n",
      "Epoch: 2 \tBatch: 23 \tLoss: 1.4947630167007446\n",
      "Epoch: 2 \tBatch: 24 \tLoss: 1.5035021305084229\n",
      "Epoch: 2 \tBatch: 25 \tLoss: 1.490078330039978\n",
      "Epoch: 2 \tBatch: 26 \tLoss: 1.492620587348938\n",
      "Epoch: 2 \tBatch: 27 \tLoss: 1.4798555374145508\n",
      "Epoch: 2 \tBatch: 28 \tLoss: 1.526188850402832\n",
      "Epoch: 2 \tBatch: 29 \tLoss: 1.5031532049179077\n",
      "Epoch: 2 \tBatch: 30 \tLoss: 1.4856085777282715\n",
      "Epoch: 2 \tBatch: 31 \tLoss: 1.4910738468170166\n",
      "Epoch: 2 \tBatch: 32 \tLoss: 1.4998711347579956\n",
      "Epoch: 2 \tBatch: 33 \tLoss: 1.4934847354888916\n",
      "Epoch: 2 \tBatch: 34 \tLoss: 1.4675511121749878\n",
      "Epoch: 2 \tBatch: 35 \tLoss: 1.4992401599884033\n",
      "Epoch: 2 \tBatch: 36 \tLoss: 1.4807544946670532\n",
      "Epoch: 2 \tBatch: 37 \tLoss: 1.4947198629379272\n",
      "Epoch: 2 \tBatch: 38 \tLoss: 1.4997069835662842\n",
      "Epoch: 2 \tBatch: 39 \tLoss: 1.4966816902160645\n",
      "Epoch: 2 \tBatch: 40 \tLoss: 1.5024664402008057\n",
      "Epoch: 2 \tBatch: 41 \tLoss: 1.4746184349060059\n",
      "Epoch: 2 \tBatch: 42 \tLoss: 1.4911693334579468\n",
      "Epoch: 2 \tBatch: 43 \tLoss: 1.5327246189117432\n",
      "Epoch: 2 \tBatch: 44 \tLoss: 1.4951024055480957\n",
      "Epoch: 2 \tBatch: 45 \tLoss: 1.501564860343933\n",
      "Epoch: 2 \tBatch: 46 \tLoss: 1.4724243879318237\n",
      "Epoch: 2 \tBatch: 47 \tLoss: 1.4991720914840698\n",
      "Epoch: 2 \tBatch: 48 \tLoss: 1.507400631904602\n",
      "Epoch: 2 \tBatch: 49 \tLoss: 1.4940718412399292\n",
      "Epoch: 2 \tBatch: 50 \tLoss: 1.500433087348938\n",
      "Epoch: 2 \tBatch: 51 \tLoss: 1.4796279668807983\n",
      "Epoch: 2 \tBatch: 52 \tLoss: 1.486295223236084\n",
      "Epoch: 2 \tBatch: 53 \tLoss: 1.4859867095947266\n",
      "Epoch: 2 \tBatch: 54 \tLoss: 1.4922524690628052\n",
      "Epoch: 2 \tBatch: 55 \tLoss: 1.490761637687683\n",
      "Epoch: 2 \tBatch: 56 \tLoss: 1.4972504377365112\n",
      "Epoch: 2 \tBatch: 57 \tLoss: 1.4781577587127686\n",
      "Epoch: 2 \tBatch: 58 \tLoss: 1.4791793823242188\n",
      "Epoch: 2 \tBatch: 59 \tLoss: 1.5286930799484253\n",
      "Epoch: 2 \tBatch: 60 \tLoss: 1.481210470199585\n",
      "Epoch: 2 \tBatch: 61 \tLoss: 1.513284683227539\n",
      "Epoch: 2 \tBatch: 62 \tLoss: 1.49141263961792\n",
      "Epoch: 2 \tBatch: 63 \tLoss: 1.4842344522476196\n",
      "Epoch: 2 \tBatch: 64 \tLoss: 1.5084338188171387\n",
      "Epoch: 2 \tBatch: 65 \tLoss: 1.491545557975769\n",
      "Epoch: 2 \tBatch: 66 \tLoss: 1.490380048751831\n",
      "Epoch: 2 \tBatch: 67 \tLoss: 1.4708350896835327\n",
      "Epoch: 2 \tBatch: 68 \tLoss: 1.504733920097351\n",
      "Epoch: 2 \tBatch: 69 \tLoss: 1.480591058731079\n",
      "Epoch: 2 \tBatch: 70 \tLoss: 1.4835047721862793\n",
      "Epoch: 2 \tBatch: 71 \tLoss: 1.5119543075561523\n",
      "Epoch: 2 \tBatch: 72 \tLoss: 1.4818252325057983\n",
      "Epoch: 2 \tBatch: 73 \tLoss: 1.4764885902404785\n",
      "Epoch: 2 \tBatch: 74 \tLoss: 1.4829849004745483\n",
      "Epoch: 2 \tBatch: 75 \tLoss: 1.4761005640029907\n",
      "Epoch: 2 \tBatch: 76 \tLoss: 1.5192152261734009\n",
      "Epoch: 2 \tBatch: 77 \tLoss: 1.4945124387741089\n",
      "Epoch: 2 \tBatch: 78 \tLoss: 1.5016560554504395\n",
      "Epoch: 2 \tBatch: 79 \tLoss: 1.4865851402282715\n",
      "Epoch: 2 \tBatch: 80 \tLoss: 1.4816577434539795\n",
      "Epoch: 2 \tBatch: 81 \tLoss: 1.4820919036865234\n",
      "Epoch: 2 \tBatch: 82 \tLoss: 1.4902726411819458\n",
      "Epoch: 2 \tBatch: 83 \tLoss: 1.486849308013916\n",
      "Epoch: 2 \tBatch: 84 \tLoss: 1.4909090995788574\n",
      "Epoch: 2 \tBatch: 85 \tLoss: 1.4796124696731567\n",
      "Epoch: 2 \tBatch: 86 \tLoss: 1.4817663431167603\n",
      "Epoch: 2 \tBatch: 87 \tLoss: 1.4905935525894165\n",
      "Epoch: 2 \tBatch: 88 \tLoss: 1.516672968864441\n",
      "Epoch: 2 \tBatch: 89 \tLoss: 1.4726909399032593\n",
      "Epoch: 2 \tBatch: 90 \tLoss: 1.5035535097122192\n",
      "Epoch: 2 \tBatch: 91 \tLoss: 1.4993635416030884\n",
      "Epoch: 2 \tBatch: 92 \tLoss: 1.5001988410949707\n",
      "Epoch: 2 \tBatch: 93 \tLoss: 1.481241226196289\n",
      "Epoch: 2 \tBatch: 94 \tLoss: 1.4836580753326416\n",
      "Epoch: 2 \tBatch: 95 \tLoss: 1.4767894744873047\n",
      "Epoch: 2 \tBatch: 96 \tLoss: 1.4817348718643188\n",
      "Epoch: 2 \tBatch: 97 \tLoss: 1.4696952104568481\n",
      "Epoch: 2 \tBatch: 98 \tLoss: 1.4866524934768677\n",
      "Epoch: 2 \tBatch: 99 \tLoss: 1.4791995286941528\n",
      "Epoch: 2 \tBatch: 100 \tLoss: 1.4863406419754028\n",
      "Epoch: 2 \tBatch: 101 \tLoss: 1.4862382411956787\n",
      "Epoch: 2 \tBatch: 102 \tLoss: 1.5043100118637085\n",
      "Epoch: 2 \tBatch: 103 \tLoss: 1.4656850099563599\n",
      "Epoch: 2 \tBatch: 104 \tLoss: 1.4919078350067139\n",
      "Epoch: 2 \tBatch: 105 \tLoss: 1.4792052507400513\n",
      "Epoch: 2 \tBatch: 106 \tLoss: 1.4910356998443604\n",
      "Epoch: 2 \tBatch: 107 \tLoss: 1.4716180562973022\n",
      "Epoch: 2 \tBatch: 108 \tLoss: 1.4925040006637573\n",
      "Epoch: 2 \tBatch: 109 \tLoss: 1.489706039428711\n",
      "Epoch: 2 \tBatch: 110 \tLoss: 1.4894142150878906\n",
      "Epoch: 2 \tBatch: 111 \tLoss: 1.4912230968475342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tBatch: 112 \tLoss: 1.5184929370880127\n",
      "Epoch: 2 \tBatch: 113 \tLoss: 1.4920798540115356\n",
      "Epoch: 2 \tBatch: 114 \tLoss: 1.482596516609192\n",
      "Epoch: 2 \tBatch: 115 \tLoss: 1.4907327890396118\n",
      "Epoch: 2 \tBatch: 116 \tLoss: 1.4942848682403564\n",
      "Epoch: 2 \tBatch: 117 \tLoss: 1.477311134338379\n",
      "Epoch: 2 \tBatch: 118 \tLoss: 1.4948338270187378\n",
      "Epoch: 2 \tBatch: 119 \tLoss: 1.4683622121810913\n",
      "Epoch: 2 \tBatch: 120 \tLoss: 1.4734385013580322\n",
      "Epoch: 2 \tBatch: 121 \tLoss: 1.4657237529754639\n",
      "Epoch: 2 \tBatch: 122 \tLoss: 1.4908056259155273\n",
      "Epoch: 2 \tBatch: 123 \tLoss: 1.4826637506484985\n",
      "Epoch: 2 \tBatch: 124 \tLoss: 1.4829357862472534\n",
      "Epoch: 2 \tBatch: 125 \tLoss: 1.4989891052246094\n",
      "Epoch: 2 \tBatch: 126 \tLoss: 1.4841424226760864\n",
      "Epoch: 2 \tBatch: 127 \tLoss: 1.4922566413879395\n",
      "Epoch: 2 \tBatch: 128 \tLoss: 1.4964268207550049\n",
      "Epoch: 2 \tBatch: 129 \tLoss: 1.4885882139205933\n",
      "Epoch: 2 \tBatch: 130 \tLoss: 1.4666271209716797\n",
      "Epoch: 2 \tBatch: 131 \tLoss: 1.4984170198440552\n",
      "Epoch: 2 \tBatch: 132 \tLoss: 1.5025861263275146\n",
      "Epoch: 2 \tBatch: 133 \tLoss: 1.4718278646469116\n",
      "Epoch: 2 \tBatch: 134 \tLoss: 1.4822381734848022\n",
      "Epoch: 2 \tBatch: 135 \tLoss: 1.4756157398223877\n",
      "Epoch: 2 \tBatch: 136 \tLoss: 1.4896093606948853\n",
      "Epoch: 2 \tBatch: 137 \tLoss: 1.4877809286117554\n",
      "Epoch: 2 \tBatch: 138 \tLoss: 1.5054911375045776\n",
      "Epoch: 2 \tBatch: 139 \tLoss: 1.5056461095809937\n",
      "Epoch: 2 \tBatch: 140 \tLoss: 1.4846066236495972\n",
      "Epoch: 2 \tBatch: 141 \tLoss: 1.4857712984085083\n",
      "Epoch: 2 \tBatch: 142 \tLoss: 1.4895554780960083\n",
      "Epoch: 2 \tBatch: 143 \tLoss: 1.4995778799057007\n",
      "Epoch: 2 \tBatch: 144 \tLoss: 1.497092604637146\n",
      "Epoch: 2 \tBatch: 145 \tLoss: 1.4952112436294556\n",
      "Epoch: 2 \tBatch: 146 \tLoss: 1.4779093265533447\n",
      "Epoch: 2 \tBatch: 147 \tLoss: 1.4830338954925537\n",
      "Epoch: 2 \tBatch: 148 \tLoss: 1.4796253442764282\n",
      "Epoch: 2 \tBatch: 149 \tLoss: 1.4789320230484009\n",
      "Epoch: 2 \tBatch: 150 \tLoss: 1.5193201303482056\n",
      "Epoch: 2 \tBatch: 151 \tLoss: 1.50917387008667\n",
      "Epoch: 2 \tBatch: 152 \tLoss: 1.4856818914413452\n",
      "Epoch: 2 \tBatch: 153 \tLoss: 1.5100069046020508\n",
      "Epoch: 2 \tBatch: 154 \tLoss: 1.4908432960510254\n",
      "Epoch: 2 \tBatch: 155 \tLoss: 1.4881486892700195\n",
      "Epoch: 2 \tBatch: 156 \tLoss: 1.5091822147369385\n",
      "Epoch: 2 \tBatch: 157 \tLoss: 1.5008745193481445\n",
      "Epoch: 2 \tBatch: 158 \tLoss: 1.4720221757888794\n",
      "Epoch: 2 \tBatch: 159 \tLoss: 1.4859768152236938\n",
      "Epoch: 2 \tBatch: 160 \tLoss: 1.4901193380355835\n",
      "Epoch: 2 \tBatch: 161 \tLoss: 1.5088603496551514\n",
      "Epoch: 2 \tBatch: 162 \tLoss: 1.484148383140564\n",
      "Epoch: 2 \tBatch: 163 \tLoss: 1.4777629375457764\n",
      "Epoch: 2 \tBatch: 164 \tLoss: 1.4809691905975342\n",
      "Epoch: 2 \tBatch: 165 \tLoss: 1.479901909828186\n",
      "Epoch: 2 \tBatch: 166 \tLoss: 1.4830291271209717\n",
      "Epoch: 2 \tBatch: 167 \tLoss: 1.5125588178634644\n",
      "Epoch: 2 \tBatch: 168 \tLoss: 1.48093581199646\n",
      "Epoch: 2 \tBatch: 169 \tLoss: 1.4869451522827148\n",
      "Epoch: 2 \tBatch: 170 \tLoss: 1.4865505695343018\n",
      "Epoch: 2 \tBatch: 171 \tLoss: 1.494503378868103\n",
      "Epoch: 2 \tBatch: 172 \tLoss: 1.48663330078125\n",
      "Epoch: 2 \tBatch: 173 \tLoss: 1.4893566370010376\n",
      "Epoch: 2 \tBatch: 174 \tLoss: 1.4796860218048096\n",
      "Epoch: 2 \tBatch: 175 \tLoss: 1.4909926652908325\n",
      "Epoch: 2 \tBatch: 176 \tLoss: 1.4760701656341553\n",
      "Epoch: 2 \tBatch: 177 \tLoss: 1.4815659523010254\n",
      "Epoch: 2 \tBatch: 178 \tLoss: 1.4705617427825928\n",
      "Epoch: 2 \tBatch: 179 \tLoss: 1.4971634149551392\n",
      "Epoch: 2 \tBatch: 180 \tLoss: 1.4888839721679688\n",
      "Epoch: 2 \tBatch: 181 \tLoss: 1.4812504053115845\n",
      "Epoch: 2 \tBatch: 182 \tLoss: 1.487206220626831\n",
      "Epoch: 2 \tBatch: 183 \tLoss: 1.4802939891815186\n",
      "Epoch: 2 \tBatch: 184 \tLoss: 1.4980847835540771\n",
      "Epoch: 2 \tBatch: 185 \tLoss: 1.4992496967315674\n",
      "Epoch: 2 \tBatch: 186 \tLoss: 1.485276460647583\n",
      "Epoch: 2 \tBatch: 187 \tLoss: 1.4994661808013916\n",
      "Epoch: 2 \tBatch: 188 \tLoss: 1.483121395111084\n",
      "Epoch: 2 \tBatch: 189 \tLoss: 1.4855260848999023\n",
      "Epoch: 2 \tBatch: 190 \tLoss: 1.4837186336517334\n",
      "Epoch: 2 \tBatch: 191 \tLoss: 1.4869476556777954\n",
      "Epoch: 2 \tBatch: 192 \tLoss: 1.4982128143310547\n",
      "Epoch: 2 \tBatch: 193 \tLoss: 1.4826096296310425\n",
      "Epoch: 2 \tBatch: 194 \tLoss: 1.474273681640625\n",
      "Epoch: 2 \tBatch: 195 \tLoss: 1.497684359550476\n",
      "Epoch: 2 \tBatch: 196 \tLoss: 1.5190880298614502\n",
      "Epoch: 2 \tBatch: 197 \tLoss: 1.483001947402954\n",
      "Epoch: 2 \tBatch: 198 \tLoss: 1.5025173425674438\n",
      "Epoch: 2 \tBatch: 199 \tLoss: 1.51180100440979\n",
      "Epoch: 2 \tBatch: 200 \tLoss: 1.4942232370376587\n",
      "Epoch: 2 \tBatch: 201 \tLoss: 1.4891587495803833\n",
      "Epoch: 2 \tBatch: 202 \tLoss: 1.477827787399292\n",
      "Epoch: 2 \tBatch: 203 \tLoss: 1.498753309249878\n",
      "Epoch: 2 \tBatch: 204 \tLoss: 1.4793490171432495\n",
      "Epoch: 2 \tBatch: 205 \tLoss: 1.466780424118042\n",
      "Epoch: 2 \tBatch: 206 \tLoss: 1.4757602214813232\n",
      "Epoch: 2 \tBatch: 207 \tLoss: 1.4944626092910767\n",
      "Epoch: 2 \tBatch: 208 \tLoss: 1.5060091018676758\n",
      "Epoch: 2 \tBatch: 209 \tLoss: 1.4880609512329102\n",
      "Epoch: 2 \tBatch: 210 \tLoss: 1.4863860607147217\n",
      "Epoch: 2 \tBatch: 211 \tLoss: 1.475586175918579\n",
      "Epoch: 2 \tBatch: 212 \tLoss: 1.510840654373169\n",
      "Epoch: 2 \tBatch: 213 \tLoss: 1.4721660614013672\n",
      "Epoch: 2 \tBatch: 214 \tLoss: 1.49130380153656\n",
      "Epoch: 2 \tBatch: 215 \tLoss: 1.4632132053375244\n",
      "Epoch: 2 \tBatch: 216 \tLoss: 1.4723942279815674\n",
      "Epoch: 2 \tBatch: 217 \tLoss: 1.4648364782333374\n",
      "Epoch: 2 \tBatch: 218 \tLoss: 1.5037273168563843\n",
      "Epoch: 2 \tBatch: 219 \tLoss: 1.489502191543579\n",
      "Epoch: 2 \tBatch: 220 \tLoss: 1.485594630241394\n",
      "Epoch: 2 \tBatch: 221 \tLoss: 1.4887597560882568\n",
      "Epoch: 2 \tBatch: 222 \tLoss: 1.4835983514785767\n",
      "Epoch: 2 \tBatch: 223 \tLoss: 1.4818862676620483\n",
      "Epoch: 2 \tBatch: 224 \tLoss: 1.4777065515518188\n",
      "Epoch: 2 \tBatch: 225 \tLoss: 1.4943126440048218\n",
      "Epoch: 2 \tBatch: 226 \tLoss: 1.5025498867034912\n",
      "Epoch: 2 \tBatch: 227 \tLoss: 1.4732587337493896\n",
      "Epoch: 2 \tBatch: 228 \tLoss: 1.491266131401062\n",
      "Epoch: 2 \tBatch: 229 \tLoss: 1.4725748300552368\n",
      "Epoch: 2 \tBatch: 230 \tLoss: 1.4887452125549316\n",
      "Epoch: 2 \tBatch: 231 \tLoss: 1.4884475469589233\n",
      "Epoch: 2 \tBatch: 232 \tLoss: 1.4661003351211548\n",
      "Epoch: 2 \tBatch: 233 \tLoss: 1.5019382238388062\n",
      "Epoch: 2 \tBatch: 234 \tLoss: 1.4767869710922241\n",
      "Epoch: 2 \tBatch: 235 \tLoss: 1.5027414560317993\n",
      "Epoch: 2 \tBatch: 236 \tLoss: 1.4889754056930542\n",
      "Epoch: 2 \tBatch: 237 \tLoss: 1.4878218173980713\n",
      "Epoch: 2 \tBatch: 238 \tLoss: 1.4845024347305298\n",
      "Epoch: 2 \tBatch: 239 \tLoss: 1.4812363386154175\n",
      "Epoch: 2 \tBatch: 240 \tLoss: 1.486788034439087\n",
      "Epoch: 2 \tBatch: 241 \tLoss: 1.4900786876678467\n",
      "Epoch: 2 \tBatch: 242 \tLoss: 1.4967423677444458\n",
      "Epoch: 2 \tBatch: 243 \tLoss: 1.4687390327453613\n",
      "Epoch: 2 \tBatch: 244 \tLoss: 1.493740439414978\n",
      "Epoch: 2 \tBatch: 245 \tLoss: 1.4912729263305664\n",
      "Epoch: 2 \tBatch: 246 \tLoss: 1.4942622184753418\n",
      "Epoch: 2 \tBatch: 247 \tLoss: 1.501950740814209\n",
      "Epoch: 2 \tBatch: 248 \tLoss: 1.4855952262878418\n",
      "Epoch: 2 \tBatch: 249 \tLoss: 1.486053228378296\n",
      "Epoch: 2 \tBatch: 250 \tLoss: 1.4994884729385376\n",
      "Epoch: 2 \tBatch: 251 \tLoss: 1.4784138202667236\n",
      "Epoch: 2 \tBatch: 252 \tLoss: 1.4978041648864746\n",
      "Epoch: 2 \tBatch: 253 \tLoss: 1.478509545326233\n",
      "Epoch: 2 \tBatch: 254 \tLoss: 1.5025147199630737\n",
      "Epoch: 2 \tBatch: 255 \tLoss: 1.5007424354553223\n",
      "Epoch: 2 \tBatch: 256 \tLoss: 1.4754269123077393\n",
      "Epoch: 2 \tBatch: 257 \tLoss: 1.4996789693832397\n",
      "Epoch: 2 \tBatch: 258 \tLoss: 1.4852551221847534\n",
      "Epoch: 2 \tBatch: 259 \tLoss: 1.5112987756729126\n",
      "Epoch: 2 \tBatch: 260 \tLoss: 1.5000752210617065\n",
      "Epoch: 2 \tBatch: 261 \tLoss: 1.479596734046936\n",
      "Epoch: 2 \tBatch: 262 \tLoss: 1.4824638366699219\n",
      "Epoch: 2 \tBatch: 263 \tLoss: 1.4965088367462158\n",
      "Epoch: 2 \tBatch: 264 \tLoss: 1.49119234085083\n",
      "Epoch: 2 \tBatch: 265 \tLoss: 1.5066838264465332\n",
      "Epoch: 2 \tBatch: 266 \tLoss: 1.49858820438385\n",
      "Epoch: 2 \tBatch: 267 \tLoss: 1.5047574043273926\n",
      "Epoch: 2 \tBatch: 268 \tLoss: 1.4743274450302124\n",
      "Epoch: 2 \tBatch: 269 \tLoss: 1.4754409790039062\n",
      "Epoch: 2 \tBatch: 270 \tLoss: 1.4769387245178223\n",
      "Epoch: 2 \tBatch: 271 \tLoss: 1.4800915718078613\n",
      "Epoch: 2 \tBatch: 272 \tLoss: 1.4922192096710205\n",
      "Epoch: 2 \tBatch: 273 \tLoss: 1.5170106887817383\n",
      "Epoch: 2 \tBatch: 274 \tLoss: 1.4809614419937134\n",
      "Epoch: 2 \tBatch: 275 \tLoss: 1.4851222038269043\n",
      "Epoch: 2 \tBatch: 276 \tLoss: 1.4805914163589478\n",
      "Epoch: 2 \tBatch: 277 \tLoss: 1.500022053718567\n",
      "Epoch: 2 \tBatch: 278 \tLoss: 1.501697301864624\n",
      "Epoch: 2 \tBatch: 279 \tLoss: 1.472496747970581\n",
      "Epoch: 2 \tBatch: 280 \tLoss: 1.4634870290756226\n",
      "Epoch: 2 \tBatch: 281 \tLoss: 1.4963934421539307\n",
      "Epoch: 2 \tBatch: 282 \tLoss: 1.479000210762024\n",
      "Epoch: 2 \tBatch: 283 \tLoss: 1.4909571409225464\n",
      "Epoch: 2 \tBatch: 284 \tLoss: 1.481752872467041\n",
      "Epoch: 2 \tBatch: 285 \tLoss: 1.4887349605560303\n",
      "Epoch: 2 \tBatch: 286 \tLoss: 1.4766064882278442\n",
      "Epoch: 2 \tBatch: 287 \tLoss: 1.491629719734192\n",
      "Epoch: 2 \tBatch: 288 \tLoss: 1.4912487268447876\n",
      "Epoch: 2 \tBatch: 289 \tLoss: 1.4739001989364624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tBatch: 290 \tLoss: 1.4922620058059692\n",
      "Epoch: 2 \tBatch: 291 \tLoss: 1.518302321434021\n",
      "Epoch: 2 \tBatch: 292 \tLoss: 1.4697290658950806\n",
      "Epoch: 2 \tBatch: 293 \tLoss: 1.4867620468139648\n",
      "Epoch: 2 \tBatch: 294 \tLoss: 1.504535436630249\n",
      "Epoch: 2 \tBatch: 295 \tLoss: 1.4718866348266602\n",
      "Epoch: 2 \tBatch: 296 \tLoss: 1.4992376565933228\n",
      "Epoch: 2 \tBatch: 297 \tLoss: 1.474982738494873\n",
      "Epoch: 2 \tBatch: 298 \tLoss: 1.4975252151489258\n",
      "Epoch: 2 \tBatch: 299 \tLoss: 1.49477219581604\n",
      "Epoch: 2 \tBatch: 300 \tLoss: 1.4758480787277222\n",
      "Epoch: 2 \tBatch: 301 \tLoss: 1.4804790019989014\n",
      "Epoch: 2 \tBatch: 302 \tLoss: 1.4758964776992798\n",
      "Epoch: 2 \tBatch: 303 \tLoss: 1.4753608703613281\n",
      "Epoch: 2 \tBatch: 304 \tLoss: 1.4771034717559814\n",
      "Epoch: 2 \tBatch: 305 \tLoss: 1.4806914329528809\n",
      "Epoch: 2 \tBatch: 306 \tLoss: 1.497828722000122\n",
      "Epoch: 2 \tBatch: 307 \tLoss: 1.4815391302108765\n",
      "Epoch: 2 \tBatch: 308 \tLoss: 1.490398645401001\n",
      "Epoch: 2 \tBatch: 309 \tLoss: 1.5069384574890137\n",
      "Epoch: 2 \tBatch: 310 \tLoss: 1.483681321144104\n",
      "Epoch: 2 \tBatch: 311 \tLoss: 1.4851752519607544\n",
      "Epoch: 2 \tBatch: 312 \tLoss: 1.5105016231536865\n",
      "Epoch: 2 \tBatch: 313 \tLoss: 1.4747165441513062\n",
      "Epoch: 2 \tBatch: 314 \tLoss: 1.4725688695907593\n",
      "Epoch: 2 \tBatch: 315 \tLoss: 1.4654018878936768\n",
      "Epoch: 2 \tBatch: 316 \tLoss: 1.471603512763977\n",
      "Epoch: 2 \tBatch: 317 \tLoss: 1.5081349611282349\n",
      "Epoch: 2 \tBatch: 318 \tLoss: 1.4870810508728027\n",
      "Epoch: 2 \tBatch: 319 \tLoss: 1.469031810760498\n",
      "Epoch: 2 \tBatch: 320 \tLoss: 1.4754858016967773\n",
      "Epoch: 2 \tBatch: 321 \tLoss: 1.4848840236663818\n",
      "Epoch: 2 \tBatch: 322 \tLoss: 1.4755737781524658\n",
      "Epoch: 2 \tBatch: 323 \tLoss: 1.4992294311523438\n",
      "Epoch: 2 \tBatch: 324 \tLoss: 1.4742597341537476\n",
      "Epoch: 2 \tBatch: 325 \tLoss: 1.4770702123641968\n",
      "Epoch: 2 \tBatch: 326 \tLoss: 1.4948196411132812\n",
      "Epoch: 2 \tBatch: 327 \tLoss: 1.475693702697754\n",
      "Epoch: 2 \tBatch: 328 \tLoss: 1.485183596611023\n",
      "Epoch: 2 \tBatch: 329 \tLoss: 1.4650275707244873\n",
      "Epoch: 2 \tBatch: 330 \tLoss: 1.4996817111968994\n",
      "Epoch: 2 \tBatch: 331 \tLoss: 1.4887843132019043\n",
      "Epoch: 2 \tBatch: 332 \tLoss: 1.4738423824310303\n",
      "Epoch: 2 \tBatch: 333 \tLoss: 1.4837316274642944\n",
      "Epoch: 2 \tBatch: 334 \tLoss: 1.4709471464157104\n",
      "Epoch: 2 \tBatch: 335 \tLoss: 1.4913276433944702\n",
      "Epoch: 2 \tBatch: 336 \tLoss: 1.4934735298156738\n",
      "Epoch: 2 \tBatch: 337 \tLoss: 1.4812462329864502\n",
      "Epoch: 2 \tBatch: 338 \tLoss: 1.4856011867523193\n",
      "Epoch: 2 \tBatch: 339 \tLoss: 1.5004487037658691\n",
      "Epoch: 2 \tBatch: 340 \tLoss: 1.5208841562271118\n",
      "Epoch: 2 \tBatch: 341 \tLoss: 1.502927303314209\n",
      "Epoch: 2 \tBatch: 342 \tLoss: 1.5175856351852417\n",
      "Epoch: 2 \tBatch: 343 \tLoss: 1.499766230583191\n",
      "Epoch: 2 \tBatch: 344 \tLoss: 1.4810675382614136\n",
      "Epoch: 2 \tBatch: 345 \tLoss: 1.4898744821548462\n",
      "Epoch: 2 \tBatch: 346 \tLoss: 1.4869463443756104\n",
      "Epoch: 2 \tBatch: 347 \tLoss: 1.4849592447280884\n",
      "Epoch: 2 \tBatch: 348 \tLoss: 1.5024305582046509\n",
      "Epoch: 2 \tBatch: 349 \tLoss: 1.485816478729248\n",
      "Epoch: 2 \tBatch: 350 \tLoss: 1.4974005222320557\n",
      "Epoch: 2 \tBatch: 351 \tLoss: 1.4916924238204956\n",
      "Epoch: 2 \tBatch: 352 \tLoss: 1.483327031135559\n",
      "Epoch: 2 \tBatch: 353 \tLoss: 1.4842991828918457\n",
      "Epoch: 2 \tBatch: 354 \tLoss: 1.4794949293136597\n",
      "Epoch: 2 \tBatch: 355 \tLoss: 1.4815846681594849\n",
      "Epoch: 2 \tBatch: 356 \tLoss: 1.4782743453979492\n",
      "Epoch: 2 \tBatch: 357 \tLoss: 1.482304334640503\n",
      "Epoch: 2 \tBatch: 358 \tLoss: 1.4840466976165771\n",
      "Epoch: 2 \tBatch: 359 \tLoss: 1.491442322731018\n",
      "Epoch: 2 \tBatch: 360 \tLoss: 1.502921462059021\n",
      "Epoch: 2 \tBatch: 361 \tLoss: 1.5022155046463013\n",
      "Epoch: 2 \tBatch: 362 \tLoss: 1.4970742464065552\n",
      "Epoch: 2 \tBatch: 363 \tLoss: 1.4953562021255493\n",
      "Epoch: 2 \tBatch: 364 \tLoss: 1.4844566583633423\n",
      "Epoch: 2 \tBatch: 365 \tLoss: 1.4787707328796387\n",
      "Epoch: 2 \tBatch: 366 \tLoss: 1.490309476852417\n",
      "Epoch: 2 \tBatch: 367 \tLoss: 1.493770718574524\n",
      "Epoch: 2 \tBatch: 368 \tLoss: 1.4744118452072144\n",
      "Epoch: 2 \tBatch: 369 \tLoss: 1.4946889877319336\n",
      "Epoch: 2 \tBatch: 370 \tLoss: 1.4839391708374023\n",
      "Epoch: 2 \tBatch: 371 \tLoss: 1.4704946279525757\n",
      "Epoch: 2 \tBatch: 372 \tLoss: 1.489285945892334\n",
      "Epoch: 2 \tBatch: 373 \tLoss: 1.4725135564804077\n",
      "Epoch: 2 \tBatch: 374 \tLoss: 1.4826748371124268\n",
      "Epoch: 2 \tBatch: 375 \tLoss: 1.4941502809524536\n",
      "Epoch: 2 \tBatch: 376 \tLoss: 1.4756224155426025\n",
      "Epoch: 2 \tBatch: 377 \tLoss: 1.4693244695663452\n",
      "Epoch: 2 \tBatch: 378 \tLoss: 1.5024943351745605\n",
      "Epoch: 2 \tBatch: 379 \tLoss: 1.4882210493087769\n",
      "Epoch: 2 \tBatch: 380 \tLoss: 1.5038251876831055\n",
      "Epoch: 2 \tBatch: 381 \tLoss: 1.4947870969772339\n",
      "Epoch: 2 \tBatch: 382 \tLoss: 1.4962074756622314\n",
      "Epoch: 2 \tBatch: 383 \tLoss: 1.4654780626296997\n",
      "Epoch: 2 \tBatch: 384 \tLoss: 1.4699904918670654\n",
      "Epoch: 2 \tBatch: 385 \tLoss: 1.482511281967163\n",
      "Epoch: 2 \tBatch: 386 \tLoss: 1.4765323400497437\n",
      "Epoch: 2 \tBatch: 387 \tLoss: 1.4782859086990356\n",
      "Epoch: 2 \tBatch: 388 \tLoss: 1.5253074169158936\n",
      "Epoch: 2 \tBatch: 389 \tLoss: 1.4751594066619873\n",
      "Epoch: 2 \tBatch: 390 \tLoss: 1.4835526943206787\n",
      "Epoch: 3 \tBatch: 0 \tLoss: 1.485643982887268\n",
      "Epoch: 3 \tBatch: 1 \tLoss: 1.4965214729309082\n",
      "Epoch: 3 \tBatch: 2 \tLoss: 1.479089617729187\n",
      "Epoch: 3 \tBatch: 3 \tLoss: 1.4708144664764404\n",
      "Epoch: 3 \tBatch: 4 \tLoss: 1.5107243061065674\n",
      "Epoch: 3 \tBatch: 5 \tLoss: 1.518361210823059\n",
      "Epoch: 3 \tBatch: 6 \tLoss: 1.4748539924621582\n",
      "Epoch: 3 \tBatch: 7 \tLoss: 1.4851207733154297\n",
      "Epoch: 3 \tBatch: 8 \tLoss: 1.473681092262268\n",
      "Epoch: 3 \tBatch: 9 \tLoss: 1.4730280637741089\n",
      "Epoch: 3 \tBatch: 10 \tLoss: 1.5031565427780151\n",
      "Epoch: 3 \tBatch: 11 \tLoss: 1.465302586555481\n",
      "Epoch: 3 \tBatch: 12 \tLoss: 1.5150973796844482\n",
      "Epoch: 3 \tBatch: 13 \tLoss: 1.4967585802078247\n",
      "Epoch: 3 \tBatch: 14 \tLoss: 1.5196552276611328\n",
      "Epoch: 3 \tBatch: 15 \tLoss: 1.4905657768249512\n",
      "Epoch: 3 \tBatch: 16 \tLoss: 1.4941644668579102\n",
      "Epoch: 3 \tBatch: 17 \tLoss: 1.491919755935669\n",
      "Epoch: 3 \tBatch: 18 \tLoss: 1.4624717235565186\n",
      "Epoch: 3 \tBatch: 19 \tLoss: 1.4924591779708862\n",
      "Epoch: 3 \tBatch: 20 \tLoss: 1.467537760734558\n",
      "Epoch: 3 \tBatch: 21 \tLoss: 1.4817405939102173\n",
      "Epoch: 3 \tBatch: 22 \tLoss: 1.4676458835601807\n",
      "Epoch: 3 \tBatch: 23 \tLoss: 1.472537636756897\n",
      "Epoch: 3 \tBatch: 24 \tLoss: 1.4733141660690308\n",
      "Epoch: 3 \tBatch: 25 \tLoss: 1.4781309366226196\n",
      "Epoch: 3 \tBatch: 26 \tLoss: 1.4615285396575928\n",
      "Epoch: 3 \tBatch: 27 \tLoss: 1.484055995941162\n",
      "Epoch: 3 \tBatch: 28 \tLoss: 1.507177472114563\n",
      "Epoch: 3 \tBatch: 29 \tLoss: 1.471535325050354\n",
      "Epoch: 3 \tBatch: 30 \tLoss: 1.4756901264190674\n",
      "Epoch: 3 \tBatch: 31 \tLoss: 1.4770793914794922\n",
      "Epoch: 3 \tBatch: 32 \tLoss: 1.4916387796401978\n",
      "Epoch: 3 \tBatch: 33 \tLoss: 1.4761319160461426\n",
      "Epoch: 3 \tBatch: 34 \tLoss: 1.4691609144210815\n",
      "Epoch: 3 \tBatch: 35 \tLoss: 1.474713683128357\n",
      "Epoch: 3 \tBatch: 36 \tLoss: 1.4658514261245728\n",
      "Epoch: 3 \tBatch: 37 \tLoss: 1.4747607707977295\n",
      "Epoch: 3 \tBatch: 38 \tLoss: 1.4694668054580688\n",
      "Epoch: 3 \tBatch: 39 \tLoss: 1.4830307960510254\n",
      "Epoch: 3 \tBatch: 40 \tLoss: 1.480238914489746\n",
      "Epoch: 3 \tBatch: 41 \tLoss: 1.4869563579559326\n",
      "Epoch: 3 \tBatch: 42 \tLoss: 1.4904062747955322\n",
      "Epoch: 3 \tBatch: 43 \tLoss: 1.4828698635101318\n",
      "Epoch: 3 \tBatch: 44 \tLoss: 1.4786261320114136\n",
      "Epoch: 3 \tBatch: 45 \tLoss: 1.4869736433029175\n",
      "Epoch: 3 \tBatch: 46 \tLoss: 1.4764039516448975\n",
      "Epoch: 3 \tBatch: 47 \tLoss: 1.474340796470642\n",
      "Epoch: 3 \tBatch: 48 \tLoss: 1.5013883113861084\n",
      "Epoch: 3 \tBatch: 49 \tLoss: 1.4765156507492065\n",
      "Epoch: 3 \tBatch: 50 \tLoss: 1.494545578956604\n",
      "Epoch: 3 \tBatch: 51 \tLoss: 1.511813998222351\n",
      "Epoch: 3 \tBatch: 52 \tLoss: 1.471164584159851\n",
      "Epoch: 3 \tBatch: 53 \tLoss: 1.4796850681304932\n",
      "Epoch: 3 \tBatch: 54 \tLoss: 1.4691596031188965\n",
      "Epoch: 3 \tBatch: 55 \tLoss: 1.479217529296875\n",
      "Epoch: 3 \tBatch: 56 \tLoss: 1.4736390113830566\n",
      "Epoch: 3 \tBatch: 57 \tLoss: 1.492514967918396\n",
      "Epoch: 3 \tBatch: 58 \tLoss: 1.4729787111282349\n",
      "Epoch: 3 \tBatch: 59 \tLoss: 1.486957311630249\n",
      "Epoch: 3 \tBatch: 60 \tLoss: 1.486005187034607\n",
      "Epoch: 3 \tBatch: 61 \tLoss: 1.475414752960205\n",
      "Epoch: 3 \tBatch: 62 \tLoss: 1.4708753824234009\n",
      "Epoch: 3 \tBatch: 63 \tLoss: 1.4939403533935547\n",
      "Epoch: 3 \tBatch: 64 \tLoss: 1.5103075504302979\n",
      "Epoch: 3 \tBatch: 65 \tLoss: 1.4980087280273438\n",
      "Epoch: 3 \tBatch: 66 \tLoss: 1.4740856885910034\n",
      "Epoch: 3 \tBatch: 67 \tLoss: 1.471217393875122\n",
      "Epoch: 3 \tBatch: 68 \tLoss: 1.4832701683044434\n",
      "Epoch: 3 \tBatch: 69 \tLoss: 1.4747027158737183\n",
      "Epoch: 3 \tBatch: 70 \tLoss: 1.4929454326629639\n",
      "Epoch: 3 \tBatch: 71 \tLoss: 1.499098777770996\n",
      "Epoch: 3 \tBatch: 72 \tLoss: 1.4862310886383057\n",
      "Epoch: 3 \tBatch: 73 \tLoss: 1.495686650276184\n",
      "Epoch: 3 \tBatch: 74 \tLoss: 1.4664720296859741\n",
      "Epoch: 3 \tBatch: 75 \tLoss: 1.5015019178390503\n",
      "Epoch: 3 \tBatch: 76 \tLoss: 1.4903955459594727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tBatch: 77 \tLoss: 1.4654992818832397\n",
      "Epoch: 3 \tBatch: 78 \tLoss: 1.4732236862182617\n",
      "Epoch: 3 \tBatch: 79 \tLoss: 1.482347011566162\n",
      "Epoch: 3 \tBatch: 80 \tLoss: 1.466111421585083\n",
      "Epoch: 3 \tBatch: 81 \tLoss: 1.4706839323043823\n",
      "Epoch: 3 \tBatch: 82 \tLoss: 1.4713503122329712\n",
      "Epoch: 3 \tBatch: 83 \tLoss: 1.4723800420761108\n",
      "Epoch: 3 \tBatch: 84 \tLoss: 1.473725438117981\n",
      "Epoch: 3 \tBatch: 85 \tLoss: 1.472153663635254\n",
      "Epoch: 3 \tBatch: 86 \tLoss: 1.4703923463821411\n",
      "Epoch: 3 \tBatch: 87 \tLoss: 1.4964325428009033\n",
      "Epoch: 3 \tBatch: 88 \tLoss: 1.483400583267212\n",
      "Epoch: 3 \tBatch: 89 \tLoss: 1.4777806997299194\n",
      "Epoch: 3 \tBatch: 90 \tLoss: 1.4877748489379883\n",
      "Epoch: 3 \tBatch: 91 \tLoss: 1.4815183877944946\n",
      "Epoch: 3 \tBatch: 92 \tLoss: 1.4873318672180176\n",
      "Epoch: 3 \tBatch: 93 \tLoss: 1.480092167854309\n",
      "Epoch: 3 \tBatch: 94 \tLoss: 1.4794172048568726\n",
      "Epoch: 3 \tBatch: 95 \tLoss: 1.486976981163025\n",
      "Epoch: 3 \tBatch: 96 \tLoss: 1.4856315851211548\n",
      "Epoch: 3 \tBatch: 97 \tLoss: 1.4738746881484985\n",
      "Epoch: 3 \tBatch: 98 \tLoss: 1.4694445133209229\n",
      "Epoch: 3 \tBatch: 99 \tLoss: 1.5156760215759277\n",
      "Epoch: 3 \tBatch: 100 \tLoss: 1.471935510635376\n",
      "Epoch: 3 \tBatch: 101 \tLoss: 1.474509596824646\n",
      "Epoch: 3 \tBatch: 102 \tLoss: 1.4847766160964966\n",
      "Epoch: 3 \tBatch: 103 \tLoss: 1.4803762435913086\n",
      "Epoch: 3 \tBatch: 104 \tLoss: 1.478214979171753\n",
      "Epoch: 3 \tBatch: 105 \tLoss: 1.4965709447860718\n",
      "Epoch: 3 \tBatch: 106 \tLoss: 1.478312611579895\n",
      "Epoch: 3 \tBatch: 107 \tLoss: 1.472984790802002\n",
      "Epoch: 3 \tBatch: 108 \tLoss: 1.4793105125427246\n",
      "Epoch: 3 \tBatch: 109 \tLoss: 1.4883235692977905\n",
      "Epoch: 3 \tBatch: 110 \tLoss: 1.4833855628967285\n",
      "Epoch: 3 \tBatch: 111 \tLoss: 1.4695451259613037\n",
      "Epoch: 3 \tBatch: 112 \tLoss: 1.5030617713928223\n",
      "Epoch: 3 \tBatch: 113 \tLoss: 1.4827195405960083\n",
      "Epoch: 3 \tBatch: 114 \tLoss: 1.4634712934494019\n",
      "Epoch: 3 \tBatch: 115 \tLoss: 1.4822778701782227\n",
      "Epoch: 3 \tBatch: 116 \tLoss: 1.4778071641921997\n",
      "Epoch: 3 \tBatch: 117 \tLoss: 1.5039730072021484\n",
      "Epoch: 3 \tBatch: 118 \tLoss: 1.5005658864974976\n",
      "Epoch: 3 \tBatch: 119 \tLoss: 1.477697730064392\n",
      "Epoch: 3 \tBatch: 120 \tLoss: 1.4877251386642456\n",
      "Epoch: 3 \tBatch: 121 \tLoss: 1.4697906970977783\n",
      "Epoch: 3 \tBatch: 122 \tLoss: 1.502539038658142\n",
      "Epoch: 3 \tBatch: 123 \tLoss: 1.486296534538269\n",
      "Epoch: 3 \tBatch: 124 \tLoss: 1.4717930555343628\n",
      "Epoch: 3 \tBatch: 125 \tLoss: 1.473372220993042\n",
      "Epoch: 3 \tBatch: 126 \tLoss: 1.48056161403656\n",
      "Epoch: 3 \tBatch: 127 \tLoss: 1.4930319786071777\n",
      "Epoch: 3 \tBatch: 128 \tLoss: 1.472294569015503\n",
      "Epoch: 3 \tBatch: 129 \tLoss: 1.4810864925384521\n",
      "Epoch: 3 \tBatch: 130 \tLoss: 1.4661206007003784\n",
      "Epoch: 3 \tBatch: 131 \tLoss: 1.475529670715332\n",
      "Epoch: 3 \tBatch: 132 \tLoss: 1.4655671119689941\n",
      "Epoch: 3 \tBatch: 133 \tLoss: 1.4907082319259644\n",
      "Epoch: 3 \tBatch: 134 \tLoss: 1.473753809928894\n",
      "Epoch: 3 \tBatch: 135 \tLoss: 1.4894373416900635\n",
      "Epoch: 3 \tBatch: 136 \tLoss: 1.4865453243255615\n",
      "Epoch: 3 \tBatch: 137 \tLoss: 1.5062485933303833\n",
      "Epoch: 3 \tBatch: 138 \tLoss: 1.4627954959869385\n",
      "Epoch: 3 \tBatch: 139 \tLoss: 1.4786707162857056\n",
      "Epoch: 3 \tBatch: 140 \tLoss: 1.4751895666122437\n",
      "Epoch: 3 \tBatch: 141 \tLoss: 1.4758301973342896\n",
      "Epoch: 3 \tBatch: 142 \tLoss: 1.5078214406967163\n",
      "Epoch: 3 \tBatch: 143 \tLoss: 1.4765443801879883\n",
      "Epoch: 3 \tBatch: 144 \tLoss: 1.4805744886398315\n",
      "Epoch: 3 \tBatch: 145 \tLoss: 1.4831950664520264\n",
      "Epoch: 3 \tBatch: 146 \tLoss: 1.481750726699829\n",
      "Epoch: 3 \tBatch: 147 \tLoss: 1.4753541946411133\n",
      "Epoch: 3 \tBatch: 148 \tLoss: 1.5087898969650269\n",
      "Epoch: 3 \tBatch: 149 \tLoss: 1.495652198791504\n",
      "Epoch: 3 \tBatch: 150 \tLoss: 1.4856528043746948\n",
      "Epoch: 3 \tBatch: 151 \tLoss: 1.4683914184570312\n",
      "Epoch: 3 \tBatch: 152 \tLoss: 1.4806289672851562\n",
      "Epoch: 3 \tBatch: 153 \tLoss: 1.4756407737731934\n",
      "Epoch: 3 \tBatch: 154 \tLoss: 1.4812195301055908\n",
      "Epoch: 3 \tBatch: 155 \tLoss: 1.5036423206329346\n",
      "Epoch: 3 \tBatch: 156 \tLoss: 1.4756907224655151\n",
      "Epoch: 3 \tBatch: 157 \tLoss: 1.4808350801467896\n",
      "Epoch: 3 \tBatch: 158 \tLoss: 1.502808928489685\n",
      "Epoch: 3 \tBatch: 159 \tLoss: 1.4910558462142944\n",
      "Epoch: 3 \tBatch: 160 \tLoss: 1.4721739292144775\n",
      "Epoch: 3 \tBatch: 161 \tLoss: 1.4884023666381836\n",
      "Epoch: 3 \tBatch: 162 \tLoss: 1.4793716669082642\n",
      "Epoch: 3 \tBatch: 163 \tLoss: 1.4658448696136475\n",
      "Epoch: 3 \tBatch: 164 \tLoss: 1.4900867938995361\n",
      "Epoch: 3 \tBatch: 165 \tLoss: 1.4848705530166626\n",
      "Epoch: 3 \tBatch: 166 \tLoss: 1.4790401458740234\n",
      "Epoch: 3 \tBatch: 167 \tLoss: 1.486333966255188\n",
      "Epoch: 3 \tBatch: 168 \tLoss: 1.4942514896392822\n",
      "Epoch: 3 \tBatch: 169 \tLoss: 1.5003825426101685\n",
      "Epoch: 3 \tBatch: 170 \tLoss: 1.4896230697631836\n",
      "Epoch: 3 \tBatch: 171 \tLoss: 1.4830724000930786\n",
      "Epoch: 3 \tBatch: 172 \tLoss: 1.4843852519989014\n",
      "Epoch: 3 \tBatch: 173 \tLoss: 1.4749562740325928\n",
      "Epoch: 3 \tBatch: 174 \tLoss: 1.488996148109436\n",
      "Epoch: 3 \tBatch: 175 \tLoss: 1.4836372137069702\n",
      "Epoch: 3 \tBatch: 176 \tLoss: 1.47538161277771\n",
      "Epoch: 3 \tBatch: 177 \tLoss: 1.470339059829712\n",
      "Epoch: 3 \tBatch: 178 \tLoss: 1.4848250150680542\n",
      "Epoch: 3 \tBatch: 179 \tLoss: 1.4831372499465942\n",
      "Epoch: 3 \tBatch: 180 \tLoss: 1.4878963232040405\n",
      "Epoch: 3 \tBatch: 181 \tLoss: 1.4618583917617798\n",
      "Epoch: 3 \tBatch: 182 \tLoss: 1.496100664138794\n",
      "Epoch: 3 \tBatch: 183 \tLoss: 1.471034049987793\n",
      "Epoch: 3 \tBatch: 184 \tLoss: 1.48836088180542\n",
      "Epoch: 3 \tBatch: 185 \tLoss: 1.4944169521331787\n",
      "Epoch: 3 \tBatch: 186 \tLoss: 1.513480305671692\n",
      "Epoch: 3 \tBatch: 187 \tLoss: 1.479651689529419\n",
      "Epoch: 3 \tBatch: 188 \tLoss: 1.4899396896362305\n",
      "Epoch: 3 \tBatch: 189 \tLoss: 1.4839986562728882\n",
      "Epoch: 3 \tBatch: 190 \tLoss: 1.4689778089523315\n",
      "Epoch: 3 \tBatch: 191 \tLoss: 1.512520432472229\n",
      "Epoch: 3 \tBatch: 192 \tLoss: 1.4824777841567993\n",
      "Epoch: 3 \tBatch: 193 \tLoss: 1.487931489944458\n",
      "Epoch: 3 \tBatch: 194 \tLoss: 1.4733655452728271\n",
      "Epoch: 3 \tBatch: 195 \tLoss: 1.4722806215286255\n",
      "Epoch: 3 \tBatch: 196 \tLoss: 1.4810231924057007\n",
      "Epoch: 3 \tBatch: 197 \tLoss: 1.4914301633834839\n",
      "Epoch: 3 \tBatch: 198 \tLoss: 1.4941034317016602\n",
      "Epoch: 3 \tBatch: 199 \tLoss: 1.4867137670516968\n",
      "Epoch: 3 \tBatch: 200 \tLoss: 1.4813265800476074\n",
      "Epoch: 3 \tBatch: 201 \tLoss: 1.4914048910140991\n",
      "Epoch: 3 \tBatch: 202 \tLoss: 1.4860855340957642\n",
      "Epoch: 3 \tBatch: 203 \tLoss: 1.4748996496200562\n",
      "Epoch: 3 \tBatch: 204 \tLoss: 1.4813734292984009\n",
      "Epoch: 3 \tBatch: 205 \tLoss: 1.4808117151260376\n",
      "Epoch: 3 \tBatch: 206 \tLoss: 1.4646918773651123\n",
      "Epoch: 3 \tBatch: 207 \tLoss: 1.481904149055481\n",
      "Epoch: 3 \tBatch: 208 \tLoss: 1.490870714187622\n",
      "Epoch: 3 \tBatch: 209 \tLoss: 1.512081503868103\n",
      "Epoch: 3 \tBatch: 210 \tLoss: 1.4732661247253418\n",
      "Epoch: 3 \tBatch: 211 \tLoss: 1.4809316396713257\n",
      "Epoch: 3 \tBatch: 212 \tLoss: 1.4851614236831665\n",
      "Epoch: 3 \tBatch: 213 \tLoss: 1.4771333932876587\n",
      "Epoch: 3 \tBatch: 214 \tLoss: 1.4710502624511719\n",
      "Epoch: 3 \tBatch: 215 \tLoss: 1.4943175315856934\n",
      "Epoch: 3 \tBatch: 216 \tLoss: 1.4693211317062378\n",
      "Epoch: 3 \tBatch: 217 \tLoss: 1.4819239377975464\n",
      "Epoch: 3 \tBatch: 218 \tLoss: 1.4851570129394531\n",
      "Epoch: 3 \tBatch: 219 \tLoss: 1.4700205326080322\n",
      "Epoch: 3 \tBatch: 220 \tLoss: 1.4910392761230469\n",
      "Epoch: 3 \tBatch: 221 \tLoss: 1.4629151821136475\n",
      "Epoch: 3 \tBatch: 222 \tLoss: 1.4803634881973267\n",
      "Epoch: 3 \tBatch: 223 \tLoss: 1.4676369428634644\n",
      "Epoch: 3 \tBatch: 224 \tLoss: 1.4775714874267578\n",
      "Epoch: 3 \tBatch: 225 \tLoss: 1.4860055446624756\n",
      "Epoch: 3 \tBatch: 226 \tLoss: 1.4662562608718872\n",
      "Epoch: 3 \tBatch: 227 \tLoss: 1.495980143547058\n",
      "Epoch: 3 \tBatch: 228 \tLoss: 1.4792182445526123\n",
      "Epoch: 3 \tBatch: 229 \tLoss: 1.4705737829208374\n",
      "Epoch: 3 \tBatch: 230 \tLoss: 1.4629513025283813\n",
      "Epoch: 3 \tBatch: 231 \tLoss: 1.4732121229171753\n",
      "Epoch: 3 \tBatch: 232 \tLoss: 1.4911333322525024\n",
      "Epoch: 3 \tBatch: 233 \tLoss: 1.4701645374298096\n",
      "Epoch: 3 \tBatch: 234 \tLoss: 1.4925943613052368\n",
      "Epoch: 3 \tBatch: 235 \tLoss: 1.4620732069015503\n",
      "Epoch: 3 \tBatch: 236 \tLoss: 1.4861950874328613\n",
      "Epoch: 3 \tBatch: 237 \tLoss: 1.4883332252502441\n",
      "Epoch: 3 \tBatch: 238 \tLoss: 1.4781798124313354\n",
      "Epoch: 3 \tBatch: 239 \tLoss: 1.49552321434021\n",
      "Epoch: 3 \tBatch: 240 \tLoss: 1.496893048286438\n",
      "Epoch: 3 \tBatch: 241 \tLoss: 1.4719408750534058\n",
      "Epoch: 3 \tBatch: 242 \tLoss: 1.4816311597824097\n",
      "Epoch: 3 \tBatch: 243 \tLoss: 1.4960583448410034\n",
      "Epoch: 3 \tBatch: 244 \tLoss: 1.4847856760025024\n",
      "Epoch: 3 \tBatch: 245 \tLoss: 1.475222110748291\n",
      "Epoch: 3 \tBatch: 246 \tLoss: 1.479537844657898\n",
      "Epoch: 3 \tBatch: 247 \tLoss: 1.4719007015228271\n",
      "Epoch: 3 \tBatch: 248 \tLoss: 1.4848393201828003\n",
      "Epoch: 3 \tBatch: 249 \tLoss: 1.477521300315857\n",
      "Epoch: 3 \tBatch: 250 \tLoss: 1.4806898832321167\n",
      "Epoch: 3 \tBatch: 251 \tLoss: 1.4615790843963623\n",
      "Epoch: 3 \tBatch: 252 \tLoss: 1.4760500192642212\n",
      "Epoch: 3 \tBatch: 253 \tLoss: 1.4754846096038818\n",
      "Epoch: 3 \tBatch: 254 \tLoss: 1.4833277463912964\n",
      "Epoch: 3 \tBatch: 255 \tLoss: 1.4724892377853394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tBatch: 256 \tLoss: 1.4726366996765137\n",
      "Epoch: 3 \tBatch: 257 \tLoss: 1.4943708181381226\n",
      "Epoch: 3 \tBatch: 258 \tLoss: 1.469704270362854\n",
      "Epoch: 3 \tBatch: 259 \tLoss: 1.4793983697891235\n",
      "Epoch: 3 \tBatch: 260 \tLoss: 1.4868911504745483\n",
      "Epoch: 3 \tBatch: 261 \tLoss: 1.4795297384262085\n",
      "Epoch: 3 \tBatch: 262 \tLoss: 1.48093581199646\n",
      "Epoch: 3 \tBatch: 263 \tLoss: 1.4880443811416626\n",
      "Epoch: 3 \tBatch: 264 \tLoss: 1.4775118827819824\n",
      "Epoch: 3 \tBatch: 265 \tLoss: 1.4813560247421265\n",
      "Epoch: 3 \tBatch: 266 \tLoss: 1.4709444046020508\n",
      "Epoch: 3 \tBatch: 267 \tLoss: 1.4784241914749146\n",
      "Epoch: 3 \tBatch: 268 \tLoss: 1.4760477542877197\n",
      "Epoch: 3 \tBatch: 269 \tLoss: 1.4773517847061157\n",
      "Epoch: 3 \tBatch: 270 \tLoss: 1.4933738708496094\n",
      "Epoch: 3 \tBatch: 271 \tLoss: 1.4642729759216309\n",
      "Epoch: 3 \tBatch: 272 \tLoss: 1.474320411682129\n",
      "Epoch: 3 \tBatch: 273 \tLoss: 1.4827688932418823\n",
      "Epoch: 3 \tBatch: 274 \tLoss: 1.4741272926330566\n",
      "Epoch: 3 \tBatch: 275 \tLoss: 1.471245288848877\n",
      "Epoch: 3 \tBatch: 276 \tLoss: 1.4698418378829956\n",
      "Epoch: 3 \tBatch: 277 \tLoss: 1.4894191026687622\n",
      "Epoch: 3 \tBatch: 278 \tLoss: 1.4711673259735107\n",
      "Epoch: 3 \tBatch: 279 \tLoss: 1.4969584941864014\n",
      "Epoch: 3 \tBatch: 280 \tLoss: 1.489412784576416\n",
      "Epoch: 3 \tBatch: 281 \tLoss: 1.4848029613494873\n",
      "Epoch: 3 \tBatch: 282 \tLoss: 1.4809736013412476\n",
      "Epoch: 3 \tBatch: 283 \tLoss: 1.4963761568069458\n",
      "Epoch: 3 \tBatch: 284 \tLoss: 1.484499454498291\n",
      "Epoch: 3 \tBatch: 285 \tLoss: 1.4722375869750977\n",
      "Epoch: 3 \tBatch: 286 \tLoss: 1.4894112348556519\n",
      "Epoch: 3 \tBatch: 287 \tLoss: 1.4722086191177368\n",
      "Epoch: 3 \tBatch: 288 \tLoss: 1.4940242767333984\n",
      "Epoch: 3 \tBatch: 289 \tLoss: 1.4836947917938232\n",
      "Epoch: 3 \tBatch: 290 \tLoss: 1.4707392454147339\n",
      "Epoch: 3 \tBatch: 291 \tLoss: 1.481937289237976\n",
      "Epoch: 3 \tBatch: 292 \tLoss: 1.4630241394042969\n",
      "Epoch: 3 \tBatch: 293 \tLoss: 1.4755802154541016\n",
      "Epoch: 3 \tBatch: 294 \tLoss: 1.4876961708068848\n",
      "Epoch: 3 \tBatch: 295 \tLoss: 1.5086047649383545\n",
      "Epoch: 3 \tBatch: 296 \tLoss: 1.4784483909606934\n",
      "Epoch: 3 \tBatch: 297 \tLoss: 1.4829726219177246\n",
      "Epoch: 3 \tBatch: 298 \tLoss: 1.4664939641952515\n",
      "Epoch: 3 \tBatch: 299 \tLoss: 1.4737316370010376\n",
      "Epoch: 3 \tBatch: 300 \tLoss: 1.4894471168518066\n",
      "Epoch: 3 \tBatch: 301 \tLoss: 1.4906303882598877\n",
      "Epoch: 3 \tBatch: 302 \tLoss: 1.512610673904419\n",
      "Epoch: 3 \tBatch: 303 \tLoss: 1.4753773212432861\n",
      "Epoch: 3 \tBatch: 304 \tLoss: 1.4965919256210327\n",
      "Epoch: 3 \tBatch: 305 \tLoss: 1.4778051376342773\n",
      "Epoch: 3 \tBatch: 306 \tLoss: 1.4865435361862183\n",
      "Epoch: 3 \tBatch: 307 \tLoss: 1.4754201173782349\n",
      "Epoch: 3 \tBatch: 308 \tLoss: 1.4789469242095947\n",
      "Epoch: 3 \tBatch: 309 \tLoss: 1.486572027206421\n",
      "Epoch: 3 \tBatch: 310 \tLoss: 1.4878721237182617\n",
      "Epoch: 3 \tBatch: 311 \tLoss: 1.5090718269348145\n",
      "Epoch: 3 \tBatch: 312 \tLoss: 1.4767354726791382\n",
      "Epoch: 3 \tBatch: 313 \tLoss: 1.5003947019577026\n",
      "Epoch: 3 \tBatch: 314 \tLoss: 1.4872252941131592\n",
      "Epoch: 3 \tBatch: 315 \tLoss: 1.4769525527954102\n",
      "Epoch: 3 \tBatch: 316 \tLoss: 1.4767721891403198\n",
      "Epoch: 3 \tBatch: 317 \tLoss: 1.4893772602081299\n",
      "Epoch: 3 \tBatch: 318 \tLoss: 1.4857338666915894\n",
      "Epoch: 3 \tBatch: 319 \tLoss: 1.4841428995132446\n",
      "Epoch: 3 \tBatch: 320 \tLoss: 1.4791427850723267\n",
      "Epoch: 3 \tBatch: 321 \tLoss: 1.5012799501419067\n",
      "Epoch: 3 \tBatch: 322 \tLoss: 1.5102684497833252\n",
      "Epoch: 3 \tBatch: 323 \tLoss: 1.4734631776809692\n",
      "Epoch: 3 \tBatch: 324 \tLoss: 1.4903072118759155\n",
      "Epoch: 3 \tBatch: 325 \tLoss: 1.4858938455581665\n",
      "Epoch: 3 \tBatch: 326 \tLoss: 1.509722352027893\n",
      "Epoch: 3 \tBatch: 327 \tLoss: 1.4794340133666992\n",
      "Epoch: 3 \tBatch: 328 \tLoss: 1.484952688217163\n",
      "Epoch: 3 \tBatch: 329 \tLoss: 1.4810539484024048\n",
      "Epoch: 3 \tBatch: 330 \tLoss: 1.485845923423767\n",
      "Epoch: 3 \tBatch: 331 \tLoss: 1.4989169836044312\n",
      "Epoch: 3 \tBatch: 332 \tLoss: 1.472520112991333\n",
      "Epoch: 3 \tBatch: 333 \tLoss: 1.4825034141540527\n",
      "Epoch: 3 \tBatch: 334 \tLoss: 1.5103514194488525\n",
      "Epoch: 3 \tBatch: 335 \tLoss: 1.492431402206421\n",
      "Epoch: 3 \tBatch: 336 \tLoss: 1.4839833974838257\n",
      "Epoch: 3 \tBatch: 337 \tLoss: 1.4859247207641602\n",
      "Epoch: 3 \tBatch: 338 \tLoss: 1.471086025238037\n",
      "Epoch: 3 \tBatch: 339 \tLoss: 1.4738091230392456\n",
      "Epoch: 3 \tBatch: 340 \tLoss: 1.478641390800476\n",
      "Epoch: 3 \tBatch: 341 \tLoss: 1.47376549243927\n",
      "Epoch: 3 \tBatch: 342 \tLoss: 1.4712170362472534\n",
      "Epoch: 3 \tBatch: 343 \tLoss: 1.4844125509262085\n",
      "Epoch: 3 \tBatch: 344 \tLoss: 1.476226568222046\n",
      "Epoch: 3 \tBatch: 345 \tLoss: 1.4827712774276733\n",
      "Epoch: 3 \tBatch: 346 \tLoss: 1.4820835590362549\n",
      "Epoch: 3 \tBatch: 347 \tLoss: 1.4762650728225708\n",
      "Epoch: 3 \tBatch: 348 \tLoss: 1.4889620542526245\n",
      "Epoch: 3 \tBatch: 349 \tLoss: 1.4681243896484375\n",
      "Epoch: 3 \tBatch: 350 \tLoss: 1.492138147354126\n",
      "Epoch: 3 \tBatch: 351 \tLoss: 1.4846552610397339\n",
      "Epoch: 3 \tBatch: 352 \tLoss: 1.481618881225586\n",
      "Epoch: 3 \tBatch: 353 \tLoss: 1.4693270921707153\n",
      "Epoch: 3 \tBatch: 354 \tLoss: 1.4631016254425049\n",
      "Epoch: 3 \tBatch: 355 \tLoss: 1.479418158531189\n",
      "Epoch: 3 \tBatch: 356 \tLoss: 1.4843273162841797\n",
      "Epoch: 3 \tBatch: 357 \tLoss: 1.480064868927002\n",
      "Epoch: 3 \tBatch: 358 \tLoss: 1.4798132181167603\n",
      "Epoch: 3 \tBatch: 359 \tLoss: 1.4857289791107178\n",
      "Epoch: 3 \tBatch: 360 \tLoss: 1.4964114427566528\n",
      "Epoch: 3 \tBatch: 361 \tLoss: 1.5156735181808472\n",
      "Epoch: 3 \tBatch: 362 \tLoss: 1.4683126211166382\n",
      "Epoch: 3 \tBatch: 363 \tLoss: 1.4842571020126343\n",
      "Epoch: 3 \tBatch: 364 \tLoss: 1.4657381772994995\n",
      "Epoch: 3 \tBatch: 365 \tLoss: 1.4863483905792236\n",
      "Epoch: 3 \tBatch: 366 \tLoss: 1.4821363687515259\n",
      "Epoch: 3 \tBatch: 367 \tLoss: 1.4810374975204468\n",
      "Epoch: 3 \tBatch: 368 \tLoss: 1.4775453805923462\n",
      "Epoch: 3 \tBatch: 369 \tLoss: 1.495545506477356\n",
      "Epoch: 3 \tBatch: 370 \tLoss: 1.478477120399475\n",
      "Epoch: 3 \tBatch: 371 \tLoss: 1.4829431772232056\n",
      "Epoch: 3 \tBatch: 372 \tLoss: 1.482727289199829\n",
      "Epoch: 3 \tBatch: 373 \tLoss: 1.4850362539291382\n",
      "Epoch: 3 \tBatch: 374 \tLoss: 1.4857972860336304\n",
      "Epoch: 3 \tBatch: 375 \tLoss: 1.480553150177002\n",
      "Epoch: 3 \tBatch: 376 \tLoss: 1.4668381214141846\n",
      "Epoch: 3 \tBatch: 377 \tLoss: 1.480264663696289\n",
      "Epoch: 3 \tBatch: 378 \tLoss: 1.4741181135177612\n",
      "Epoch: 3 \tBatch: 379 \tLoss: 1.4712406396865845\n",
      "Epoch: 3 \tBatch: 380 \tLoss: 1.4720319509506226\n",
      "Epoch: 3 \tBatch: 381 \tLoss: 1.5052409172058105\n",
      "Epoch: 3 \tBatch: 382 \tLoss: 1.5011532306671143\n",
      "Epoch: 3 \tBatch: 383 \tLoss: 1.4691059589385986\n",
      "Epoch: 3 \tBatch: 384 \tLoss: 1.4899179935455322\n",
      "Epoch: 3 \tBatch: 385 \tLoss: 1.4898133277893066\n",
      "Epoch: 3 \tBatch: 386 \tLoss: 1.478758454322815\n",
      "Epoch: 3 \tBatch: 387 \tLoss: 1.4693477153778076\n",
      "Epoch: 3 \tBatch: 388 \tLoss: 1.4639713764190674\n",
      "Epoch: 3 \tBatch: 389 \tLoss: 1.5190216302871704\n",
      "Epoch: 3 \tBatch: 390 \tLoss: 1.4794886112213135\n",
      "Epoch: 4 \tBatch: 0 \tLoss: 1.5036699771881104\n",
      "Epoch: 4 \tBatch: 1 \tLoss: 1.4877327680587769\n",
      "Epoch: 4 \tBatch: 2 \tLoss: 1.4751896858215332\n",
      "Epoch: 4 \tBatch: 3 \tLoss: 1.4772615432739258\n",
      "Epoch: 4 \tBatch: 4 \tLoss: 1.4893078804016113\n",
      "Epoch: 4 \tBatch: 5 \tLoss: 1.4795479774475098\n",
      "Epoch: 4 \tBatch: 6 \tLoss: 1.474176287651062\n",
      "Epoch: 4 \tBatch: 7 \tLoss: 1.4812442064285278\n",
      "Epoch: 4 \tBatch: 8 \tLoss: 1.4697048664093018\n",
      "Epoch: 4 \tBatch: 9 \tLoss: 1.478398323059082\n",
      "Epoch: 4 \tBatch: 10 \tLoss: 1.4985178709030151\n",
      "Epoch: 4 \tBatch: 11 \tLoss: 1.4811804294586182\n",
      "Epoch: 4 \tBatch: 12 \tLoss: 1.4720051288604736\n",
      "Epoch: 4 \tBatch: 13 \tLoss: 1.483238697052002\n",
      "Epoch: 4 \tBatch: 14 \tLoss: 1.4806543588638306\n",
      "Epoch: 4 \tBatch: 15 \tLoss: 1.4640201330184937\n",
      "Epoch: 4 \tBatch: 16 \tLoss: 1.4777780771255493\n",
      "Epoch: 4 \tBatch: 17 \tLoss: 1.4634920358657837\n",
      "Epoch: 4 \tBatch: 18 \tLoss: 1.4791735410690308\n",
      "Epoch: 4 \tBatch: 19 \tLoss: 1.4772472381591797\n",
      "Epoch: 4 \tBatch: 20 \tLoss: 1.4745850563049316\n",
      "Epoch: 4 \tBatch: 21 \tLoss: 1.4816974401474\n",
      "Epoch: 4 \tBatch: 22 \tLoss: 1.5048396587371826\n",
      "Epoch: 4 \tBatch: 23 \tLoss: 1.476952314376831\n",
      "Epoch: 4 \tBatch: 24 \tLoss: 1.463777780532837\n",
      "Epoch: 4 \tBatch: 25 \tLoss: 1.4738167524337769\n",
      "Epoch: 4 \tBatch: 26 \tLoss: 1.4782428741455078\n",
      "Epoch: 4 \tBatch: 27 \tLoss: 1.485341191291809\n",
      "Epoch: 4 \tBatch: 28 \tLoss: 1.4823113679885864\n",
      "Epoch: 4 \tBatch: 29 \tLoss: 1.4728083610534668\n",
      "Epoch: 4 \tBatch: 30 \tLoss: 1.4726881980895996\n",
      "Epoch: 4 \tBatch: 31 \tLoss: 1.4633631706237793\n",
      "Epoch: 4 \tBatch: 32 \tLoss: 1.469389796257019\n",
      "Epoch: 4 \tBatch: 33 \tLoss: 1.4790879487991333\n",
      "Epoch: 4 \tBatch: 34 \tLoss: 1.4783282279968262\n",
      "Epoch: 4 \tBatch: 35 \tLoss: 1.4956787824630737\n",
      "Epoch: 4 \tBatch: 36 \tLoss: 1.478195309638977\n",
      "Epoch: 4 \tBatch: 37 \tLoss: 1.4798163175582886\n",
      "Epoch: 4 \tBatch: 38 \tLoss: 1.4804601669311523\n",
      "Epoch: 4 \tBatch: 39 \tLoss: 1.4835318326950073\n",
      "Epoch: 4 \tBatch: 40 \tLoss: 1.461982011795044\n",
      "Epoch: 4 \tBatch: 41 \tLoss: 1.4699541330337524\n",
      "Epoch: 4 \tBatch: 42 \tLoss: 1.4699771404266357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 43 \tLoss: 1.489268183708191\n",
      "Epoch: 4 \tBatch: 44 \tLoss: 1.469541311264038\n",
      "Epoch: 4 \tBatch: 45 \tLoss: 1.5027605295181274\n",
      "Epoch: 4 \tBatch: 46 \tLoss: 1.4733315706253052\n",
      "Epoch: 4 \tBatch: 47 \tLoss: 1.4768829345703125\n",
      "Epoch: 4 \tBatch: 48 \tLoss: 1.4858673810958862\n",
      "Epoch: 4 \tBatch: 49 \tLoss: 1.4871010780334473\n",
      "Epoch: 4 \tBatch: 50 \tLoss: 1.4726492166519165\n",
      "Epoch: 4 \tBatch: 51 \tLoss: 1.5044803619384766\n",
      "Epoch: 4 \tBatch: 52 \tLoss: 1.470339298248291\n",
      "Epoch: 4 \tBatch: 53 \tLoss: 1.485892653465271\n",
      "Epoch: 4 \tBatch: 54 \tLoss: 1.4789611101150513\n",
      "Epoch: 4 \tBatch: 55 \tLoss: 1.4703569412231445\n",
      "Epoch: 4 \tBatch: 56 \tLoss: 1.4985142946243286\n",
      "Epoch: 4 \tBatch: 57 \tLoss: 1.4708284139633179\n",
      "Epoch: 4 \tBatch: 58 \tLoss: 1.4646592140197754\n",
      "Epoch: 4 \tBatch: 59 \tLoss: 1.4910576343536377\n",
      "Epoch: 4 \tBatch: 60 \tLoss: 1.4834039211273193\n",
      "Epoch: 4 \tBatch: 61 \tLoss: 1.4705003499984741\n",
      "Epoch: 4 \tBatch: 62 \tLoss: 1.5025408267974854\n",
      "Epoch: 4 \tBatch: 63 \tLoss: 1.4655919075012207\n",
      "Epoch: 4 \tBatch: 64 \tLoss: 1.4707136154174805\n",
      "Epoch: 4 \tBatch: 65 \tLoss: 1.4845362901687622\n",
      "Epoch: 4 \tBatch: 66 \tLoss: 1.4801011085510254\n",
      "Epoch: 4 \tBatch: 67 \tLoss: 1.4731312990188599\n",
      "Epoch: 4 \tBatch: 68 \tLoss: 1.474007487297058\n",
      "Epoch: 4 \tBatch: 69 \tLoss: 1.4777493476867676\n",
      "Epoch: 4 \tBatch: 70 \tLoss: 1.4669413566589355\n",
      "Epoch: 4 \tBatch: 71 \tLoss: 1.4691270589828491\n",
      "Epoch: 4 \tBatch: 72 \tLoss: 1.4831433296203613\n",
      "Epoch: 4 \tBatch: 73 \tLoss: 1.469427227973938\n",
      "Epoch: 4 \tBatch: 74 \tLoss: 1.4843127727508545\n",
      "Epoch: 4 \tBatch: 75 \tLoss: 1.4800467491149902\n",
      "Epoch: 4 \tBatch: 76 \tLoss: 1.4780309200286865\n",
      "Epoch: 4 \tBatch: 77 \tLoss: 1.4717917442321777\n",
      "Epoch: 4 \tBatch: 78 \tLoss: 1.4760756492614746\n",
      "Epoch: 4 \tBatch: 79 \tLoss: 1.4661380052566528\n",
      "Epoch: 4 \tBatch: 80 \tLoss: 1.4733744859695435\n",
      "Epoch: 4 \tBatch: 81 \tLoss: 1.4754408597946167\n",
      "Epoch: 4 \tBatch: 82 \tLoss: 1.4973784685134888\n",
      "Epoch: 4 \tBatch: 83 \tLoss: 1.47318434715271\n",
      "Epoch: 4 \tBatch: 84 \tLoss: 1.4639629125595093\n",
      "Epoch: 4 \tBatch: 85 \tLoss: 1.4716618061065674\n",
      "Epoch: 4 \tBatch: 86 \tLoss: 1.4942846298217773\n",
      "Epoch: 4 \tBatch: 87 \tLoss: 1.4695273637771606\n",
      "Epoch: 4 \tBatch: 88 \tLoss: 1.4776532649993896\n",
      "Epoch: 4 \tBatch: 89 \tLoss: 1.4849494695663452\n",
      "Epoch: 4 \tBatch: 90 \tLoss: 1.4782038927078247\n",
      "Epoch: 4 \tBatch: 91 \tLoss: 1.510460615158081\n",
      "Epoch: 4 \tBatch: 92 \tLoss: 1.476997971534729\n",
      "Epoch: 4 \tBatch: 93 \tLoss: 1.4834749698638916\n",
      "Epoch: 4 \tBatch: 94 \tLoss: 1.4828822612762451\n",
      "Epoch: 4 \tBatch: 95 \tLoss: 1.4823458194732666\n",
      "Epoch: 4 \tBatch: 96 \tLoss: 1.4879908561706543\n",
      "Epoch: 4 \tBatch: 97 \tLoss: 1.4913846254348755\n",
      "Epoch: 4 \tBatch: 98 \tLoss: 1.4939134120941162\n",
      "Epoch: 4 \tBatch: 99 \tLoss: 1.4715410470962524\n",
      "Epoch: 4 \tBatch: 100 \tLoss: 1.4877463579177856\n",
      "Epoch: 4 \tBatch: 101 \tLoss: 1.4795233011245728\n",
      "Epoch: 4 \tBatch: 102 \tLoss: 1.4977160692214966\n",
      "Epoch: 4 \tBatch: 103 \tLoss: 1.4708448648452759\n",
      "Epoch: 4 \tBatch: 104 \tLoss: 1.4700576066970825\n",
      "Epoch: 4 \tBatch: 105 \tLoss: 1.4704251289367676\n",
      "Epoch: 4 \tBatch: 106 \tLoss: 1.4678447246551514\n",
      "Epoch: 4 \tBatch: 107 \tLoss: 1.4866161346435547\n",
      "Epoch: 4 \tBatch: 108 \tLoss: 1.464819073677063\n",
      "Epoch: 4 \tBatch: 109 \tLoss: 1.4744116067886353\n",
      "Epoch: 4 \tBatch: 110 \tLoss: 1.4841678142547607\n",
      "Epoch: 4 \tBatch: 111 \tLoss: 1.4633135795593262\n",
      "Epoch: 4 \tBatch: 112 \tLoss: 1.4614217281341553\n",
      "Epoch: 4 \tBatch: 113 \tLoss: 1.4996989965438843\n",
      "Epoch: 4 \tBatch: 114 \tLoss: 1.4872283935546875\n",
      "Epoch: 4 \tBatch: 115 \tLoss: 1.4939630031585693\n",
      "Epoch: 4 \tBatch: 116 \tLoss: 1.4823271036148071\n",
      "Epoch: 4 \tBatch: 117 \tLoss: 1.4926029443740845\n",
      "Epoch: 4 \tBatch: 118 \tLoss: 1.4922524690628052\n",
      "Epoch: 4 \tBatch: 119 \tLoss: 1.488150715827942\n",
      "Epoch: 4 \tBatch: 120 \tLoss: 1.4644960165023804\n",
      "Epoch: 4 \tBatch: 121 \tLoss: 1.4919418096542358\n",
      "Epoch: 4 \tBatch: 122 \tLoss: 1.4684574604034424\n",
      "Epoch: 4 \tBatch: 123 \tLoss: 1.514418125152588\n",
      "Epoch: 4 \tBatch: 124 \tLoss: 1.486038088798523\n",
      "Epoch: 4 \tBatch: 125 \tLoss: 1.4657721519470215\n",
      "Epoch: 4 \tBatch: 126 \tLoss: 1.4812849760055542\n",
      "Epoch: 4 \tBatch: 127 \tLoss: 1.4703006744384766\n",
      "Epoch: 4 \tBatch: 128 \tLoss: 1.4904592037200928\n",
      "Epoch: 4 \tBatch: 129 \tLoss: 1.4817782640457153\n",
      "Epoch: 4 \tBatch: 130 \tLoss: 1.4823744297027588\n",
      "Epoch: 4 \tBatch: 131 \tLoss: 1.472923755645752\n",
      "Epoch: 4 \tBatch: 132 \tLoss: 1.4935665130615234\n",
      "Epoch: 4 \tBatch: 133 \tLoss: 1.4692462682724\n",
      "Epoch: 4 \tBatch: 134 \tLoss: 1.4700210094451904\n",
      "Epoch: 4 \tBatch: 135 \tLoss: 1.4752596616744995\n",
      "Epoch: 4 \tBatch: 136 \tLoss: 1.4761662483215332\n",
      "Epoch: 4 \tBatch: 137 \tLoss: 1.4924007654190063\n",
      "Epoch: 4 \tBatch: 138 \tLoss: 1.4773269891738892\n",
      "Epoch: 4 \tBatch: 139 \tLoss: 1.475243330001831\n",
      "Epoch: 4 \tBatch: 140 \tLoss: 1.4798792600631714\n",
      "Epoch: 4 \tBatch: 141 \tLoss: 1.4899802207946777\n",
      "Epoch: 4 \tBatch: 142 \tLoss: 1.468156099319458\n",
      "Epoch: 4 \tBatch: 143 \tLoss: 1.471668004989624\n",
      "Epoch: 4 \tBatch: 144 \tLoss: 1.4927467107772827\n",
      "Epoch: 4 \tBatch: 145 \tLoss: 1.495660424232483\n",
      "Epoch: 4 \tBatch: 146 \tLoss: 1.484605312347412\n",
      "Epoch: 4 \tBatch: 147 \tLoss: 1.4817962646484375\n",
      "Epoch: 4 \tBatch: 148 \tLoss: 1.472036600112915\n",
      "Epoch: 4 \tBatch: 149 \tLoss: 1.4708514213562012\n",
      "Epoch: 4 \tBatch: 150 \tLoss: 1.4744715690612793\n",
      "Epoch: 4 \tBatch: 151 \tLoss: 1.4909154176712036\n",
      "Epoch: 4 \tBatch: 152 \tLoss: 1.4697308540344238\n",
      "Epoch: 4 \tBatch: 153 \tLoss: 1.4909212589263916\n",
      "Epoch: 4 \tBatch: 154 \tLoss: 1.470121145248413\n",
      "Epoch: 4 \tBatch: 155 \tLoss: 1.4897606372833252\n",
      "Epoch: 4 \tBatch: 156 \tLoss: 1.4669239521026611\n",
      "Epoch: 4 \tBatch: 157 \tLoss: 1.4769132137298584\n",
      "Epoch: 4 \tBatch: 158 \tLoss: 1.484531283378601\n",
      "Epoch: 4 \tBatch: 159 \tLoss: 1.4838682413101196\n",
      "Epoch: 4 \tBatch: 160 \tLoss: 1.4815549850463867\n",
      "Epoch: 4 \tBatch: 161 \tLoss: 1.4723002910614014\n",
      "Epoch: 4 \tBatch: 162 \tLoss: 1.4714313745498657\n",
      "Epoch: 4 \tBatch: 163 \tLoss: 1.471953272819519\n",
      "Epoch: 4 \tBatch: 164 \tLoss: 1.4830516576766968\n",
      "Epoch: 4 \tBatch: 165 \tLoss: 1.4781639575958252\n",
      "Epoch: 4 \tBatch: 166 \tLoss: 1.4884753227233887\n",
      "Epoch: 4 \tBatch: 167 \tLoss: 1.4769198894500732\n",
      "Epoch: 4 \tBatch: 168 \tLoss: 1.4723365306854248\n",
      "Epoch: 4 \tBatch: 169 \tLoss: 1.4724911451339722\n",
      "Epoch: 4 \tBatch: 170 \tLoss: 1.476117491722107\n",
      "Epoch: 4 \tBatch: 171 \tLoss: 1.4892246723175049\n",
      "Epoch: 4 \tBatch: 172 \tLoss: 1.4768410921096802\n",
      "Epoch: 4 \tBatch: 173 \tLoss: 1.4945838451385498\n",
      "Epoch: 4 \tBatch: 174 \tLoss: 1.505211591720581\n",
      "Epoch: 4 \tBatch: 175 \tLoss: 1.478016972541809\n",
      "Epoch: 4 \tBatch: 176 \tLoss: 1.4722909927368164\n",
      "Epoch: 4 \tBatch: 177 \tLoss: 1.4729820489883423\n",
      "Epoch: 4 \tBatch: 178 \tLoss: 1.5098423957824707\n",
      "Epoch: 4 \tBatch: 179 \tLoss: 1.4831676483154297\n",
      "Epoch: 4 \tBatch: 180 \tLoss: 1.47787606716156\n",
      "Epoch: 4 \tBatch: 181 \tLoss: 1.4909340143203735\n",
      "Epoch: 4 \tBatch: 182 \tLoss: 1.4756238460540771\n",
      "Epoch: 4 \tBatch: 183 \tLoss: 1.4774224758148193\n",
      "Epoch: 4 \tBatch: 184 \tLoss: 1.485016107559204\n",
      "Epoch: 4 \tBatch: 185 \tLoss: 1.4639489650726318\n",
      "Epoch: 4 \tBatch: 186 \tLoss: 1.4756464958190918\n",
      "Epoch: 4 \tBatch: 187 \tLoss: 1.4723787307739258\n",
      "Epoch: 4 \tBatch: 188 \tLoss: 1.4771980047225952\n",
      "Epoch: 4 \tBatch: 189 \tLoss: 1.4933419227600098\n",
      "Epoch: 4 \tBatch: 190 \tLoss: 1.4739645719528198\n",
      "Epoch: 4 \tBatch: 191 \tLoss: 1.4773048162460327\n",
      "Epoch: 4 \tBatch: 192 \tLoss: 1.4787495136260986\n",
      "Epoch: 4 \tBatch: 193 \tLoss: 1.4848428964614868\n",
      "Epoch: 4 \tBatch: 194 \tLoss: 1.4907042980194092\n",
      "Epoch: 4 \tBatch: 195 \tLoss: 1.4735970497131348\n",
      "Epoch: 4 \tBatch: 196 \tLoss: 1.4870613813400269\n",
      "Epoch: 4 \tBatch: 197 \tLoss: 1.475737452507019\n",
      "Epoch: 4 \tBatch: 198 \tLoss: 1.479656457901001\n",
      "Epoch: 4 \tBatch: 199 \tLoss: 1.4881176948547363\n",
      "Epoch: 4 \tBatch: 200 \tLoss: 1.5076274871826172\n",
      "Epoch: 4 \tBatch: 201 \tLoss: 1.486439824104309\n",
      "Epoch: 4 \tBatch: 202 \tLoss: 1.4769723415374756\n",
      "Epoch: 4 \tBatch: 203 \tLoss: 1.4833961725234985\n",
      "Epoch: 4 \tBatch: 204 \tLoss: 1.4656264781951904\n",
      "Epoch: 4 \tBatch: 205 \tLoss: 1.4719386100769043\n",
      "Epoch: 4 \tBatch: 206 \tLoss: 1.4740991592407227\n",
      "Epoch: 4 \tBatch: 207 \tLoss: 1.5082205533981323\n",
      "Epoch: 4 \tBatch: 208 \tLoss: 1.4659945964813232\n",
      "Epoch: 4 \tBatch: 209 \tLoss: 1.5009993314743042\n",
      "Epoch: 4 \tBatch: 210 \tLoss: 1.498319149017334\n",
      "Epoch: 4 \tBatch: 211 \tLoss: 1.4642382860183716\n",
      "Epoch: 4 \tBatch: 212 \tLoss: 1.4771559238433838\n",
      "Epoch: 4 \tBatch: 213 \tLoss: 1.4965691566467285\n",
      "Epoch: 4 \tBatch: 214 \tLoss: 1.4898282289505005\n",
      "Epoch: 4 \tBatch: 215 \tLoss: 1.4722555875778198\n",
      "Epoch: 4 \tBatch: 216 \tLoss: 1.4618558883666992\n",
      "Epoch: 4 \tBatch: 217 \tLoss: 1.4628671407699585\n",
      "Epoch: 4 \tBatch: 218 \tLoss: 1.467961072921753\n",
      "Epoch: 4 \tBatch: 219 \tLoss: 1.477465271949768\n",
      "Epoch: 4 \tBatch: 220 \tLoss: 1.4821529388427734\n",
      "Epoch: 4 \tBatch: 221 \tLoss: 1.4877610206604004\n",
      "Epoch: 4 \tBatch: 222 \tLoss: 1.4699395895004272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 223 \tLoss: 1.4850571155548096\n",
      "Epoch: 4 \tBatch: 224 \tLoss: 1.482789158821106\n",
      "Epoch: 4 \tBatch: 225 \tLoss: 1.4612743854522705\n",
      "Epoch: 4 \tBatch: 226 \tLoss: 1.4906188249588013\n",
      "Epoch: 4 \tBatch: 227 \tLoss: 1.4777623414993286\n",
      "Epoch: 4 \tBatch: 228 \tLoss: 1.4770538806915283\n",
      "Epoch: 4 \tBatch: 229 \tLoss: 1.5055549144744873\n",
      "Epoch: 4 \tBatch: 230 \tLoss: 1.4696582555770874\n",
      "Epoch: 4 \tBatch: 231 \tLoss: 1.4896230697631836\n",
      "Epoch: 4 \tBatch: 232 \tLoss: 1.4625054597854614\n",
      "Epoch: 4 \tBatch: 233 \tLoss: 1.4712793827056885\n",
      "Epoch: 4 \tBatch: 234 \tLoss: 1.4902493953704834\n",
      "Epoch: 4 \tBatch: 235 \tLoss: 1.4692727327346802\n",
      "Epoch: 4 \tBatch: 236 \tLoss: 1.4816917181015015\n",
      "Epoch: 4 \tBatch: 237 \tLoss: 1.4756529331207275\n",
      "Epoch: 4 \tBatch: 238 \tLoss: 1.4891045093536377\n",
      "Epoch: 4 \tBatch: 239 \tLoss: 1.4856232404708862\n",
      "Epoch: 4 \tBatch: 240 \tLoss: 1.4884443283081055\n",
      "Epoch: 4 \tBatch: 241 \tLoss: 1.4787620306015015\n",
      "Epoch: 4 \tBatch: 242 \tLoss: 1.482285976409912\n",
      "Epoch: 4 \tBatch: 243 \tLoss: 1.4628177881240845\n",
      "Epoch: 4 \tBatch: 244 \tLoss: 1.4851583242416382\n",
      "Epoch: 4 \tBatch: 245 \tLoss: 1.4673742055892944\n",
      "Epoch: 4 \tBatch: 246 \tLoss: 1.4645744562149048\n",
      "Epoch: 4 \tBatch: 247 \tLoss: 1.4967753887176514\n",
      "Epoch: 4 \tBatch: 248 \tLoss: 1.474198818206787\n",
      "Epoch: 4 \tBatch: 249 \tLoss: 1.4714574813842773\n",
      "Epoch: 4 \tBatch: 250 \tLoss: 1.481209635734558\n",
      "Epoch: 4 \tBatch: 251 \tLoss: 1.4837133884429932\n",
      "Epoch: 4 \tBatch: 252 \tLoss: 1.4738801717758179\n",
      "Epoch: 4 \tBatch: 253 \tLoss: 1.4758046865463257\n",
      "Epoch: 4 \tBatch: 254 \tLoss: 1.500319480895996\n",
      "Epoch: 4 \tBatch: 255 \tLoss: 1.4713815450668335\n",
      "Epoch: 4 \tBatch: 256 \tLoss: 1.4752877950668335\n",
      "Epoch: 4 \tBatch: 257 \tLoss: 1.4688471555709839\n",
      "Epoch: 4 \tBatch: 258 \tLoss: 1.4900237321853638\n",
      "Epoch: 4 \tBatch: 259 \tLoss: 1.4633586406707764\n",
      "Epoch: 4 \tBatch: 260 \tLoss: 1.4690085649490356\n",
      "Epoch: 4 \tBatch: 261 \tLoss: 1.4850183725357056\n",
      "Epoch: 4 \tBatch: 262 \tLoss: 1.4862937927246094\n",
      "Epoch: 4 \tBatch: 263 \tLoss: 1.4779752492904663\n",
      "Epoch: 4 \tBatch: 264 \tLoss: 1.4641084671020508\n",
      "Epoch: 4 \tBatch: 265 \tLoss: 1.5064527988433838\n",
      "Epoch: 4 \tBatch: 266 \tLoss: 1.4722275733947754\n",
      "Epoch: 4 \tBatch: 267 \tLoss: 1.484775424003601\n",
      "Epoch: 4 \tBatch: 268 \tLoss: 1.482974886894226\n",
      "Epoch: 4 \tBatch: 269 \tLoss: 1.4771860837936401\n",
      "Epoch: 4 \tBatch: 270 \tLoss: 1.4689207077026367\n",
      "Epoch: 4 \tBatch: 271 \tLoss: 1.4837284088134766\n",
      "Epoch: 4 \tBatch: 272 \tLoss: 1.4904513359069824\n",
      "Epoch: 4 \tBatch: 273 \tLoss: 1.4650635719299316\n",
      "Epoch: 4 \tBatch: 274 \tLoss: 1.4759315252304077\n",
      "Epoch: 4 \tBatch: 275 \tLoss: 1.470241904258728\n",
      "Epoch: 4 \tBatch: 276 \tLoss: 1.4771690368652344\n",
      "Epoch: 4 \tBatch: 277 \tLoss: 1.4622882604599\n",
      "Epoch: 4 \tBatch: 278 \tLoss: 1.4723817110061646\n",
      "Epoch: 4 \tBatch: 279 \tLoss: 1.482118844985962\n",
      "Epoch: 4 \tBatch: 280 \tLoss: 1.4902215003967285\n",
      "Epoch: 4 \tBatch: 281 \tLoss: 1.4687143564224243\n",
      "Epoch: 4 \tBatch: 282 \tLoss: 1.4960838556289673\n",
      "Epoch: 4 \tBatch: 283 \tLoss: 1.4870953559875488\n",
      "Epoch: 4 \tBatch: 284 \tLoss: 1.4894908666610718\n",
      "Epoch: 4 \tBatch: 285 \tLoss: 1.4731242656707764\n",
      "Epoch: 4 \tBatch: 286 \tLoss: 1.4727898836135864\n",
      "Epoch: 4 \tBatch: 287 \tLoss: 1.4763610363006592\n",
      "Epoch: 4 \tBatch: 288 \tLoss: 1.478316307067871\n",
      "Epoch: 4 \tBatch: 289 \tLoss: 1.4731535911560059\n",
      "Epoch: 4 \tBatch: 290 \tLoss: 1.483414649963379\n",
      "Epoch: 4 \tBatch: 291 \tLoss: 1.4642916917800903\n",
      "Epoch: 4 \tBatch: 292 \tLoss: 1.4766758680343628\n",
      "Epoch: 4 \tBatch: 293 \tLoss: 1.4728901386260986\n",
      "Epoch: 4 \tBatch: 294 \tLoss: 1.4930232763290405\n",
      "Epoch: 4 \tBatch: 295 \tLoss: 1.4916019439697266\n",
      "Epoch: 4 \tBatch: 296 \tLoss: 1.4693340063095093\n",
      "Epoch: 4 \tBatch: 297 \tLoss: 1.4825868606567383\n",
      "Epoch: 4 \tBatch: 298 \tLoss: 1.482237696647644\n",
      "Epoch: 4 \tBatch: 299 \tLoss: 1.477052092552185\n",
      "Epoch: 4 \tBatch: 300 \tLoss: 1.4753788709640503\n",
      "Epoch: 4 \tBatch: 301 \tLoss: 1.4890882968902588\n",
      "Epoch: 4 \tBatch: 302 \tLoss: 1.4986357688903809\n",
      "Epoch: 4 \tBatch: 303 \tLoss: 1.49106764793396\n",
      "Epoch: 4 \tBatch: 304 \tLoss: 1.487480878829956\n",
      "Epoch: 4 \tBatch: 305 \tLoss: 1.4817619323730469\n",
      "Epoch: 4 \tBatch: 306 \tLoss: 1.4655377864837646\n",
      "Epoch: 4 \tBatch: 307 \tLoss: 1.479328989982605\n",
      "Epoch: 4 \tBatch: 308 \tLoss: 1.4818202257156372\n",
      "Epoch: 4 \tBatch: 309 \tLoss: 1.4933851957321167\n",
      "Epoch: 4 \tBatch: 310 \tLoss: 1.476123571395874\n",
      "Epoch: 4 \tBatch: 311 \tLoss: 1.4788836240768433\n",
      "Epoch: 4 \tBatch: 312 \tLoss: 1.4801275730133057\n",
      "Epoch: 4 \tBatch: 313 \tLoss: 1.4919980764389038\n",
      "Epoch: 4 \tBatch: 314 \tLoss: 1.4654483795166016\n",
      "Epoch: 4 \tBatch: 315 \tLoss: 1.4689197540283203\n",
      "Epoch: 4 \tBatch: 316 \tLoss: 1.4774316549301147\n",
      "Epoch: 4 \tBatch: 317 \tLoss: 1.4963812828063965\n",
      "Epoch: 4 \tBatch: 318 \tLoss: 1.4663594961166382\n",
      "Epoch: 4 \tBatch: 319 \tLoss: 1.4757335186004639\n",
      "Epoch: 4 \tBatch: 320 \tLoss: 1.4697211980819702\n",
      "Epoch: 4 \tBatch: 321 \tLoss: 1.4803088903427124\n",
      "Epoch: 4 \tBatch: 322 \tLoss: 1.4689500331878662\n",
      "Epoch: 4 \tBatch: 323 \tLoss: 1.4732309579849243\n",
      "Epoch: 4 \tBatch: 324 \tLoss: 1.4777560234069824\n",
      "Epoch: 4 \tBatch: 325 \tLoss: 1.4769469499588013\n",
      "Epoch: 4 \tBatch: 326 \tLoss: 1.4757503271102905\n",
      "Epoch: 4 \tBatch: 327 \tLoss: 1.4771480560302734\n",
      "Epoch: 4 \tBatch: 328 \tLoss: 1.476908564567566\n",
      "Epoch: 4 \tBatch: 329 \tLoss: 1.4764267206192017\n",
      "Epoch: 4 \tBatch: 330 \tLoss: 1.507941484451294\n",
      "Epoch: 4 \tBatch: 331 \tLoss: 1.4734585285186768\n",
      "Epoch: 4 \tBatch: 332 \tLoss: 1.4637880325317383\n",
      "Epoch: 4 \tBatch: 333 \tLoss: 1.4807782173156738\n",
      "Epoch: 4 \tBatch: 334 \tLoss: 1.483921766281128\n",
      "Epoch: 4 \tBatch: 335 \tLoss: 1.4800872802734375\n",
      "Epoch: 4 \tBatch: 336 \tLoss: 1.4823776483535767\n",
      "Epoch: 4 \tBatch: 337 \tLoss: 1.4746735095977783\n",
      "Epoch: 4 \tBatch: 338 \tLoss: 1.4695831537246704\n",
      "Epoch: 4 \tBatch: 339 \tLoss: 1.4696511030197144\n",
      "Epoch: 4 \tBatch: 340 \tLoss: 1.4815442562103271\n",
      "Epoch: 4 \tBatch: 341 \tLoss: 1.4832990169525146\n",
      "Epoch: 4 \tBatch: 342 \tLoss: 1.489642858505249\n",
      "Epoch: 4 \tBatch: 343 \tLoss: 1.4693955183029175\n",
      "Epoch: 4 \tBatch: 344 \tLoss: 1.5037471055984497\n",
      "Epoch: 4 \tBatch: 345 \tLoss: 1.4965453147888184\n",
      "Epoch: 4 \tBatch: 346 \tLoss: 1.484690546989441\n",
      "Epoch: 4 \tBatch: 347 \tLoss: 1.471728801727295\n",
      "Epoch: 4 \tBatch: 348 \tLoss: 1.4635388851165771\n",
      "Epoch: 4 \tBatch: 349 \tLoss: 1.4797967672348022\n",
      "Epoch: 4 \tBatch: 350 \tLoss: 1.4649349451065063\n",
      "Epoch: 4 \tBatch: 351 \tLoss: 1.471441388130188\n",
      "Epoch: 4 \tBatch: 352 \tLoss: 1.4727612733840942\n",
      "Epoch: 4 \tBatch: 353 \tLoss: 1.4735437631607056\n",
      "Epoch: 4 \tBatch: 354 \tLoss: 1.4959869384765625\n",
      "Epoch: 4 \tBatch: 355 \tLoss: 1.4638683795928955\n",
      "Epoch: 4 \tBatch: 356 \tLoss: 1.5022834539413452\n",
      "Epoch: 4 \tBatch: 357 \tLoss: 1.4765112400054932\n",
      "Epoch: 4 \tBatch: 358 \tLoss: 1.4698325395584106\n",
      "Epoch: 4 \tBatch: 359 \tLoss: 1.476234793663025\n",
      "Epoch: 4 \tBatch: 360 \tLoss: 1.4692294597625732\n",
      "Epoch: 4 \tBatch: 361 \tLoss: 1.466195821762085\n",
      "Epoch: 4 \tBatch: 362 \tLoss: 1.4886518716812134\n",
      "Epoch: 4 \tBatch: 363 \tLoss: 1.48941171169281\n",
      "Epoch: 4 \tBatch: 364 \tLoss: 1.4644278287887573\n",
      "Epoch: 4 \tBatch: 365 \tLoss: 1.490970492362976\n",
      "Epoch: 4 \tBatch: 366 \tLoss: 1.464337944984436\n",
      "Epoch: 4 \tBatch: 367 \tLoss: 1.4751886129379272\n",
      "Epoch: 4 \tBatch: 368 \tLoss: 1.4988398551940918\n",
      "Epoch: 4 \tBatch: 369 \tLoss: 1.4891324043273926\n",
      "Epoch: 4 \tBatch: 370 \tLoss: 1.4897438287734985\n",
      "Epoch: 4 \tBatch: 371 \tLoss: 1.4669392108917236\n",
      "Epoch: 4 \tBatch: 372 \tLoss: 1.4676411151885986\n",
      "Epoch: 4 \tBatch: 373 \tLoss: 1.4810858964920044\n",
      "Epoch: 4 \tBatch: 374 \tLoss: 1.503762125968933\n",
      "Epoch: 4 \tBatch: 375 \tLoss: 1.5094600915908813\n",
      "Epoch: 4 \tBatch: 376 \tLoss: 1.4771428108215332\n",
      "Epoch: 4 \tBatch: 377 \tLoss: 1.4769372940063477\n",
      "Epoch: 4 \tBatch: 378 \tLoss: 1.4786622524261475\n",
      "Epoch: 4 \tBatch: 379 \tLoss: 1.4752873182296753\n",
      "Epoch: 4 \tBatch: 380 \tLoss: 1.4785945415496826\n",
      "Epoch: 4 \tBatch: 381 \tLoss: 1.4878219366073608\n",
      "Epoch: 4 \tBatch: 382 \tLoss: 1.5002249479293823\n",
      "Epoch: 4 \tBatch: 383 \tLoss: 1.4731831550598145\n",
      "Epoch: 4 \tBatch: 384 \tLoss: 1.4936208724975586\n",
      "Epoch: 4 \tBatch: 385 \tLoss: 1.476269245147705\n",
      "Epoch: 4 \tBatch: 386 \tLoss: 1.4661344289779663\n",
      "Epoch: 4 \tBatch: 387 \tLoss: 1.4668633937835693\n",
      "Epoch: 4 \tBatch: 388 \tLoss: 1.475264549255371\n",
      "Epoch: 4 \tBatch: 389 \tLoss: 1.476083517074585\n",
      "Epoch: 4 \tBatch: 390 \tLoss: 1.4615304470062256\n",
      "Training Complete. Final loss = 1.4615304470062256\n"
     ]
    }
   ],
   "source": [
    "train(cnn, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def calc_accuracy(model, dataloader):\n",
    "    num_correct = 0\n",
    "    num_examples = len(dataloader.dataset)                       # test DATA not test LOADER\n",
    "    for inputs, labels in dataloader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        predictions = model(inputs)\n",
    "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
    "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 98.574\n",
      "Validation Accuracy: 98.11999999999999\n",
      "Test Accuracy: 98.24000000000001\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy:', calc_accuracy(cnn, train_loader))\n",
    "print('Validation Accuracy:', calc_accuracy(cnn, val_loader))\n",
    "print('Test Accuracy:', calc_accuracy(cnn, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Benchmarking of Fully Connected vs Convolutional Architecture on MNIST\n",
    "\n",
    "Lets empirically test the translation invariance of the two architectures. \n",
    "\n",
    "The training set now has a center crop transformation which crops the central pixels of the image to a given size. However, the test set has a random crop transformation which crops a random region of the image to a given size.\n",
    "\n",
    "So we are training the neural networks with a centrally cropped image but testing it on cropped images with translations applied.\n",
    "\n",
    "The network that has more translation invariant features should performs better on this test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "crop_size = 22\n",
    "\n",
    "traintransforms = []\n",
    "traintransforms.append(transforms.CenterCrop(crop_size))\n",
    "traintransforms.append(transforms.ToTensor())\n",
    "traintransforms = transforms.Compose(traintransforms)\n",
    "\n",
    "# GET THE TRAINING DATASET\n",
    "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
    "                            transform=traintransforms,          # transform the data from a PIL image to a tensor\n",
    "                            train=True,                               # is this training data?\n",
    "                            download=True                             # should i download it if it's not already here?\n",
    "                           )\n",
    "\n",
    "testtransforms = []\n",
    "testtransforms.append(transforms.RandomCrop(crop_size))\n",
    "testtransforms.append(transforms.ToTensor())\n",
    "testtransforms = transforms.Compose(testtransforms)\n",
    "\n",
    "# GET THE TEST DATASET\n",
    "test_data = datasets.MNIST(root='MNIST-data',\n",
    "                           transform=testtransforms,\n",
    "                           train=False,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Example\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Example\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# PRINT THEIR LENGTHS AND VISUALISE AN EXAMPLE\n",
    "x = train_data[np.random.randint(0, len(train_data))][0]    # get a random training example\n",
    "print('Train Example')\n",
    "plt.imshow(x[0].numpy())\n",
    "plt.show()\n",
    "x = test_data[np.random.randint(0, len(test_data))][0]    # get a random test example\n",
    "print('Test Example')\n",
    "plt.imshow(x[0].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# MAKE TRAINING DATALOADER\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE VALIDATION DATALOADER\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE TEST DATALOADER\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the architectures of our fully connected and convolutional networks as well as a function which returns the number of parameters in each model. Since the number of parameters in a model is a rough measure of its capacity, the networks should have an approximately equal number of parameters to make it a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "class FullyConnectedNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()    # initialise parent module\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(crop_size*crop_size, 225),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(225, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(50, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, crop_size*crop_size)\n",
    "        x = self.fc_layers(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = torch.nn.Sequential( # put your convolutional architecture here using torch.nn.Sequential \n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64*(crop_size-12)*(crop_size-12), 10) # put your convolutional architecture here using torch.nn.Sequential \n",
    "            \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)# pass through conv layers\n",
    "        x = x.view(x.shape[0], -1)# flatten output ready for fully connected layer\n",
    "        x = self.fc_layers(x)# pass through fully connected layer\n",
    "        x = F.softmax(x, dim=1)# softmax activation function on outputs\n",
    "        return x\n",
    "\n",
    "def get_n_params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005 # set learning rate\n",
    "epochs = 5 # set number of epochs\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in cnn: 128522\n",
      "Training Complete. Final loss = 1.4745460748672485\n"
     ]
    }
   ],
   "source": [
    "cnn = ConvNet().to(device)#instantiate model\n",
    "print('Number of parameters in cnn:', get_n_params(cnn))\n",
    "criterion = torch.nn.CrossEntropyLoss() #use cross entropy loss function\n",
    "optimiser = torch.optim.Adam(cnn.parameters(), lr=learning_rate) # use Adam optimizer, passing it the parameters of your model and the learning rate\n",
    "train(cnn, epochs, verbose=False, tag='CNN Loss/Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in fnn: 137285\n",
      "Training Complete. Final loss = 1.5748454332351685\n"
     ]
    }
   ],
   "source": [
    "fnn = FullyConnectedNet().to(device)\n",
    "print('Number of parameters in fnn:', get_n_params(fnn))\n",
    "optimiser = torch.optim.Adam(fnn.parameters(), lr=learning_rate) # use Adam optimizer, passing it the parameters of your model and the learning rate\n",
    "train(fnn, epochs, verbose=False, tag='FNN Loss/Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Train Accuracy: 97.932\n",
      "CNN Validation Accuracy: 97.50999999999999\n",
      "CNN Test Accuracy: 71.98 \n",
      "\n",
      "FNN Train Accuracy: 87.044\n",
      "FNN Validation Accuracy: 86.3\n",
      "FNN Test Accuracy: 46.87\n"
     ]
    }
   ],
   "source": [
    "print('CNN Train Accuracy:', calc_accuracy(cnn, train_loader))\n",
    "print('CNN Validation Accuracy:', calc_accuracy(cnn, val_loader))\n",
    "print('CNN Test Accuracy:', calc_accuracy(cnn, test_loader), '\\n')\n",
    "\n",
    "print('FNN Train Accuracy:', calc_accuracy(fnn, train_loader))\n",
    "print('FNN Validation Accuracy:', calc_accuracy(fnn, val_loader))\n",
    "print('FNN Test Accuracy:', calc_accuracy(fnn, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, there is a significant disparity between the test accuracy of the two architectures, with the CNN have ~+20% test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrambling test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

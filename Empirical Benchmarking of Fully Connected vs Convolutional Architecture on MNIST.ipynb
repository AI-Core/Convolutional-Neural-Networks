{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Benchmarking of Fully Connected vs Convolutional Architecture on MNIST\n",
    "\n",
    "Lets empirically test the translation invariance of the two architectures. \n",
    "\n",
    "The training set now has a center crop transformation which crops the central pixels of the image to a given size. However, the test set has a random crop transformation which crops a random region of the image to a given size.\n",
    "\n",
    "So we are training the neural networks with a centrally cropped image but testing it on cropped images with translations applied.\n",
    "\n",
    "The network that has more translation invariant features should performs better on this test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "crop_size = 22\n",
    "\n",
    "traintransforms = []\n",
    "traintransforms.append(transforms.CenterCrop(crop_size))\n",
    "traintransforms.append(transforms.ToTensor())\n",
    "traintransforms = transforms.Compose(traintransforms)\n",
    "\n",
    "# GET THE TRAINING DATASET\n",
    "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
    "                            transform=traintransforms,          # transform the data from a PIL image to a tensor\n",
    "                            train=True,                               # is this training data?\n",
    "                            download=True                             # should i download it if it's not already here?\n",
    "                           )\n",
    "\n",
    "testtransforms = []\n",
    "testtransforms.append(transforms.RandomCrop(crop_size))\n",
    "testtransforms.append(transforms.ToTensor())\n",
    "testtransforms = transforms.Compose(testtransforms)\n",
    "\n",
    "# GET THE TEST DATASET\n",
    "test_data = datasets.MNIST(root='MNIST-data',\n",
    "                           transform=testtransforms,\n",
    "                           train=False,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Example\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Example\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# PRINT THEIR LENGTHS AND VISUALISE AN EXAMPLE\n",
    "x = train_data[np.random.randint(0, len(train_data))][0]    # get a random training example\n",
    "print('Train Example')\n",
    "plt.imshow(x[0].numpy())\n",
    "plt.show()\n",
    "x = test_data[np.random.randint(0, len(test_data))][0]    # get a random test example\n",
    "print('Test Example')\n",
    "plt.imshow(x[0].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# MAKE TRAINING DATALOADER\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE VALIDATION DATALOADER\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE TEST DATALOADER\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the architectures of our fully connected and convolutional networks as well as a function which returns the number of parameters in each model. Since the number of parameters in a model is a rough measure of its capacity, the networks should have an approximately equal number of parameters to make it a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "class FullyConnectedNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()    # initialise parent module\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(crop_size*crop_size, 225),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(225, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(50, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, crop_size*crop_size)\n",
    "        x = self.fc_layers(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = torch.nn.Sequential( # put your convolutional architecture here using torch.nn.Sequential \n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64*(crop_size-12)*(crop_size-12), 10) # put your convolutional architecture here using torch.nn.Sequential \n",
    "            \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)# pass through conv layers\n",
    "        x = x.view(x.shape[0], -1)# flatten output ready for fully connected layer\n",
    "        x = self.fc_layers(x)# pass through fully connected layer\n",
    "        x = F.softmax(x, dim=1)# softmax activation function on outputs\n",
    "        return x\n",
    "\n",
    "def get_n_params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0005 # set learning rate\n",
    "epochs = 5 # set number of epochs\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in cnn: 128522\n",
      "Training Complete. Final loss = 1.4613263607025146\n"
     ]
    }
   ],
   "source": [
    "def train(model, epochs, verbose=True, tag='Loss/Train'):\n",
    "    for epoch in range(epochs):\n",
    "        for idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # pass x through your model to get a prediction\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the cost\n",
    "            if verbose: print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss.item())\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of all of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and store all of the model's param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            \n",
    "            writer.add_scalar(tag, loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "    print('Training Complete. Final loss =',loss.item())\n",
    "    \n",
    "\n",
    "use_cuda = torch.cuda.is_available() # checks if gpu is available\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "cnn = ConvNet().to(device)#instantiate model\n",
    "print('Number of parameters in cnn:', get_n_params(cnn))\n",
    "criterion = torch.nn.CrossEntropyLoss() #use cross entropy loss function\n",
    "optimiser = torch.optim.Adam(cnn.parameters(), lr=learning_rate) # use Adam optimizer, passing it the parameters of your model and the learning rate\n",
    "train(cnn, epochs, verbose=False, tag='CNN Loss/Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn = FullyConnectedNet().to(device)\n",
    "print('Number of parameters in fnn:', get_n_params(fnn))\n",
    "optimiser = torch.optim.Adam(fnn.parameters(), lr=learning_rate) # use Adam optimizer, passing it the parameters of your model and the learning rate\n",
    "train(fnn, epochs, verbose=False, tag='FNN Loss/Train')\n",
    "\n",
    "print('CNN Train Accuracy:', calc_accuracy(cnn, train_loader))\n",
    "print('CNN Validation Accuracy:', calc_accuracy(cnn, val_loader))\n",
    "print('CNN Test Accuracy:', calc_accuracy(cnn, test_loader), '\\n')\n",
    "\n",
    "print('FNN Train Accuracy:', calc_accuracy(fnn, train_loader))\n",
    "print('FNN Validation Accuracy:', calc_accuracy(fnn, val_loader))\n",
    "print('FNN Test Accuracy:', calc_accuracy(fnn, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, there is a significant disparity between the test accuracy of the two architectures, with the CNN have ~+20% test accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "The fully connected neural network we looked at in the previous lesson takes in a vector as input thus a flattened image could be passed in as input and used for classification problems successfully. But this is not the best way to do it. When trying to interpret an image, the spatial relations between the different pixels is crucial to our understanding. When we flatten the image, we lose this information.\n",
    "\n",
    "Convolutional neural networks solve this very problem. Rather than performing a matrix multiplication, a convolution operation is performed which can take in a 2d input and give a 2d output hence keep the information about the spatial relations of the pixels. This greatly increases their performance on image and video processing tasks.\n",
    "\n",
    "In the convolution proccess, you have a filter which you start on the top left side of the image and slide across the whole image, taking a dot product between the values of the filter and pixel values of the image. Bear in mind that colour images have three colour channels so your filter may be 3d so you take the dot product across a 3d volume. Each dot product corresponds to a single activation value in a 2d matrix of neurons which corresponds to a single layer in the output.\n",
    "\n",
    "![image](images/CNN_RGB.JPG)\n",
    "\n",
    "The animation below shows how a 1x3x3 filter is applied to a 1x5x5 image. Notice how the output has high  values when the filter is passed over locations where there is an X shape in the input image. This is because the values of the filter are such that it is performing pattern matching for the X shape.\n",
    "\n",
    "![image](images/convolution_animation.gif)\n",
    "\n",
    "Convolution operations have 3 parameters, the kernel size, which is the width and height of our filter, the stride, which is the number of pixels we translate our kernel by to compute the next feature, and the padding which is how many layers of 0 padding we add to the input image. We som\n",
    "\n",
    "![image](images/CNN_diagram.JPG)\n",
    "\n",
    "Each computed feature is a linear function of the pixels in a local region as opposed to fully connected nets where each computed feature is a linear function of all the pixels in the image.\n",
    "\n",
    "We have some prior understanding of how image data should be processed. We apply the same set of weights at different locations all over the image because we know that features are repeated at different locations throughout the image. This makes the learned features translation invariant.\n",
    "\n",
    "\n",
    "![image](images/CNN_FNN_comparison.JPG)\n",
    "\n",
    "\n",
    "For a long time, operations like this were used in computer vision to find different patterns in images with the engineers having to manually tune the values of the filters to perform the required function. The only difference now is that we apply an activation such as Relu or Sigmoid at each layer and after setting up the structure of the network, we initialize the filter values randomly before using gradient descent to automatically tune the values of the filters. We can also apply max pooling operations to subsample the output at each layer therefore reducing the number of parameters that need to be learned for the succeeding convolution operation.\n",
    "\n",
    "\n",
    "Just like before, each layer in the whole network learns higher level abstract features from the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# GET THE TRAINING DATASET\n",
    "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
    "                            transform=transforms.ToTensor(),          # transform the data from a PIL image to a tensor\n",
    "                            train=True,                               # is this training data?\n",
    "                            download=True                             # should i download it if it's not already here?\n",
    "                           )\n",
    "\n",
    "# GET THE TEST DATASET\n",
    "test_data = datasets.MNIST(root='MNIST-data',\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           train=False,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = train_data[np.random.randint(0, 300)][0]    # get a random example\n",
    "#print(x)\n",
    "plt.imshow(x[0].numpy(),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# MAKE TRAINING DATALOADER\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE VALIDATION DATALOADER\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE TEST DATALOADER\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            # torch.nn.conv2d(in_channels, out_channels, kernel_size)\n",
    "            # in_channels is the number of layers which it takes in (i.e.num color channels in 1st layer)\n",
    "            # out_channels is the number of different filters that we use\n",
    "            # kernel_size is the depthxwidthxheight of the kernel#\n",
    "            # stride is how many pixels we shift the kernel by each time\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005 # set learning rate\n",
    "epochs = 5# set number of epochs\n",
    "\n",
    "cnn = \n",
    "criterion = \n",
    "optimiser = \n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for idx, (inputs, labels) in enumerate(train_loader):\n",
    "            ## get x, y from data\n",
    "            \n",
    "            # pass x through your model to get a prediction\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the cost\n",
    "            if verbose: print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss.item())\n",
    "            \n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of all of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and store all of the model's param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            \n",
    "            writer.add_scalar(tag, loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "    print('Training Complete. Final loss =',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(cnn, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def calc_accuracy(model, dataloader):\n",
    "    num_correct = 0\n",
    "    num_examples = len(dataloader.dataset)                       # test DATA not test LOADER\n",
    "    for inputs, labels in dataloader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        predictions = model(inputs)\n",
    "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
    "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 98.574\n",
      "Validation Accuracy: 98.11999999999999\n",
      "Test Accuracy: 98.24000000000001\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy:', calc_accuracy(cnn, train_loader))\n",
    "print('Validation Accuracy:', calc_accuracy(cnn, val_loader))\n",
    "print('Test Accuracy:', calc_accuracy(cnn, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Benchmarking of Fully Connected vs Convolutional Architecture on MNIST\n",
    "\n",
    "Lets empirically test the translation invariance of the two architectures. \n",
    "\n",
    "The training set now has a center crop transformation which crops the central pixels of the image to a given size. However, the test set has a random crop transformation which crops a random region of the image to a given size.\n",
    "\n",
    "So we are training the neural networks with a centrally cropped image but testing it on cropped images with translations applied.\n",
    "\n",
    "The network that has more translation invariant features should performs better on this test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "crop_size = 22\n",
    "\n",
    "traintransforms = []\n",
    "traintransforms.append(transforms.##write here)\n",
    "traintransforms.append(transforms.##)\n",
    "traintransforms = transforms.Compose(traintransforms)\n",
    "\n",
    "# GET THE TRAINING DATASET\n",
    "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
    "                            transform=traintransforms,          # transform the data from a PIL image to a tensor\n",
    "                            train=True,                               # is this training data?\n",
    "                            download=True                             # should i download it if it's not already here?\n",
    "                           )\n",
    "\n",
    "testtransforms = []\n",
    "testtransforms.append(transforms.##write here)\n",
    "testtransforms.append(transforms.##)\n",
    "testtransforms = transforms.Compose(testtransforms)\n",
    "\n",
    "# GET THE TEST DATASET\n",
    "test_data = datasets.MNIST(root='MNIST-data',\n",
    "                           transform=testtransforms,\n",
    "                           train=False,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Example\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Example\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# PRINT THEIR LENGTHS AND VISUALISE AN EXAMPLE\n",
    "x = train_data[np.random.randint(0, len(train_data))][0]    # get a random training example\n",
    "print('Train Example')\n",
    "plt.imshow(x[0].numpy())\n",
    "plt.show()\n",
    "x = test_data[np.random.randint(0, len(test_data))][0]    # get a random test example\n",
    "print('Test Example')\n",
    "plt.imshow(x[0].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# MAKE TRAINING DATALOADER\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE VALIDATION DATALOADER\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE TEST DATALOADER\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the architectures of our fully connected and convolutional networks as well as a function which returns the number of parameters in each model. Since the number of parameters in a model is a rough measure of its capacity, the networks should have an approximately equal number of parameters to make it a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "class FullyConnectedNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()    # initialise parent module\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            ##\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, crop_size*crop_size)\n",
    "        x = self.fc_layers(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = torch.nn.Sequential( # put your convolutional architecture here using torch.nn.Sequential \n",
    "            ##\n",
    "        )\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            ##\n",
    "            \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)# pass through conv layers\n",
    "        x = x.view(x.shape[0], -1)# flatten output ready for fully connected layer\n",
    "        x = self.fc_layers(x)# pass through fully connected layer\n",
    "        x = F.softmax(x, dim=1)# softmax activation function on outputs\n",
    "        return x\n",
    "\n",
    "def get_n_params(model):\n",
    "    #model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    #n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005 # set learning rate\n",
    "epochs = 5 # set number of epochs\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in cnn: 128522\n",
      "Training Complete. Final loss = 1.4745460748672485\n"
     ]
    }
   ],
   "source": [
    "cnn = ConvNet().to(device)#instantiate model\n",
    "print('Number of parameters in cnn:', get_n_params(cnn))\n",
    "criterion = torch.nn.CrossEntropyLoss() #use cross entropy loss function\n",
    "optimiser = torch.optim.Adam(cnn.parameters(), lr=learning_rate) # use Adam optimizer, passing it the parameters of your model and the learning rate\n",
    "train(##)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in fnn: 137285\n",
      "Training Complete. Final loss = 1.5748454332351685\n"
     ]
    }
   ],
   "source": [
    "fnn = FullyConnectedNet().to(device)\n",
    "print('Number of parameters in fnn:', get_n_params(fnn))\n",
    "optimiser = torch.optim.Adam(fnn.parameters(), lr=learning_rate) # use Adam optimizer, passing it the parameters of your model and the learning rate\n",
    "train(##)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Train Accuracy: 97.932\n",
      "CNN Validation Accuracy: 97.50999999999999\n",
      "CNN Test Accuracy: 71.98 \n",
      "\n",
      "FNN Train Accuracy: 87.044\n",
      "FNN Validation Accuracy: 86.3\n",
      "FNN Test Accuracy: 46.87\n"
     ]
    }
   ],
   "source": [
    "print('CNN Train Accuracy:', calc_accuracy(cnn, train_loader))\n",
    "print('CNN Validation Accuracy:', calc_accuracy(cnn, val_loader))\n",
    "print('CNN Test Accuracy:', calc_accuracy(cnn, test_loader), '\\n')\n",
    "\n",
    "print('FNN Train Accuracy:', calc_accuracy(fnn, train_loader))\n",
    "print('FNN Validation Accuracy:', calc_accuracy(fnn, val_loader))\n",
    "print('FNN Test Accuracy:', calc_accuracy(fnn, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, there is a significant disparity between the test accuracy of the two architectures, with the CNN have ~+20% test accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Datasets\n",
    "\n",
    "__Prerequisites__\n",
    "\n",
    "- [Convolutional Neural Networks](https://github.com/AI-Core/Convolutional-Neural-Networks)\n",
    "\n",
    "So far, we have only used the MNIST dataset, which is easily accessible through torchvision. What do we do when we have our own data which we want to use with PyTorch?\n",
    "\n",
    "To be compatible with a torch dataloader, we have to write a class from which we can create instances of the dataset. This class must overwrite the \\_\\_len\\_\\_ and \\_\\_getitem\\_\\_ functions. The \\_\\_len\\_\\_ function must return the length of the dataset we are loading in. The \\_\\_getitem\\_\\_ function must return an example datapoint given the index of it.\n",
    "\n",
    "In our \\_\\_init\\_\\_ function, we read in the variables which we can use to get items from our dataset. In this case, \n",
    "\n",
    "Today, we will implement a class which loads in the S40 detection dataset. Detection is when we want our algorithm to draw rectangles around the locations of specific objects within the image. As opposed to classification where we simply have a binary output indicating if the object is contained within the image.\n",
    "The dataset consists of an \"images\" folder which contains the input images and an \"annotations\" folder which, for each image, contains an xml file with the same name as the image and contains the co-ordinates for the top-left and bottom-right corners of the rectangular bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "class S40dataset():\n",
    "\n",
    "    def __init__(self, img_dir='S40-data/images', annotation_dir='S40-data/annotations', transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.img_names = os.listdir(img_dir)                                  # list all files in the img folder\n",
    "        self.img_names.sort()                                                   # order the images alphabetically\n",
    "        self.img_names = [os.path.join(img_dir, img_name) for img_name in self.img_names]   # join folder and file names\n",
    "\n",
    "        self.annotation_names = os.listdir(annotation_dir)                      # list all annotation files\n",
    "        self.annotation_names.sort()                                            # order annotation files alphabetically\n",
    "        self.annotation_names = [os.path.join(annotation_dir, ann_name) for ann_name in self.annotation_names]   # join folder and file names\n",
    "\n",
    "        print(self.img_names)\n",
    "        print(self.annotation_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx] #get the path of the image at that index\n",
    "        img = Image.open(img_name) #open the image using the path\n",
    "\n",
    "        annotation_name = self.annotation_names[idx] #get the path to the label file\n",
    "        annotation_tree = ET.parse(annotation_name) #use xml parser to load the file\n",
    "        bndbox_xml = annotation_tree.find(\"object\").find(\"bndbox\") #get the tag which contains our labels\n",
    "        \n",
    "        #get the x and y values for the corners of the rectangle\n",
    "        xmax = int(bndbox_xml.find('xmax').text) \n",
    "        ymax = int(bndbox_xml.find('ymax').text)\n",
    "        xmin = int(bndbox_xml.find('xmin').text)\n",
    "        ymin = int(bndbox_xml.find('ymin').text)\n",
    "        #print(xmax, ymax, xmin, ymin)\n",
    "\n",
    "        # Convert from corner co-ordinates format into center co-ordinate, width and height format\n",
    "        w = xmax - xmin #\n",
    "        h = ymax - ymin\n",
    "        x = int(xmin + w / 2)\n",
    "        y = int(ymin + h / 2)\n",
    "\n",
    "        # Normlise the labels so the values are expressed as a proportion of the whole image\n",
    "        x /= img.size[0]\n",
    "        w /= img.size[0]\n",
    "        y /= img.size[1]\n",
    "        h /= img.size[1]\n",
    "\n",
    "        bndbox = (x, y, w, h)\n",
    "        \n",
    "        #apply any transforms\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        bndbox = torch.tensor(bndbox)\n",
    "\n",
    "        return img, bndbox\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "# Convert from  center co-ordinate, width and height format into corner co-ordinates format\n",
    "def unpack_bndbox(bndbox, img):\n",
    "    bndbox = list(bndbox[0])\n",
    "    x, y, w, h = tuple(bndbox)\n",
    "    x *= img.size[0]\n",
    "    w *= img.size[0]\n",
    "    y *= img.size[1]\n",
    "    h *= img.size[1]\n",
    "    xmin = x - w / 2\n",
    "    xmax = x + w / 2\n",
    "    ymin = y - h / 2\n",
    "    ymax = y + h / 2\n",
    "    bndbox = [xmin, ymin, xmax, ymax]\n",
    "    return bndbox\n",
    "\n",
    "def show(batch, pred_bndbox=None):\n",
    "    img, bndbox = batch\n",
    "\n",
    "    img = img[0]\n",
    "    print(img.shape)\n",
    "    img = transforms.ToPILImage()(img)\n",
    "    img = transforms.Resize((512, 512))(img)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    bndbox = unpack_bndbox(bndbox, img)\n",
    "    print(bndbox)\n",
    "    draw.rectangle(bndbox)\n",
    "    if pred_bndbox is not None:\n",
    "        pred_bndbox = unpack_bndbox(pred_bndbox, img)\n",
    "        draw.rectangle(pred_bndbox, outline=1000)\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we build an algorithm to learn solutions to this problem? Find out in the CNN Detection notebook!\n",
    "\n",
    "__Next Steps__\n",
    "\n",
    "- [CNN Detection](https://github.com/AI-Core/Convolutional-Neural-Networks/blob/master/CNN%20Detection.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
